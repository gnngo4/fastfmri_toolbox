{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "sys.path.append(\"ComputeCanada/frequency_tagging\")\n",
    "sys.path.append(\"/opt/supervenn\")\n",
    "from supervenn import supervenn\n",
    "from utils import (\n",
    "    get_roi_colour_codes,\n",
    "    change_font,\n",
    "    HCP_ATLAS as hcp_label,\n",
    "    im_generate_single_subject_maps,\n",
    "    NORMAL_3T_SUB_IDS,\n",
    "    NORMAL_7T_SUB_IDS,\n",
    "    VARY_SUB_IDS,\n",
    "    VARY_020_ENTRAIN_CONDITIONS,\n",
    "    VARY_021_ENTRAIN_CONDITIONS,\n",
    "    PICKLE_DIR,\n",
    "    get_frequency_text_codes,\n",
    "    MAIN,\n",
    ")\n",
    "change_font()\n",
    "\n",
    "tmpdir = Path(\"/tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get HCP info\n",
    "- `hcp_mappings`: dict of ROI: dscalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get HCP labels\n",
    "\"\"\"\n",
    "_HCP_INFO = !wb_command -file-information {hcp_label}\n",
    "HCP_LABELS = []\n",
    "HCP_COUNTER = 0\n",
    "for i in _HCP_INFO:\n",
    "    if len(i) == 60 and any([\"L_\" in i, \"R_\" in i]):\n",
    "        hcp_colors = tuple([float(f\"0.{k}\") for k in [j.split(' ') [0] for j in i.split('0.')][-3:]] + [1])\n",
    "        if ' R_' in i:\n",
    "            roi = i.split(\"_ROI\")[0].split(' R_')[1]\n",
    "            HCP_LABELS.append(f\"R_{roi}_ROI\")\n",
    "        if ' L_' in i:\n",
    "            roi = i.split(\"_ROI\")[0].split(' L_')[1]\n",
    "            HCP_LABELS.append(f\"L_{roi}_ROI\")\n",
    "        HCP_COUNTER += 1\n",
    "\n",
    "\"\"\"Get HCP label coordinates\n",
    "\"\"\"\n",
    "hcp_mapping = {}\n",
    "for roi_label in HCP_LABELS:\n",
    "    out_dscalar = tmpdir / f\"{roi_label}.dscalar.nii\"\n",
    "    if out_dscalar.exists():\n",
    "        hcp_mapping[roi_label] = out_dscalar\n",
    "        continue\n",
    "    !wb_command -cifti-label-to-roi {hcp_label} {out_dscalar} -name {roi_label}\n",
    "    assert out_dscalar.exists(), f\"{out_dscalar.stem} does not exist.\"\n",
    "    hcp_mapping[roi_label] = out_dscalar\n",
    "hcp_rois = list(set([k.split('_')[1] for k in hcp_mapping.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate intermodulation maps:\n",
    "- Runtime: `corr_types = [\"uncp\"]`: 827 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set up for visualizing dual frequency tagging across each subject using fractional overlap\n",
    "\"\"\"\n",
    "ROI_FO = .8\n",
    "corr_type = \"uncp\"\n",
    "FORCE_RUN = False\n",
    "\n",
    "\"\"\"Save png\n",
    "\"\"\"\n",
    "im_map_pkl = PICKLE_DIR / f\"im_map_fo-{ROI_FO}_corr-{corr_type}.pkl\"\n",
    "im_map_proportion_pkl = PICKLE_DIR / f\"im_map_proportion_fo-{ROI_FO}_corr-{corr_type}.pkl\"\n",
    "if im_map_pkl.exists() and im_map_proportion_pkl.exists() and not FORCE_RUN:\n",
    "    df = pd.read_pickle(im_map_pkl)\n",
    "    proportion_df = pd.read_pickle(im_map_proportion_pkl)\n",
    "else:\n",
    "    # 3T control/entrain\n",
    "    label = \"3TNormal\"\n",
    "    df_data = None\n",
    "    proportion_data = None\n",
    "    for _roi_task_id in [\"control\",\"entrain\"]:\n",
    "        experiment_id = \"1_frequency_tagging\" \n",
    "        mri_id = \"3T\"\n",
    "        n_subjects = len(NORMAL_3T_SUB_IDS)\n",
    "        roi_task_ids = [_roi_task_id] * n_subjects\n",
    "        roi_f_1s = [.125] * n_subjects\n",
    "        roi_f_2s = [.2] * n_subjects\n",
    "        df_data, proportion_data = im_generate_single_subject_maps(\n",
    "            hcp_mapping,\n",
    "            label, experiment_id, mri_id, NORMAL_3T_SUB_IDS, \n",
    "            roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO, HCP_LABELS,\n",
    "            df_data=df_data,\n",
    "            proportion_data=proportion_data,\n",
    "            corr_type=corr_type,\n",
    "            ROI_FO=ROI_FO,\n",
    "        )\n",
    "        clear_output()\n",
    "    # 7T AttendAway\n",
    "    label = \"7TNormal\"\n",
    "    experiment_id = \"1_attention\" \n",
    "    mri_id = \"7T\"\n",
    "    n_subjects = len(NORMAL_7T_SUB_IDS)\n",
    "    roi_task_ids = [\"AttendAway\"] * n_subjects\n",
    "    roi_f_1s = [.125] * n_subjects\n",
    "    roi_f_2s = [.2] * n_subjects\n",
    "    df_data, proportion_data = im_generate_single_subject_maps(\n",
    "        hcp_mapping,\n",
    "        label, experiment_id, mri_id, NORMAL_7T_SUB_IDS, \n",
    "        roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO, HCP_LABELS,\n",
    "        df_data=df_data,\n",
    "        proportion_data=proportion_data,\n",
    "        corr_type=corr_type,\n",
    "        ROI_FO=ROI_FO\n",
    "    )\n",
    "    clear_output()\n",
    "    # 3T/7T vary\n",
    "    experiment_id = \"1_frequency_tagging\"\n",
    "    for label, mri_id in zip([\"3TVary\",\"7TVary\"], [\"3T\",\"7T\"]):\n",
    "        sub_ids = [VARY_SUB_IDS[0]] * 3 + [VARY_SUB_IDS[1]] * 3\n",
    "        roi_task_ids = [i[0] for i in VARY_020_ENTRAIN_CONDITIONS] + [i[0] for i in VARY_021_ENTRAIN_CONDITIONS]\n",
    "        roi_f_1s = [i[1] for i in VARY_020_ENTRAIN_CONDITIONS] + [i[1] for i in VARY_021_ENTRAIN_CONDITIONS]\n",
    "        roi_f_2s = [i[2] for i in VARY_020_ENTRAIN_CONDITIONS] + [i[2] for i in VARY_021_ENTRAIN_CONDITIONS]\n",
    "        df_data, proportion_data = im_generate_single_subject_maps(\n",
    "            hcp_mapping,\n",
    "            label, experiment_id, mri_id, sub_ids,\n",
    "            roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO, HCP_LABELS,\n",
    "            df_data=df_data,\n",
    "            proportion_data=proportion_data,\n",
    "            corr_type=corr_type,\n",
    "            ROI_FO=ROI_FO,\n",
    "        )\n",
    "        clear_output()\n",
    "        \n",
    "    df = pd.DataFrame(df_data)\n",
    "    df.to_pickle(im_map_pkl)\n",
    "    proportion_df = pd.DataFrame(proportion_data)\n",
    "    proportion_df.to_pickle(im_map_proportion_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate f1, f2, 2f1, 2f2, f2-f1, f1+f2, 2f1-f2, 2f2-f1 locked maps across MRI strength and varying entrainment experiments\n",
    "- sub_ids = 020, 021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    load_mean_dtseries,\n",
    "    WORKING_DIR,\n",
    "    crop_and_save,\n",
    ")\n",
    "\n",
    "from wbplot import dscalar\n",
    "\n",
    "DFM_VARY_LOCKED_MAPS = WORKING_DIR / \"figures\" / \"intermodulation_mapping\" / \"locked_vary_maps\"\n",
    "if not DFM_VARY_LOCKED_MAPS.exists():\n",
    "    DFM_VARY_LOCKED_MAPS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "VERTEX_TO = 59412\n",
    "PALETTE_PARAMS = {\n",
    "    \"disp-zero\": False,\n",
    "    \"disp-neg\": False,\n",
    "    \"disp-pos\": True,\n",
    "    \"pos-user\": (0, 1.),\n",
    "    \"neg-user\": (-1,0),\n",
    "    \"interpolate\": True,\n",
    "}\n",
    "# Cropped settings\n",
    "LEFT=590;TOP=80;RIGHT=1140;BOTTOM=460\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(matrix1, matrix2):\n",
    "    \"\"\"V\n",
    "    Compute the Dice coefficient between two binary matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    matrix1 (numpy array): First binary matrix.\n",
    "    matrix2 (numpy array): Second binary matrix.\n",
    "    \n",
    "    Returns:\n",
    "    float: Dice coefficient between matrix1 and matrix2.\n",
    "    \"\"\"\n",
    "    # Ensure the input matrices are binary\n",
    "    matrix1 = np.asarray(matrix1).astype(bool)\n",
    "    matrix2 = np.asarray(matrix2).astype(bool)\n",
    "    \n",
    "    # Compute the intersection\n",
    "    intersection = np.logical_and(matrix1, matrix2).sum()\n",
    "    \n",
    "    # Compute the Dice coefficient\n",
    "    dice_coeff = 2 * intersection / (matrix1.sum() + matrix2.sum())\n",
    "    \n",
    "    return dice_coeff\n",
    "\n",
    "\n",
    "expected_n_runs = 6\n",
    "\n",
    "data_dict = {}\n",
    "for sub_id in [\"020\", \"021\"]:\n",
    "\n",
    "    if sub_id == \"020\":\n",
    "        entrain_conditions = [i[0] for i in VARY_020_ENTRAIN_CONDITIONS]\n",
    "    elif sub_id == \"021\":\n",
    "        entrain_conditions = [i[0] for i in VARY_021_ENTRAIN_CONDITIONS]\n",
    "\n",
    "    entrain_2f1_sub_f2 = []\n",
    "    entrain_f2_sub_f1 = []\n",
    "    entrain_f1 = []\n",
    "    entrain_2f1 = []\n",
    "    entrain_2f2_sub_f1 = []\n",
    "    entrain_f2 = []\n",
    "    entrain_f1_add_f2 = []\n",
    "    entrain_2f2 = []\n",
    "    entrain1_activations = !ls /scratch/fastfmri/*merged*IM*roi-entrain*uncp*/sub-{sub_id}/bootstrap/*task-{entrain_conditions[0]}*data-train*activations*\n",
    "    if sub_id == \"020\":\n",
    "        entrain_lists = [entrain_2f1_sub_f2, entrain_f2_sub_f1, entrain_f1, entrain_2f1, entrain_2f2_sub_f1, entrain_f2, entrain_f1_add_f2, entrain_2f2]\n",
    "    else:\n",
    "        entrain_lists = [entrain_2f1_sub_f2, entrain_f2_sub_f1, entrain_f1, entrain_2f1, entrain_2f2_sub_f1, entrain_f2, entrain_f1_add_f2, entrain_2f2]\n",
    "    for ix, _entrain_list in enumerate(entrain_lists):\n",
    "        _entrain_list += [ entrain1_activations[ix], entrain1_activations[ix+8] ]\n",
    "\n",
    "    entrain2_activations = !ls /scratch/fastfmri/*merged*IM*roi-entrain*uncp*/sub-{sub_id}/bootstrap/*task-{entrain_conditions[1]}*data-train*activations*\n",
    "    if sub_id == \"020\":\n",
    "        entrain_lists = [entrain_f2_sub_f1, entrain_2f1_sub_f2, entrain_f1, entrain_f2, entrain_2f2_sub_f1, entrain_2f1, entrain_2f2, entrain_f1_add_f2]\n",
    "    else:\n",
    "        entrain_lists = [entrain_f2_sub_f1, entrain_f1, entrain_2f1_sub_f2, entrain_2f2_sub_f1, entrain_f2, entrain_f1_add_f2, entrain_2f1, entrain_2f2]\n",
    "    for ix, _entrain_list in enumerate(entrain_lists):\n",
    "        _entrain_list += [ entrain2_activations[ix], entrain2_activations[ix+8] ]\n",
    "\n",
    "    entrain3_activations = !ls /scratch/fastfmri/*merged*IM*roi-entrain*uncp*/sub-{sub_id}/bootstrap/*task-{entrain_conditions[2]}*data-train*activations*\n",
    "    if sub_id == \"020\":\n",
    "        entrain_lists = [entrain_f2_sub_f1, entrain_f1, entrain_f2, entrain_2f2_sub_f1, entrain_2f1_sub_f2, entrain_2f1, entrain_f1_add_f2, entrain_2f2]\n",
    "    else:\n",
    "        entrain_lists = [entrain_f2_sub_f1, entrain_2f1_sub_f2, entrain_f1, entrain_2f2_sub_f1, entrain_f2, entrain_2f1, entrain_f1_add_f2, entrain_2f2]\n",
    "    for ix, _entrain_list in enumerate(entrain_lists):\n",
    "        _entrain_list += [ entrain3_activations[ix], entrain3_activations[ix+8] ]\n",
    "\n",
    "    assert len(entrain_f1) == expected_n_runs == len(entrain_f2) == len(entrain_2f1_sub_f2) == len(entrain_2f1) == len(entrain_2f2_sub_f1) == len(entrain_f2) == len(entrain_f1_add_f2) == len(entrain_2f2)\n",
    "\n",
    "    for entrain_descriptor, entrain_f in zip(\n",
    "        [\"f1\",\"f2\",\"2f1_sub_f2\",\"2f1\",\"2f2_sub_f1\",\"f2_sub_f1\",\"f1_add_f2\",\"2f2\"],\n",
    "        [entrain_f1, entrain_f2,entrain_2f1_sub_f2,entrain_2f1,entrain_2f2_sub_f1,entrain_f2_sub_f1,entrain_f1_add_f2,entrain_2f2]\n",
    "    ):\n",
    "        \n",
    "        print(sub_id, entrain_descriptor)\n",
    "        png_out = DFM_VARY_LOCKED_MAPS / f\"sub-{sub_id}_{entrain_descriptor}.png\"\n",
    "        for ix, activation in enumerate(entrain_f):\n",
    "            _data = (load_mean_dtseries(activation)[:VERTEX_TO] > .8).astype(float)\n",
    "            if ix == 0:\n",
    "                data = _data\n",
    "            else:\n",
    "                data += _data\n",
    "        data /= 6\n",
    "        dscalar(\n",
    "            png_out, data, \n",
    "            orientation=\"portrait\", \n",
    "            hemisphere='right',\n",
    "            palette=\"magma\", \n",
    "            palette_params=PALETTE_PARAMS,\n",
    "            transparent=False,\n",
    "            flatmap=True,\n",
    "            flatmap_style='plain',\n",
    "        )\n",
    "        crop_and_save(png_out, str(png_out).replace(\"png\", \"cropped.png\"), LEFT, TOP, RIGHT, BOTTOM)\n",
    "        clear_output()\n",
    "        \n",
    "    mosaic = [\"multiplex\",\"f1\",\"f2\",\"f2_sub_f1\",\"f1_add_f2\",\"2f1\",\"2f2\",\"2f1-f2\",\"2f2-f1\"]\n",
    "    fig, ax_dict = plt.subplot_mosaic([mosaic],figsize=(7,1), dpi=300,layout=\"constrained\")\n",
    "    dcoeff_dict = {}\n",
    "    for ix, (entrain_descriptor, entrain_f) in enumerate(zip(\n",
    "        mosaic,\n",
    "        [\n",
    "            [entrain_f1, entrain_f2],\n",
    "            entrain_f1, \n",
    "            entrain_f2,\n",
    "            entrain_f2_sub_f1,\n",
    "            entrain_f1_add_f2,\n",
    "            entrain_2f1,\n",
    "            entrain_2f2,\n",
    "            entrain_2f1_sub_f2,\n",
    "            entrain_2f2_sub_f1,\n",
    "        ]\n",
    "    )):\n",
    "        if entrain_descriptor == \"multiplex\":\n",
    "            entrain_f1 = entrain_f[0]\n",
    "            entrain_f2 = entrain_f[1]\n",
    "            entrain_f1.sort()\n",
    "            entrain_f2.sort()\n",
    "            dcoeff = np.zeros((len(entrain_f1), len(entrain_f1)))\n",
    "            for ix, (dscalarf1_1,dscalarf2_1) in enumerate(zip(entrain_f1,entrain_f2)):\n",
    "                for iy, (dscalarf1_2,dscalarf2_2) in enumerate(zip(entrain_f1,entrain_f2)):\n",
    "                    if ix <= iy:\n",
    "                        dcoeff[ix,iy] = np.nan\n",
    "                        continue\n",
    "                    X1 = np.array(nib.load(dscalarf1_1).get_fdata())\n",
    "                    X2 = np.array(nib.load(dscalarf2_1).get_fdata())\n",
    "                    Y1 = np.array(nib.load(dscalarf1_2).get_fdata())\n",
    "                    Y2 = np.array(nib.load(dscalarf2_2).get_fdata())\n",
    "                    X = (( (X1.mean(0) >= .8).astype(int) + (X2.mean(0) >= .8).astype(int) ) == 2).astype(int)\n",
    "                    Y = (( (Y1.mean(0) >= .8).astype(int) + (Y2.mean(0) >= .8).astype(int) ) == 2).astype(int)\n",
    "                    dcoeff[ix,iy] = dice_coefficient(X,Y)\n",
    "            dcoeff_dict[entrain_descriptor] = dcoeff\n",
    "        else:\n",
    "            entrain_f.sort()\n",
    "            dcoeff = np.zeros((len(entrain_f), len(entrain_f)))\n",
    "            for ix, dscalar1 in enumerate(entrain_f):\n",
    "                for iy,dscalar2 in enumerate(entrain_f):\n",
    "                    if ix <= iy:\n",
    "                        dcoeff[ix,iy] = np.nan\n",
    "                        continue\n",
    "                    X = np.array(nib.load(dscalar1).get_fdata())\n",
    "                    Y = np.array(nib.load(dscalar2).get_fdata())\n",
    "                    X = (X.mean(0) >= .8).astype(int)\n",
    "                    Y = (Y.mean(0) >= .8).astype(int)\n",
    "                    dcoeff[ix,iy] = dice_coefficient(X,Y)\n",
    "            dcoeff_dict[entrain_descriptor] = dcoeff\n",
    "\n",
    "    max_v = 0\n",
    "    for entrain_descriptor, dcoeff in dcoeff_dict.items():\n",
    "        if np.nanmax(dcoeff) > max_v:\n",
    "            max_v = np.nanmax(dcoeff)\n",
    "    for entrain_descriptor, dcoeff in dcoeff_dict.items():\n",
    "        ax = ax_dict[entrain_descriptor]\n",
    "        cmap = plt.get_cmap(\"BuPu\")\n",
    "        cmap.set_bad(\"white\")\n",
    "        ax.imshow(dcoeff, cmap=cmap, vmin=0, vmax=max_v, aspect=\"auto\", interpolation=\"none\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        for s in [\"left\",\"right\",\"top\",\"bottom\"]:\n",
    "            ax.spines[s].set_linewidth(.5)\n",
    "        entrain_descriptor = entrain_descriptor.replace(\"_sub_\",\"-\")\n",
    "        entrain_descriptor = entrain_descriptor.replace(\"_add_\",\"+\")\n",
    "        if entrain_descriptor == \"multiplex\":\n",
    "            ax.set_title(\"Multiplex\", fontsize=8)\n",
    "        else:\n",
    "            ax.set_title(get_frequency_text_codes()[entrain_descriptor], fontsize=8)\n",
    "        ax.set_xticks([i for i in range(5)])\n",
    "        ax.set_yticks([i+1 for i in range(5)])\n",
    "        if sub_id == \"020\":\n",
    "            ax.set_xticklabels([\"3T/A\",\"3T/B\",\"3T/C\",\"7T/A\",\"7T/B\"], fontsize=7, rotation=90)\n",
    "            ax.set_yticklabels([\"3T/B\",\"3T/C\",\"7T/A\",\"7T/B\",\"7T/C\"], fontsize=7, rotation=0)\n",
    "        if sub_id == \"021\":\n",
    "            ax.set_xticklabels([\"3T/D\",\"3T/E\",\"3T/F\",\"7T/D\",\"7T/E\"], fontsize=7, rotation=90)\n",
    "            ax.set_yticklabels([\"3T/E\",\"3T/F\",\"7T/D\",\"7T/E\",\"7T/F\"], fontsize=7, rotation=0)\n",
    "\n",
    "        ax.tick_params(axis=\"both\", width=.5, length=2, pad=0)\n",
    "\n",
    "    fig.savefig(MAIN / f\"Supp_X_IM_HEATMAPS_{sub_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermodulation composition across FOV and all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    NORMAL_3T_SUB_IDS, NORMAL_3T_CONDITIONS,\n",
    "    NORMAL_7T_SUB_IDS, NORMAL_7T_ENTRAIN_CONDITIONS,\n",
    "    VARY_SUB_IDS, VARY_020_ENTRAIN_CONDITIONS, VARY_021_ENTRAIN_CONDITIONS,\n",
    ")\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "text_dict = get_frequency_text_codes()\n",
    "roi_c_dict = get_roi_colour_codes()\n",
    "colors = [\n",
    "    'white',\n",
    "    roi_c_dict[\"f1\"], \n",
    "    roi_c_dict[\"f2\"], \n",
    "    roi_c_dict[\"f1f2\"], \n",
    "    \"cyan\"\n",
    "]\n",
    "custom_cmap = ListedColormap(colors)\n",
    "custom_cmap.set_bad(\"white\")\n",
    "FONTSIZE = 8\n",
    "LINEWIDTH = .4\n",
    "\n",
    "sub_ids = NORMAL_3T_SUB_IDS * 2 + NORMAL_7T_SUB_IDS + \\\n",
    "    [VARY_SUB_IDS[0]] * 3 + \\\n",
    "    [VARY_SUB_IDS[1]] * 3 + \\\n",
    "    [VARY_SUB_IDS[0]] * 3 + \\\n",
    "    [VARY_SUB_IDS[1]] * 3\n",
    "experiments = [\"3TNormal\"] * 18 + [\"7TNormal\"] * 4 + \\\n",
    "    [\"3TVary\"] * 6 + [\"7TVary\"] * 6\n",
    "task_conditions = [NORMAL_3T_CONDITIONS[0][0]]*9 + \\\n",
    "    [NORMAL_3T_CONDITIONS[1][0]]*9 + \\\n",
    "    [NORMAL_7T_ENTRAIN_CONDITIONS[0][0]]*4 + \\\n",
    "    [i[0] for i in VARY_020_ENTRAIN_CONDITIONS] + \\\n",
    "    [i[0] for i in VARY_021_ENTRAIN_CONDITIONS] + \\\n",
    "    [i[0] for i in VARY_020_ENTRAIN_CONDITIONS] + \\\n",
    "    [i[0] for i in VARY_021_ENTRAIN_CONDITIONS]\n",
    "\n",
    "#n_linspace = 1000\n",
    "#template = np.linspace(0,1,n_linspace)\n",
    "add_split = [9, 18, 22,22+6,22+12]\n",
    "IM_CODES = [\n",
    "    \"f1\",\"f2\",\"f2-f1\",\"f1+f2\",\"2f1-f2\",\"2f2-f1\",\"2f1\",\"2f2\"\n",
    "]\n",
    "for im_code in IM_CODES:\n",
    "    _df = proportion_df[(proportion_df.im_code==im_code)]\n",
    "    _df.drop_duplicates(subset=[\"im_code\",\"hcp_label\",\"sub_id\",\"roi_task_id\",\"experiment\"], inplace=True)\n",
    "\n",
    "    _df[\"f1_count\"] = _df[\"activated_vertex_count\"] * _df[\"f1\"]\n",
    "    _df[\"f2_count\"] = _df[\"activated_vertex_count\"] * _df[\"f2\"]\n",
    "    _df[\"f1&f2_count\"] = _df[\"activated_vertex_count\"] * _df[\"f1&f2\"]\n",
    "    _df[\"fim_count\"] = _df[\"activated_vertex_count\"] * _df[\"fim\"]\n",
    "    _df = _df.groupby([\"sub_id\",\"experiment\",\"roi_task_id\"])[[\"f1_count\",\"f2_count\",\"f1&f2_count\",\"fim_count\"]].sum().reset_index()\n",
    "    _df[\"all_count\"] = _df[\"f1_count\"] + _df[\"f2_count\"] + _df[\"f1&f2_count\"] + _df[\"fim_count\"]\n",
    "    for col_id in [\"f1_count\",\"f2_count\",\"f1&f2_count\",\"fim_count\"]:\n",
    "        _df[col_id] = _df[col_id] / _df[\"all_count\"]\n",
    "\n",
    "    # Reorganize\n",
    "    _df = pd.concat([\n",
    "        _df[_df.roi_task_id==\"control\"],\n",
    "        _df[_df.roi_task_id==\"entrain\"],\n",
    "        _df[_df.roi_task_id==\"AttendAway\"],\n",
    "        _df[(_df.experiment==\"3TVary\") & (_df.roi_task_id.str.contains(\"entrain\"))],\n",
    "        _df[(_df.experiment==\"7TVary\") & (_df.roi_task_id.str.contains(\"entrain\"))],\n",
    "    ])\n",
    "\n",
    "    max_vertex_count = _df[\"all_count\"].max()\n",
    "    n_linspace = int(max_vertex_count)\n",
    "    template = np.linspace(0,1,n_linspace)\n",
    "\n",
    "    vstacked_proportions = None\n",
    "    ypos = 3\n",
    "    fig, ax = plt.subplots(figsize=(1,4.2),dpi=300,layout=\"constrained\")\n",
    "    vertex_counts = []\n",
    "    for experiment_ix, (sub_id,experiment,task_condition) in enumerate(zip(sub_ids,experiments,task_conditions)):\n",
    "        _add_split = False\n",
    "        if experiment_ix in add_split:\n",
    "            _add_split = True\n",
    "            ypos += 1\n",
    "        row = _df[(_df.sub_id == sub_id) & (_df.experiment==experiment) & (_df.roi_task_id == task_condition)]\n",
    "        n_rows = row.shape[0]\n",
    "        assert n_rows in [0,1]\n",
    "        row_proportion = np.zeros_like(template)\n",
    "\n",
    "        #print(im_code, sub_id, experiment, task_condition, n_rows)\n",
    "        #continue\n",
    "        \n",
    "        # No vertices were allocated to this IM map and experiment\n",
    "        if n_rows != 0:\n",
    "            p_f1 = int(row[\"f1_count\"].values[0] * row.all_count.values[0])\n",
    "            p_f2 = int(row[\"f2_count\"].values[0] * row.all_count.values[0])\n",
    "            p_fim = int(row[\"fim_count\"].values[0] * row.all_count.values[0])\n",
    "            p_f1f2 = int(row[\"f1&f2_count\"].values[0] * row.all_count.values[0])\n",
    "            if im_code == \"f2-f1\":\n",
    "                #print(sub_id, experiment, task_condition, p_f1,p_f2,p_f1f2, p_fim, 100*(p_f1+p_f1f2)/(p_f1+p_f2+p_f1f2+p_fim))\n",
    "            #end_i = i+1\n",
    "            p_cumu_1 = 0\n",
    "            p_cumu_2 = 0\n",
    "\n",
    "            #import pdb; pdb.set_trace()\n",
    "            count = 0\n",
    "            for ix, (_p, c) in enumerate(zip([p_f1, p_f2, p_f1f2, p_fim],range(1,4+1,1))):\n",
    "                #if _p == 0:\n",
    "                    #continue\n",
    "                for _ in range(_p):\n",
    "                    row_proportion[count] = c\n",
    "                    count += 1\n",
    "                #p_cumu_2 += _p\n",
    "                #coords = np.where((template >= p_cumu_1) & (template <= p_cumu_2))\n",
    "                #row_proportion[coords] = c\n",
    "                #p_cumu_1 += _p\n",
    "        else:\n",
    "            pass\n",
    "            #row_proportion -= 1\n",
    "\n",
    "        try:\n",
    "            vertex_count = row.all_count.values[0]\n",
    "        except:\n",
    "            vertex_count = 0\n",
    "\n",
    "        vertex_counts.append(vertex_count)\n",
    "\n",
    "        \"\"\"\n",
    "        if vertex_count > 0:\n",
    "            ax.scatter(1100,ypos,s=vertex_count/100,c='k',edgecolor='k',linewidth=LINEWIDTH,zorder=12)\n",
    "        else:\n",
    "            pass\n",
    "            #ax.scatter(1100,ypos,s=12,c='k',marker='x',linewidth=LINEWIDTH,zorder=12)\n",
    "        \"\"\"\n",
    "        ypos += 1\n",
    "\n",
    "        row_proportion = row_proportion[np.newaxis,:]\n",
    "        if vstacked_proportions is None:\n",
    "            vstacked_proportions = np.zeros((3,n_linspace)) - 1\n",
    "            vstacked_proportions = np.vstack((vstacked_proportions, row_proportion))\n",
    "        else:\n",
    "            if _add_split:\n",
    "                vstacked_proportions = np.vstack((vstacked_proportions, np.zeros((1,n_linspace)) - 1))\n",
    "            vstacked_proportions = np.vstack((vstacked_proportions, row_proportion))\n",
    "\n",
    "    vstacked_proportions = np.ma.array(vstacked_proportions, mask=(vstacked_proportions==-1))\n",
    "            \n",
    "    ax.imshow(vstacked_proportions, cmap=custom_cmap, interpolation=\"none\", aspect=\"auto\", vmin=0,vmax=4)\n",
    "\n",
    "    for j in (\"top\",\"right\",\"bottom\",\"left\"):\n",
    "        ax.spines[j].set_visible(False)\n",
    "\n",
    "    for i in range(9):\n",
    "        ax.plot([0,max_vertex_count],[2.5+i,2.5+i], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\",alpha=.4)\n",
    "        ax.plot([0,max_vertex_count],[2.5+10+i,2.5+10+i], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\",alpha=.4)\n",
    "        if i < 4:\n",
    "            ax.plot([0,max_vertex_count],[2.5+20+i,2.5+20+i], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\",alpha=.4)\n",
    "        if i < 6:\n",
    "            ax.plot([0,max_vertex_count],[2.5+25+i,2.5+25+i], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\",alpha=.4)\n",
    "            ax.plot([0,max_vertex_count],[2.5+32+i,2.5+32+i], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\",alpha=.4)\n",
    "\n",
    "    ax.plot([0,max_vertex_count],[2.5,2.5], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\")\n",
    "    ax.plot([0,max_vertex_count],[2.5+9,2.5+9], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\")\n",
    "    ax.plot([0,max_vertex_count],[2.5+10,2.5+10], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\")\n",
    "    ax.plot([0,max_vertex_count],[2.5+19,2.5+19], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\")\n",
    "    ax.plot([0,max_vertex_count],[2.5+20,2.5+20], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\")\n",
    "    ax.plot([0,max_vertex_count],[2.5+24,2.5+24], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\")\n",
    "    ax.plot([0,max_vertex_count],[2.5+25,2.5+25], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\")\n",
    "    ax.plot([0,max_vertex_count],[2.5+31,2.5+31], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\")\n",
    "    ax.plot([0,max_vertex_count],[2.5+32,2.5+32], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\")\n",
    "    ax.plot([0,max_vertex_count],[2.5+38,2.5+38], c=\"k\", lw=LINEWIDTH, linestyle=\"dashed\")\n",
    "    ax.plot([0,0],[2.5,2.5+9],c='k',lw=LINEWIDTH,linestyle=\"dashed\")\n",
    "    ax.plot([max_vertex_count,max_vertex_count],[2.5,2.5+9],c='k',lw=LINEWIDTH,linestyle=\"dashed\")\n",
    "    ax.plot([0,0],[2.5+10,2.5+19],c='k',lw=LINEWIDTH,linestyle=\"dashed\")\n",
    "    ax.plot([max_vertex_count,max_vertex_count],[2.5+10,2.5+19],c='k',lw=LINEWIDTH,linestyle=\"dashed\")\n",
    "    ax.plot([0,0],[2.5+20,2.5+24],c='k',lw=LINEWIDTH,linestyle=\"dashed\")\n",
    "    ax.plot([max_vertex_count,max_vertex_count],[2.5+20,2.5+24],c='k',lw=LINEWIDTH,linestyle=\"dashed\")\n",
    "    ax.plot([0,0],[2.5+25,2.5+31],c='k',lw=LINEWIDTH,linestyle=\"dashed\")\n",
    "    ax.plot([max_vertex_count,max_vertex_count],[2.5+25,2.5+31],c='k',lw=LINEWIDTH,linestyle=\"dashed\")\n",
    "    ax.plot([0,0],[2.5+32,2.5+38],c='k',lw=LINEWIDTH,linestyle=\"dashed\")\n",
    "    ax.plot([max_vertex_count,max_vertex_count],[2.5+32,2.5+38],c='k',lw=LINEWIDTH,linestyle=\"dashed\")\n",
    "\n",
    "    #ax.grid(True)\n",
    "\n",
    "    yticks = [\n",
    "        0,1,2,3,4,5,6,7,8,\n",
    "        10,11,12,13,14,15,16,17,18,\n",
    "        20,21,22,23,\n",
    "        25,26,27,28,29,30,\n",
    "        32,33,34,35,36,37,\n",
    "    ]\n",
    "    ax.set_yticks([j+3 for j in yticks])\n",
    "    YTICKLABELS = [1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9,10,11,12,13,14,14,14,15,15,15,14,14,14,15,15,15]\n",
    "    ax.set_yticklabels([f\"{i:03}\" for i in YTICKLABELS], fontsize=FONTSIZE)\n",
    "    ax.tick_params(axis=\"y\",length=4, pad=2,width=LINEWIDTH)\n",
    "\n",
    "    ax.set_xticks([0, max_vertex_count])\n",
    "    ax.set_xticklabels([f\"{int(i)}\" for i in ax.get_xticks()], fontsize=FONTSIZE)\n",
    "    ax.tick_params(axis=\"x\",length=1, pad=2,width=LINEWIDTH)\n",
    "\n",
    "    ax.set_xlabel(\"Vertex count\", fontsize=FONTSIZE)\n",
    "\n",
    "    _title = text_dict[im_code]\n",
    "    ax.set_title(f\"{_title}\", fontsize=FONTSIZE,y=.92)\n",
    "\n",
    "    #print(im_code, min(vertex_counts), max(vertex_counts))\n",
    "\n",
    "    fig.savefig(MAIN / f\"Figure_X_{im_code}_composition.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(1,2),dpi=400,)\n",
    "FONTSIZE = 6\n",
    "offset = .1\n",
    "ax.set_xlim(0,10)\n",
    "ax.set_ylim(0,10)\n",
    "ax.text(3.,.2,\"Composition\", fontsize=FONTSIZE, ha=\"center\", va=\"center\")\n",
    "ax.scatter(1,6,s=40,c=roi_c_dict[\"f1\"],linewidths=.2,edgecolors='k',marker='s')\n",
    "ax.text(2,6-offset,text_dict[\"f1\"],fontsize=FONTSIZE,va=\"center\",ha=\"left\")\n",
    "ax.scatter(1,5,s=40,c=roi_c_dict[\"f2\"],linewidths=.2,edgecolors='k',marker='s')\n",
    "ax.text(2,5-offset,text_dict[\"f2\"],fontsize=FONTSIZE,va=\"center\",ha=\"left\")\n",
    "ax.scatter(1,4,s=40,c=roi_c_dict[\"f1f2\"],linewidths=.2,edgecolors='k',marker='s')\n",
    "ax.text(2,4-offset,\"Multiplex\",fontsize=FONTSIZE,va=\"center\",ha=\"left\")\n",
    "ax.scatter(1,3,s=40,c=\"cyan\",linewidths=.2,edgecolors='k',marker='s')\n",
    "ax.text(2,3-offset,\"$f_{IM}$ only\",fontsize=FONTSIZE,va=\"center\",ha=\"left\")\n",
    "for _spine in (\"left\",\"right\",\"bottom\",\"top\"):\n",
    "    ax.spines[_spine].set_visible(False)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "\n",
    "fig.savefig(MAIN / \"Figure_X_composition_schematic.png\", dpi=600, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

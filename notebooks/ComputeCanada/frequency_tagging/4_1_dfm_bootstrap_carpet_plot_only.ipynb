{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"ComputeCanada/frequency_tagging\")\n",
    "from utils import (\n",
    "    read_pkl, \n",
    "    find_quadrant_id_from_keys,\n",
    "    read_bootstrap_txt,\n",
    "    extract_carpet_data,\n",
    "    decorate_fig_carpetplot,\n",
    "    get_roi_colour_codes,\n",
    "    set_base_dir,\n",
    "    CARPET_PLOTS, SCRATCH_DIR, PICKLE_DIR,\n",
    "    TimeSeries\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create subject level carpet plots in `CARPET_PLOTS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "58\n",
      "58\n",
      "58\n",
      "58\n",
      "58\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "datadir = SCRATCH_DIR\n",
    "n_bootstraps = 400\n",
    "bootstrap_id = 0\n",
    "window_size = (39, 219)\n",
    "close_figures = True\n",
    "\n",
    "experiment_id = \"1_frequency_tagging\" \n",
    "normal_3T_sub_ids = [\"000\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\"] \n",
    "normal_7T_sub_ids = [\"Pilot001\", \"Pilot009\", \"Pilot010\", \"Pilot011\"]\n",
    "vary_sub_ids = [\"020\"]*3 + [\"021\"]*3\n",
    "vary_task_ids = [f\"entrain{i}\" for i in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]]\n",
    "\n",
    "sub_ids = normal_3T_sub_ids*2 + normal_7T_sub_ids + vary_sub_ids*3*2\n",
    "experiment_ids = [\"1_frequency_tagging\"]*len(normal_3T_sub_ids)*2 + [\"1_attention\"]*len(normal_7T_sub_ids) + [\"1_frequency_tagging\"]*len(vary_sub_ids)*3*2\n",
    "mri_ids = [\"3T\"]*len(normal_3T_sub_ids)*2 + [\"7T\"]*len(normal_7T_sub_ids) + [\"3T\"]*len(vary_sub_ids)*3 + [\"7T\"]*len(vary_sub_ids)*3\n",
    "roi_task_ids= [\"entrain\"]*len(normal_3T_sub_ids) + [\"entrain\"]*len(normal_3T_sub_ids) + ['AttendAway']*len(normal_7T_sub_ids) + ([\"entrainA\"]*3 + [\"entrainD\"]*3 + [\"entrainB\"]*3 + [\"entrainE\"]*3 + [\"entrainC\"]*3 + [\"entrainF\"]*3) * 2\n",
    "task_ids= [\"control\"]*len(normal_3T_sub_ids) + [\"entrain\"]*len(normal_3T_sub_ids) + ['AttendAway']*len(normal_7T_sub_ids) + vary_task_ids*3*2\n",
    "roi_frequencies = [[.125, .2]]*(len(normal_3T_sub_ids)*2+len(normal_7T_sub_ids)) + ([[.125,.2]]*3 + [[.125,.2]]*3 + [[.125,.175]]*3 + [[.15,.2]]*3 + [[.125,.15]]*3 + [[.175,.2]]*3) * 2\n",
    "task_frequencies = [[.125, .2]]*(len(normal_3T_sub_ids)*2+len(normal_7T_sub_ids)) + [[.125,.2],[.125,.175],[.125,.15],[.125,.2],[.15,.2],[.175,.2]]*3*2\n",
    "\n",
    "TR = .3\n",
    "fos = [.8]\n",
    "pvals = [\"uncp\"]\n",
    "fig_out_dir = Path(set_base_dir(str(CARPET_PLOTS)))\n",
    "stim_start = 14\n",
    "cmap = \"Greys_r\"\n",
    "vmin, vmax = -1.31, 1.31\n",
    "\n",
    "for i in [sub_ids, experiment_ids, mri_ids, roi_task_ids, task_ids, roi_frequencies, task_frequencies]:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /scratch/fastfmri/experiment-1_frequency_tagging_mri-3T_smooth-0_truncate-39-219_n-400_batch-merged_desc-basic_roi-entrain-0.125_pval-uncp_fo-0.8_bootstrap/sub-008/bootstrap/task-control_bootstrapped_data.pkl\n",
      "Reading: /scratch/fastfmri/experiment-1_frequency_tagging_mri-3T_smooth-0_truncate-39-219_n-400_batch-merged_desc-basic_roi-entrain-0.2_pval-uncp_fo-0.8_bootstrap/sub-008/bootstrap/task-control_bootstrapped_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app/notebooks/ComputeCanada/frequency_tagging/utils.py:1061: RuntimeWarning: invalid value encountered in divide\n",
      "  y_all = (( y_all - y_all.mean(0)) / y_all.std(0) ).T\n"
     ]
    }
   ],
   "source": [
    "for fo, pval in itertools.product(fos,pvals):\n",
    "        \n",
    "    for experiment_id, mri_id, sub_id, roi_task_id, task_id, frequencies, _task_frequencies in zip(\n",
    "        experiment_ids, mri_ids, sub_ids, roi_task_ids, task_ids, roi_frequencies, task_frequencies,\n",
    "    ):\n",
    "\n",
    "        f_data = {}\n",
    "        for roi_f in frequencies:\n",
    "            f_data[roi_f] = read_pkl(\n",
    "                datadir, \n",
    "                n_bootstraps, \n",
    "                sub_id, \n",
    "                roi_task_id, \n",
    "                roi_f, \n",
    "                task_id,\n",
    "                experiment_id=experiment_id,\n",
    "                mri_id=mri_id,\n",
    "                pval=pval,\n",
    "                fo=fo,\n",
    "            )\n",
    "\n",
    "        task_quadrant = find_quadrant_id_from_keys(f_data[frequencies[0]], task_id)\n",
    "                \n",
    "        assert frequencies[1]>frequencies[0]\n",
    "        f1_dict = f_data[frequencies[0]].copy()\n",
    "        f2_dict = f_data[frequencies[1]].copy()\n",
    "        f1_coords = f1_dict['roi_coords']\n",
    "        f2_coords = f2_dict['roi_coords']\n",
    "        f1_only_coords = f1_coords.astype(int) + f2_coords.astype(int)\n",
    "        f1_only_coords = f1_only_coords[f1_coords]\n",
    "        f2_only_coords = f1_coords.astype(int) + f2_coords.astype(int)\n",
    "        f2_only_coords = f2_only_coords[f2_coords]\n",
    "        # Masks\n",
    "        inter_from_f1 = f1_only_coords == 2\n",
    "        f1_from_f1 = f1_only_coords == 1\n",
    "        f2_from_f2 = f2_only_coords == 1\n",
    "        n_f1, n_f1f2, n_f2 = f1_from_f1.sum(), inter_from_f1.sum(), f2_from_f2.sum()\n",
    "\n",
    "        # Load data from nifti\n",
    "        # 1) untruncated, 2) preprocessed\n",
    "        bootstrap_txt = Path(f\"/scratch/fastfmri/experiment-{experiment_id}_mri-{mri_id}_smooth-0_truncate-{window_size[0]}-{window_size[1]}_n-25_batch-00_desc-basic_pval-{pval}_bootstrap/sub-{sub_id}/task-{task_id}{task_quadrant}_test_splits.txt\")\n",
    "        assert bootstrap_txt.exists(), f\"{bootstrap_txt} not found.\"\n",
    "        data_from_dtseries_raw, data_from_dtseries_windowed, data_from_dtseries_preprocessed = read_bootstrap_txt(bootstrap_txt, bootstrap_id) # Load single bootstrap\n",
    "\n",
    "        data_from_dtseries_raw = np.hstack(\n",
    "            [\n",
    "                data_from_dtseries_raw[f1_coords,:][f1_from_f1,:].T,\n",
    "                data_from_dtseries_raw[f1_coords,:][inter_from_f1,:].T,\n",
    "                data_from_dtseries_raw[f2_coords,:][f2_from_f2,:].T,\n",
    "            ]\n",
    "        )\n",
    "        data_from_dtseries_windowed = np.hstack(\n",
    "            [\n",
    "                data_from_dtseries_windowed[f1_coords,1:][f1_from_f1,:].T,\n",
    "                data_from_dtseries_windowed[f1_coords,1:][inter_from_f1,:].T,\n",
    "                data_from_dtseries_windowed[f2_coords,1:][f2_from_f2,:].T,\n",
    "            ]\n",
    "        )\n",
    "        data_from_dtseries_preprocessed = np.hstack(\n",
    "            [\n",
    "                data_from_dtseries_preprocessed[f1_coords,1:][f1_from_f1,:].T,\n",
    "                data_from_dtseries_preprocessed[f1_coords,1:][inter_from_f1,:].T,\n",
    "                data_from_dtseries_preprocessed[f2_coords,1:][f2_from_f2,:].T,\n",
    "            ]\n",
    "        )\n",
    "        # Load data from pickle\n",
    "        # 3) preprocessed, 4) preprocessed & phased\n",
    "        _, f1_data_from_pkl_preprocessed = extract_carpet_data(f1_dict, task_id, task_quadrant, bootstrap_id, False)\n",
    "        f1_phased_tps, f1_data_from_pkl_preprocessed_phased = extract_carpet_data(f1_dict, task_id, task_quadrant, bootstrap_id, True)\n",
    "        _, f2_data_from_pkl_preprocessed = extract_carpet_data(f2_dict, task_id, task_quadrant, bootstrap_id, False)\n",
    "        f2_phased_tps, f2_data_from_pkl_preprocessed_phased = extract_carpet_data(f2_dict, task_id, task_quadrant, bootstrap_id, True)\n",
    "        intersected_phased_tps = [i for i in set(f1_phased_tps).intersection(f2_phased_tps)]\n",
    "        f1_phased_tp_mask = [tp in intersected_phased_tps for tp in f1_phased_tps]\n",
    "        f2_phased_tp_mask = [tp in intersected_phased_tps for tp in f2_phased_tps]\n",
    "\n",
    "        data_from_pkl_preprocessed = np.hstack(\n",
    "            [\n",
    "                f1_data_from_pkl_preprocessed[:,f1_from_f1],\n",
    "                f1_data_from_pkl_preprocessed[:,inter_from_f1],\n",
    "                f2_data_from_pkl_preprocessed[:,f2_from_f2],\n",
    "            ]\n",
    "        )\n",
    "        data_from_pkl_preprocessed_phased = np.hstack(\n",
    "            [\n",
    "                f1_data_from_pkl_preprocessed_phased[:,f1_from_f1][f1_phased_tp_mask,:],\n",
    "                f1_data_from_pkl_preprocessed_phased[:,inter_from_f1][f1_phased_tp_mask,:],\n",
    "                f2_data_from_pkl_preprocessed_phased[:,f2_from_f2][f2_phased_tp_mask,:],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        ts_labels = [\n",
    "            \"raw\", \"windowed\", \"denoised\", \"denoised_rephased\"\n",
    "        ]\n",
    "        ts_data = [\n",
    "            data_from_dtseries_raw, \n",
    "            data_from_dtseries_windowed,\n",
    "            data_from_dtseries_preprocessed,\n",
    "            #data_from_pkl_preprocessed, \n",
    "            data_from_pkl_preprocessed_phased,\n",
    "        ]\n",
    "\n",
    "        # Get sorting order based on `data_from_dtseries_preprocessed`\n",
    "        # Sort for each set of vertices: f1, f1f2, and f2 (this is the order that the reoriented data)\n",
    "        # Note: `data_from_dtseries_preprocessed` == `data_from_pkl_preprocessed`\n",
    "\n",
    "        # This will error out if there is any of f1, f2, or f1f2 has 0 vertices.. I THINK?\n",
    "        y = data_from_dtseries_preprocessed.copy()\n",
    "        y = (( y - y.mean(0)) / y.std(0) ).T\n",
    "        sorted_voxels = {}\n",
    "        y_f1 = y[:n_f1,:].copy()\n",
    "        y_f1f2 = y[n_f1:n_f1+n_f1f2,:].copy()\n",
    "        y_f2 = y[n_f1+n_f1f2:,:].copy()\n",
    "        for f_group, y in zip([\"f1\",\"f1f2\",\"f2\"], [y_f1, y_f1f2, y_f2]):\n",
    "            C = np.corrcoef(y)\n",
    "            correlation_strength = np.abs(C).sum(axis=1)\n",
    "            sorted_voxels[f_group] = np.argsort(correlation_strength)[::-1]\n",
    "\n",
    "        for y_ix, (y_label, y) in enumerate(zip(ts_labels, ts_data)):\n",
    "            \n",
    "            fig, ax = plt.subplots(\n",
    "                nrows=1,ncols=1, figsize=(2.,1.2), dpi=300,\n",
    "                #gridspec_kw=dict(height_ratios=[286, 119]),\n",
    "            )\n",
    "            \n",
    "            y = (( y - y.mean(0)) / y.std(0) ).T\n",
    "\n",
    "            # Sort\n",
    "            y[:n_f1, :] = y[:n_f1,:][sorted_voxels[\"f1\"],:]\n",
    "            y[n_f1:n_f1+n_f1f2, :] = y[n_f1:n_f1+n_f1f2,:][sorted_voxels[\"f1f2\"],:]\n",
    "            y[n_f1+n_f1f2:, :] = y[n_f1+n_f1f2:,:][sorted_voxels[\"f2\"],:]\n",
    "\n",
    "            im = ax.imshow(y, cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto')\n",
    "            if y_ix == 0:\n",
    "                for i in window_size:\n",
    "                    ax.plot([i/TR]*2,[0,n_f1+n_f2+n_f1f2], color='orange', linestyle='-', linewidth=1)\n",
    "                ax.plot([stim_start/TR]*2,[0,n_f1+n_f2+n_f1f2], color='green', linestyle='dotted', linewidth=1)\n",
    "            fig, ax = decorate_fig_carpetplot(y,fig, ax, im, _task_frequencies[0], _task_frequencies[1], n_f1, n_f2, n_f1f2, FONTSIZE=8, TR=TR)\n",
    "\n",
    "            fig.tight_layout()\n",
    "\n",
    "            png_out = fig_out_dir / f\"experiment-{experiment_id}_mri-{mri_id}_sub-{sub_id}_task-{roi_task_id}_task-{task_id}_pval-{pval}_fo-{fo}_{y_ix}{y_label}.png\"\n",
    "            fig.savefig(png_out, dpi='figure')\n",
    "            \n",
    "            #if y_label == \"denoised\" and task_id == \"entrain\":\n",
    "                #import pdb; pdb.set_trace()\n",
    "\n",
    "            if close_figures:\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "        clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

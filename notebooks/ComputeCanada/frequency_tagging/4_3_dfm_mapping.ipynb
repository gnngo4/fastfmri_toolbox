{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app/notebooks/font_library/aptos-extrabold.ttf\n",
      "/opt/app/notebooks/font_library/aptos-black-italic.ttf\n",
      "/opt/app/notebooks/font_library/aptos-italic.ttf\n",
      "/opt/app/notebooks/font_library/aptos-light-italic.ttf\n",
      "/opt/app/notebooks/font_library/aptos.ttf\n",
      "/opt/app/notebooks/font_library/aptos-light.ttf\n",
      "/opt/app/notebooks/font_library/aptos-extrabold-italic 2.ttf\n",
      "/opt/app/notebooks/font_library/aptos-black.ttf\n",
      "/opt/app/notebooks/font_library/aptos-semibold.ttf\n",
      "/opt/app/notebooks/font_library/aptos-bold.ttf\n",
      "/opt/app/notebooks/font_library/aptos-extrabold-italic.ttf\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/opt/wbplot\")\n",
    "\n",
    "from wbplot import dscalar\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "\n",
    "sys.path.append(\"ComputeCanada/frequency_tagging\")\n",
    "from dfm import (\n",
    "    get_roi_colour_codes,\n",
    "    change_font,\n",
    ")\n",
    "change_font()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get HCP info\n",
    "- `hcp_mappings`: dict of ROI: dscalars\n",
    "- `hcp_rois`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get HCP labels\n",
    "\"\"\"\n",
    "dlabel_dir = Path(\"/opt/app/notebooks/data/dlabels\")\n",
    "hcp_label = dlabel_dir / \"Q1-Q6_RelatedValidation210.CorticalAreas_dil_Final_Final_Areas_Group_Colors.32k_fs_LR.dlabel.nii\"\n",
    "\n",
    "_HCP_INFO = !wb_command -file-information {hcp_label}\n",
    "HCP_LABELS = []\n",
    "HCP_COUNTER = 0\n",
    "for i in _HCP_INFO:\n",
    "    if len(i) == 60 and any([\"L_\" in i, \"R_\" in i]):\n",
    "        hcp_colors = tuple([float(f\"0.{k}\") for k in [j.split(' ') [0] for j in i.split('0.')][-3:]] + [1])\n",
    "        if ' R_' in i:\n",
    "            roi = i.split(\"_ROI\")[0].split(' R_')[1]\n",
    "            HCP_LABELS.append(f\"R_{roi}_ROI\")\n",
    "        if ' L_' in i:\n",
    "            roi = i.split(\"_ROI\")[0].split(' L_')[1]\n",
    "            HCP_LABELS.append(f\"L_{roi}_ROI\")\n",
    "        HCP_COUNTER += 1\n",
    "\n",
    "\"\"\"Get HCP label coordinates\n",
    "\"\"\"\n",
    "dscalar_dir = Path(\"/opt/app/notebooks/data/dscalars\")\n",
    "tmpdir = Path(\"/tmp\")\n",
    "template_dscalar = dscalar_dir / \"S1200.MyelinMap_BC_MSMAll.32k_fs_LR.dscalar.nii\"\n",
    "\n",
    "hcp_mapping = {}\n",
    "for roi_label in HCP_LABELS:\n",
    "    out_dscalar = tmpdir / f\"{roi_label}.dscalar.nii\"\n",
    "    if out_dscalar.exists():\n",
    "        hcp_mapping[roi_label] = out_dscalar\n",
    "        continue\n",
    "    !wb_command -cifti-label-to-roi {hcp_label} {out_dscalar} -name {roi_label}\n",
    "    assert out_dscalar.exists(), f\"{out_dscalar.stem} does not exist.\"\n",
    "    hcp_mapping[roi_label] = out_dscalar\n",
    "hcp_rois = list(set([k.split('_')[1] for k in hcp_mapping.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_fractional_overlap(data):\n",
    "\n",
    "    return data.sum(0) / data.shape[0]\n",
    "\n",
    "def map_data_to_value(data_list):\n",
    "\n",
    "    for ix, (k,v) in enumerate(data_list):\n",
    "\n",
    "        if ix == 0:\n",
    "            new_data = k.copy() * v\n",
    "        else:\n",
    "            new_data += k * v\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def combine_f1_f2(f1,f2,fo=1.,mask=None, f1_c=.01, f2_c=.82, f1f2_c=.28, mask_c=.01):\n",
    "    f1_data = convert_to_fractional_overlap(nib.load(f1).get_fdata())\n",
    "    f2_data = convert_to_fractional_overlap(nib.load(f2).get_fdata())\n",
    "    f1_data = (f1_data >= fo).astype(int)\n",
    "    f2_data = (f2_data >= fo).astype(int)\n",
    "    f1f2_data = ((f1_data + f2_data) == 2).astype(int)\n",
    "    f1_data -= f1f2_data\n",
    "    f2_data -= f1f2_data\n",
    "    if mask:\n",
    "        mask_data = convert_to_fractional_overlap(nib.load(mask).get_fdata())\n",
    "        mask_data = (mask_data >= 1.).astype(int)\n",
    "        mask_data -= f1f2_data\n",
    "        mask_data -= f1_data\n",
    "        mask_data -= f2_data\n",
    "    data_dict = [(f1_data, f1_c), (f2_data, f2_c), (f1f2_data, f1f2_c)]\n",
    "    if mask:\n",
    "        data_dict.append((mask_data,mask_c))\n",
    "\n",
    "    return map_data_to_value(data_dict)\n",
    "\n",
    "def get_quadrant_id(mask_path):\n",
    "    rel_mask_path = mask_path.split(\"/\")[-1]\n",
    "    idx_start = rel_mask_path.find(\"Q\")\n",
    "    quadrant_id = rel_mask_path[idx_start:idx_start+2]\n",
    "    assert quadrant_id in ['Q1', 'Q2'], f\"{quadrant_id} not Q1 or Q2\"\n",
    "\n",
    "    return quadrant_id\n",
    "\n",
    "def merge_and_binarize_mask(data, f1_c, f2_c, f1f2_c, mask_c):\n",
    "    data_dict = {\n",
    "        \"f1\": data.copy(),\n",
    "        \"f2\": data.copy(),\n",
    "        \"f1Uf2\": data.copy(),\n",
    "    }\n",
    "    data_dict[\"f1\"][(data_dict[\"f1\"]==f1_c) | (data_dict[\"f1\"]==f1f2_c)] = 1\n",
    "    data_dict[\"f1\"][(data_dict[\"f1\"]==f2_c)] = 0\n",
    "    data_dict[\"f2\"][(data_dict[\"f2\"]==f2_c) | (data_dict[\"f2\"]==f1f2_c)] = 1\n",
    "    data_dict[\"f2\"][(data_dict[\"f2\"]==f1_c)] = 0\n",
    "    data_dict[\"f1Uf2\"][(data_dict[\"f1Uf2\"]==f1f2_c)] = 1\n",
    "    data_dict[\"f1Uf2\"][(data_dict[\"f1Uf2\"]==f1_c) | (data_dict[\"f1Uf2\"]==f2_c)] = 0\n",
    "    for v in data_dict.values():\n",
    "        v[v==mask_c] = 0\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "@lru_cache(maxsize=360)\n",
    "def read_roi_path(roi_path):\n",
    "    return nib.load(roi_path).get_fdata()[0,:]\n",
    "\n",
    "def append_data(\n",
    "    df_data,\n",
    "    hcp_mapping,\n",
    "    map_data,\n",
    "    power_f1_data,\n",
    "    power_f2_data,\n",
    "    pd_f1_data, \n",
    "    pd_f2_data, \n",
    "    q_id,\n",
    "    experiment_label, \n",
    "    sub_id, \n",
    "    roi_fo,\n",
    "    roi_task_id,\n",
    "    task_id,\n",
    "):\n",
    "    \"\"\"Create function to store vertex level data for each HCP ROI:\n",
    "    - columns: [cohort_id, sub_ids, quadrant_id, hcp_roi, frequency_of_roi, vertex_count, vertex_coordinates, f1_BOLD_power, f2_BOLD_power, f1_phase_delay, f2_phase_delay]\n",
    "        - roi_fo = region fractional overlap threshold\n",
    "        - cohort_id = cohort_id of each dataset [3T/7T Normal/Vary]\n",
    "            - sub_ids = sub_id of all ROIs in cohort\n",
    "                - quadrant_id = each subject will have a quadrant_id (corresponding to quadrant stimulation)\n",
    "                - hcp_roi = all HCP ROIs, convert L/R to express laterality\n",
    "                    - CONTRA/IPSI\n",
    "                    - frequency_of_roi = each `hcp_roi` will have a ROI corresponding to f1, f2 or both (f1Uf2)\n",
    "                        - vertex_count = each `frequency_of_roi` will have a vertex_count\n",
    "                        - vertex_coordinates = each `frequency_of_roi` will have coordinates to all its vertices\n",
    "                        - f1_BOLD_power = each `frequency_of_roi` will have a np.array of power values corresponding to each vertex\n",
    "                        - f2_BOLD_power = each `frequency_of_roi` will have a np.array of power values corresponding to each vertex\n",
    "                        - f1_phase_delay = each `frequency_of_roi` will have a np.array of phase delay values corresponding to each vertex\n",
    "                        - f2_phase_delay = each `frequency_of_roi` will have a np.array of phase delay values corresponding to each vertex\n",
    "    \"\"\"\n",
    "    for frequency_of_roi, f_data in map_data.items():\n",
    "        for roi_label, roi_path in hcp_mapping.items():\n",
    "            if q_id == \"Q1\":\n",
    "                contra = \"L_\"\n",
    "            elif q_id == \"Q2\":\n",
    "                contra = \"R_\"\n",
    "            else:\n",
    "                raise ValueError(f\"{q_id} not supported.\")\n",
    "\n",
    "            if roi_label.startswith(contra):\n",
    "                roi_label = f\"CONTRA_{roi_label[2:-4]}\"\n",
    "            else:\n",
    "                roi_label = f\"IPSI_{roi_label[2:-4]}\"\n",
    "\n",
    "            roi_mask = read_roi_path(roi_path)\n",
    "            assert roi_mask.shape == f_data.shape\n",
    "\n",
    "            hcp_and_f_roi = roi_mask * f_data\n",
    "            vertex_coordinates = np.where(hcp_and_f_roi == 1)\n",
    "            vertex_count = hcp_and_f_roi.sum()\n",
    "            if vertex_count == 0:\n",
    "                continue\n",
    "\n",
    "            if roi_task_id == \"control\":\n",
    "                f1_BOLD_power = None\n",
    "                f2_BOLD_power = None\n",
    "            else:\n",
    "                f1_BOLD_power = power_f1_data[hcp_and_f_roi==1]\n",
    "                f2_BOLD_power = power_f2_data[hcp_and_f_roi==1]\n",
    "            if task_id != roi_task_id:\n",
    "                f1_phase_delay = None\n",
    "                f2_phase_delay = None\n",
    "            else:\n",
    "                f1_phase_delay = pd_f1_data[hcp_and_f_roi==1]\n",
    "                f2_phase_delay = pd_f2_data[hcp_and_f_roi==1]\n",
    "\n",
    "            df_data[\"roi_task_id\"].append(roi_task_id)\n",
    "            df_data[\"task_id\"].append(task_id)\n",
    "            df_data[\"roi_fo\"].append(roi_fo)\n",
    "            df_data[\"experiment_id\"].append(experiment_label)\n",
    "            df_data[\"sub_id\"].append(sub_id)\n",
    "            df_data[\"quadrant_id\"].append(q_id)\n",
    "            df_data[\"hcp_roi\"].append(roi_label)\n",
    "            df_data[\"frequency_of_roi\"].append(frequency_of_roi)\n",
    "            df_data[\"vertex_count\"].append(vertex_count)\n",
    "            df_data[\"vertex_coordinates\"].append(vertex_coordinates)\n",
    "            df_data[\"f1_BOLD_power\"].append(f1_BOLD_power)\n",
    "            df_data[\"f2_BOLD_power\"].append(f2_BOLD_power)\n",
    "            df_data[\"f1_phase_delay\"].append(f1_phase_delay)\n",
    "            df_data[\"f2_phase_delay\"].append(f2_phase_delay)\n",
    "\n",
    "    return df_data\n",
    "\n",
    "def contains_all_strings(input_str, string_list):\n",
    "    for string in string_list:\n",
    "        if string not in input_str:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_activations(experiment_id, mri_id, roi_task_id, roi_f_1, fo, sub_id, data_split_id=\"train\", match_str=\"activations.dtseries.nii\", additional_match_strs=None, additional_match_str=None, corr_type=\"uncp\"):\n",
    "    import os\n",
    "    directory = f\"/scratch/fastfmri/experiment-{experiment_id}_mri-{mri_id}_smooth-0_truncate-39-219_n-200_batch-merged_desc-basic_roi-{roi_task_id}-{roi_f_1}_pval-{corr_type}_fo-{fo}_bootstrap/sub-{sub_id}/bootstrap/\"\n",
    "    if additional_match_strs is not None:\n",
    "        match_str = additional_match_strs + [match_str, f\"data-{data_split_id}\"]\n",
    "        activations_files = []\n",
    "        for file in os.listdir(directory):\n",
    "            if contains_all_strings(file, match_str):\n",
    "                activations_files.append(file)\n",
    "    else:\n",
    "        activations_files = [file for file in os.listdir(directory) if f'data-{data_split_id}' in file and match_str in file]\n",
    "\n",
    "    return [f\"{directory}{i}\" for i in activations_files]\n",
    "\n",
    "def set_base_dir(basedir):\n",
    "    basedir = Path(basedir)\n",
    "    if not basedir.exists():\n",
    "        basedir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    return basedir\n",
    "\n",
    "def load_mean_dtseries(dtseries):\n",
    "    mean_power = nib.load(dtseries).get_fdata().mean(0)\n",
    "    return mean_power\n",
    "\n",
    "def generate_single_subject_maps(\n",
    "    label, experiment_id, mri_id, sub_ids, \n",
    "    roi_task_ids, roi_f_1s, roi_f_2s, roi_fo,\n",
    "    df_data=None,\n",
    "    corr_type=\"uncp\",\n",
    "    ROI_FO=.8, SUB_THRESHOLD=.5,\n",
    "    LEFT=590, TOP=80, RIGHT=1140, BOTTOM=460, VERTEX_TO = 59412,\n",
    "    FORCE_TASK_ID=None,\n",
    "):\n",
    "\n",
    "    if df_data is None:\n",
    "        df_data = defaultdict(list)\n",
    "\n",
    "    for ix, (sub_id, roi_task_id, roi_f_1, roi_f_2) in enumerate(zip(\n",
    "        sub_ids,\n",
    "        roi_task_ids, \n",
    "        roi_f_1s,\n",
    "        roi_f_2s,\n",
    "    )):\n",
    "\n",
    "        if FORCE_TASK_ID is None:\n",
    "            _roi_task_id = roi_task_id\n",
    "        else:\n",
    "            _roi_task_id = FORCE_TASK_ID\n",
    "\n",
    "        png_out = Path(set_base_dir(f\"./ComputeCanada/frequency_tagging/figures/dual_frequency_mapping\")) / f\"label-{label}_mri-{mri_id}_sub-{sub_id}_task-{roi_task_id}_f-{roi_f_1}-{roi_f_2}_corr-{corr_type}_fo-{ROI_FO}.png\"\n",
    "        dscalar_out = Path(set_base_dir(f\"./ComputeCanada/frequency_tagging/figures/dual_frequency_mapping_cifti\")) / f\"label-{label}_mri-{mri_id}_sub-{sub_id}_task-{roi_task_id}_f-{roi_f_1}-{roi_f_2}_corr-{corr_type}_fo-{ROI_FO}.dtseries.nii\"\n",
    "        if png_out.exists():\n",
    "            #continue\n",
    "            pass\n",
    "\n",
    "        f1 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_1, .8, sub_id, match_str=\"activations.dtseries.nii\", corr_type=corr_type)\n",
    "        f2 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_2, .8, sub_id, match_str=\"activations.dtseries.nii\", corr_type=corr_type)\n",
    "        mask = find_activations(experiment_id, mri_id, roi_task_id, roi_f_1, .8, sub_id, match_str=\"mask.dtseries.nii\", corr_type=corr_type)\n",
    "        pd_f1 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_1, .8, sub_id, data_split_id = \"train\", match_str=\"phasedelay.dtseries.nii\", additional_match_strs=[roi_task_id,f\"f-{roi_f_1}\"], corr_type=corr_type)\n",
    "        pd_f2 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_2, .8, sub_id, data_split_id = \"train\", match_str=\"phasedelay.dtseries.nii\", additional_match_strs=[roi_task_id,f\"f-{roi_f_2}\"], corr_type=corr_type)\n",
    "        power_f1 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_1, .8, sub_id, data_split_id = \"test\", match_str=\"power.dtseries.nii\", additional_match_strs=[_roi_task_id,f\"f-{roi_f_1}\"], corr_type=corr_type)\n",
    "        power_f2 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_2, .8, sub_id, data_split_id = \"test\", match_str=\"power.dtseries.nii\", additional_match_strs=[_roi_task_id,f\"f-{roi_f_2}\"], corr_type=corr_type)\n",
    "        for f_label, f in zip([\"f1\",\"f2\",\"mask\",\"pd_f1\",\"pd_f2\",\"power_f1\",\"power_f2\"], [f1,f2, mask, pd_f1, pd_f2, power_f1, power_f2]):\n",
    "            if roi_task_id == \"control\" and experiment_id == \"1_frequency_tagging\":\n",
    "                if f_label in [\"f1\", \"f2\", \"mask\"]:\n",
    "                    assert len(f) == 1, f\"{sub_id}, {f_label} - {f}\"\n",
    "            else:\n",
    "                assert len(f) == 1, f\"{sub_id}, {f_label} - {f}, {experiment_id} {roi_task_id}\"\n",
    "\n",
    "        f1, f2 = f1[0], f2[0]\n",
    "        data = combine_f1_f2(f1, f2, fo=ROI_FO, mask=mask[0], f1_c=f1_c,f2_c=f2_c,f1f2_c=f1f2_c,mask_c=mask_c)\n",
    "        data = data[:VERTEX_TO]\n",
    "\n",
    "        map_data = merge_and_binarize_mask(data,f1_c,f2_c,f1f2_c,mask_c)\n",
    "        pd_f1_data = load_mean_dtseries(pd_f1[0])[:VERTEX_TO]\n",
    "        pd_f2_data = load_mean_dtseries(pd_f2[0])[:VERTEX_TO]\n",
    "        if roi_task_id == \"control\" and experiment_id == \"1_frequency_tagging\":\n",
    "            power_f1_data = None\n",
    "            power_f2_data = None\n",
    "        # Power metrics were not calculated for control task condition (no voxels allocated to task-control ROIs)\n",
    "        else:\n",
    "            power_f1_data = load_mean_dtseries(power_f1[0])[:VERTEX_TO]\n",
    "            power_f2_data = load_mean_dtseries(power_f2[0])[:VERTEX_TO]\n",
    "        q_id = get_quadrant_id(mask[0])\n",
    "        df_data = append_data(\n",
    "            df_data, \n",
    "            hcp_mapping, \n",
    "            map_data, \n",
    "            power_f1_data, \n",
    "            power_f2_data, \n",
    "            pd_f1_data, \n",
    "            pd_f2_data, \n",
    "            q_id,\n",
    "            label, \n",
    "            sub_id,\n",
    "            roi_fo,\n",
    "            roi_task_id,\n",
    "            _roi_task_id, \n",
    "        )\n",
    "\n",
    "        palette_params = {\n",
    "            \"disp-zero\": False,\n",
    "            \"disp-neg\": True,\n",
    "            \"disp-pos\": True,\n",
    "            \"pos-user\": (0, 1.),\n",
    "            \"neg-user\": (-1,0),\n",
    "            \"interpolate\": True,\n",
    "        }\n",
    "        # Save f1f2 map as dtseries\n",
    "        f1_img = nib.load(f1)\n",
    "        dscalar_to_save_as_cifti = np.zeros((1,f1_img.shape[-1]))\n",
    "        dscalar_to_save_as_cifti[0,:VERTEX_TO] = data\n",
    "        f1f2_img = nib.Cifti2Image(dscalar_to_save_as_cifti, header=f1_img.header)\n",
    "        f1f2_img.header.matrix[0].number_of_series_points = 1\n",
    "        nib.save(f1f2_img, dscalar_out)\n",
    "        dscalar(\n",
    "            png_out, data, \n",
    "            orientation=\"portrait\", \n",
    "            hemisphere='right',\n",
    "            palette=PALETTE, \n",
    "            palette_params=palette_params,\n",
    "            transparent=False,\n",
    "            flatmap=True,\n",
    "            flatmap_style='plain',\n",
    "        )\n",
    "        crop_and_save(png_out, str(png_out).replace(\"png\", \"cropped.png\"), LEFT, TOP, RIGHT, BOTTOM)\n",
    "        \n",
    "        track = [len(v) for k,v in df_data.items()]\n",
    "        print(track)\n",
    "\n",
    "    return df_data\n",
    "\n",
    "def crop_and_save(input_file, output_file, left, top, right, bottom):\n",
    "    from PIL import Image\n",
    "    try:\n",
    "        # Open the input image\n",
    "        with Image.open(input_file) as img:\n",
    "            # Crop the image\n",
    "            cropped_img = img.crop((left, top, right, bottom))\n",
    "            # Save the cropped image\n",
    "            cropped_img.save(output_file)\n",
    "            print(\"Cropped image saved successfully as\", output_file)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save visualizations and create DataFrame storing data for simple analysis\n",
    "- Datasets\n",
    "    - `3TNormal` ($f_1$=.125, $f_2$=.2)\n",
    "    - `7TNormal` ($f_1$=.125, $f_2$=.2)\n",
    "    - `3TVary` varying frequencies \n",
    "    - `7TVary` varying frequencies\n",
    "- Region info `df.frequency_of_roi` and `df.hcp_roi`\n",
    "    - Region data of $f_1$ *include* $f_1$&$f_2$ intersected vertices, same goes for $f_2$\n",
    "    - roi choice includes `[\"f1\",\"f2\",\"f1Uf2\"]`, where `f1Uf2` denotes intersected vertices\n",
    "- Data includes\n",
    "    - Note: to load `fdrp` (from `uncp`) corrected data change the `corr_type` variable in the cell below\n",
    "    - `df.roi_task_id` task used to generate the ROI\n",
    "    - `df.roi_fo` fractional overlap used to generate the ROI\n",
    "    - ...\n",
    "    - `df.hcp_roi` HCP ROI used to filter data from\n",
    "    - `df.frequency_of_roi` frequency of ROI used to filter data from (related to the frequency of `df.roi_task_id`)\n",
    "    - `df.vertex_count` total vertices in the HCP ROI & identified with the frequency of the task\n",
    "    - `df.vertex_coordinates` coordinates on a 32k_fs_LR surface (consistent with file structure of `template_dscalar`)\n",
    "    - `df.f1_[BOLD_power,phase_delay]` vertex-wise power extracted from $f_1$ region (this is set to 0 for $f_2$-only regions)\n",
    "    - `df.f2_[BOLD_power,phase_delay]` vertex-wise power extracted from $f_2$ region (this is set to 0 for $f_1$-only regions)\n",
    "        - `f1Uf2` regions contain both $f_1$ and $f_2$ metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m         roi_f_1s \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m.125\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(sub_ids)\n\u001b[1;32m     25\u001b[0m         roi_f_2s \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m.2\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(sub_ids)\n\u001b[0;32m---> 26\u001b[0m         df_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_single_subject_maps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmri_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mroi_task_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroi_f_1s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroi_f_2s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mROI_FO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdf_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcorr_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorr_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mROI_FO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mROI_FO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSUB_THRESHOLD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mFORCE_TASK_ID\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontrol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 3T control under entrain condition (set this to get power measurements with entrain ROIs)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3TNormal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 239\u001b[0m, in \u001b[0;36mgenerate_single_subject_maps\u001b[0;34m(label, experiment_id, mri_id, sub_ids, roi_task_ids, roi_f_1s, roi_f_2s, roi_fo, df_data, corr_type, ROI_FO, SUB_THRESHOLD, LEFT, TOP, RIGHT, BOTTOM, VERTEX_TO, FORCE_TASK_ID)\u001b[0m\n\u001b[1;32m    237\u001b[0m     power_f2_data \u001b[38;5;241m=\u001b[39m load_mean_dtseries(power_f2[\u001b[38;5;241m0\u001b[39m])[:VERTEX_TO]\n\u001b[1;32m    238\u001b[0m q_id \u001b[38;5;241m=\u001b[39m get_quadrant_id(mask[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 239\u001b[0m df_data \u001b[38;5;241m=\u001b[39m \u001b[43mappend_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhcp_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpower_f1_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpower_f2_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpd_f1_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpd_f2_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43msub_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroi_fo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroi_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_roi_task_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m palette_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisp-zero\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisp-neg\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterpolate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    262\u001b[0m }\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Save f1f2 map as dtseries\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 110\u001b[0m, in \u001b[0;36mappend_data\u001b[0;34m(df_data, hcp_mapping, map_data, power_f1_data, power_f2_data, pd_f1_data, pd_f2_data, q_id, experiment_label, sub_id, roi_fo, roi_task_id, task_id)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     roi_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPSI_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroi_label[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 110\u001b[0m roi_mask \u001b[38;5;241m=\u001b[39m \u001b[43mread_roi_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m roi_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m f_data\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    113\u001b[0m hcp_and_f_roi \u001b[38;5;241m=\u001b[39m roi_mask \u001b[38;5;241m*\u001b[39m f_data\n",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m, in \u001b[0;36mread_roi_path\u001b[0;34m(roi_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m360\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_roi_path\u001b[39m(roi_path):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_fdata()[\u001b[38;5;241m0\u001b[39m,:]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nibabel/loadsave.py:110\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     is_valid, sniff \u001b[38;5;241m=\u001b[39m image_klass\u001b[38;5;241m.\u001b[39mpath_maybe_image(filename, sniff)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid:\n\u001b[0;32m--> 110\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mimage_klass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[1;32m    113\u001b[0m matches, msg \u001b[38;5;241m=\u001b[39m _signature_matches_extension(filename)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nibabel/dataobj_images.py:503\u001b[0m, in \u001b[0;36mDataobjImage.from_filename\u001b[0;34m(klass, filename, mmap, keep_file_open)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap should be one of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mTrue, False, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    502\u001b[0m file_map \u001b[38;5;241m=\u001b[39m klass\u001b[38;5;241m.\u001b[39mfilespec_to_file_map(filename)\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_file_open\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_file_open\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nibabel/cifti2/cifti2.py:1505\u001b[0m, in \u001b[0;36mCifti2Image.from_file_map\u001b[0;34m(klass, file_map, mmap, keep_file_open)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Construct cifti image.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# Use array proxy object where possible\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m dataobj \u001b[38;5;241m=\u001b[39m nifti_img\u001b[38;5;241m.\u001b[39mdataobj\n\u001b[0;32m-> 1505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCifti2Image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreshape_dataobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcifti_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnifti_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnifti_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nibabel/cifti2/cifti2.py:1465\u001b[0m, in \u001b[0;36mCifti2Image.__init__\u001b[0;34m(self, dataobj, header, nifti_header, extra, file_map, dtype)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_data_dtype(dataobj\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_headers()\n\u001b[0;32m-> 1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataobj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1466\u001b[0m     warn(\n\u001b[1;32m   1467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataobj shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataobj\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match shape \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected from CIFTI-2 header \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mmatrix\u001b[38;5;241m.\u001b[39mget_data_shape()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1469\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nibabel/cifti2/cifti2.py:1309\u001b[0m, in \u001b[0;36mCifti2Matrix.get_data_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1307\u001b[0m base_shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapped_indices) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m-> 1309\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mcifti2_axes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_index_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmim\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m mim\u001b[38;5;241m.\u001b[39mapplies_to_matrix_dimension:\n\u001b[1;32m   1311\u001b[0m         base_shape[idx] \u001b[38;5;241m=\u001b[39m size\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nibabel/cifti2/cifti2_axes.py:148\u001b[0m, in \u001b[0;36mfrom_index_mapping\u001b[0;34m(mim)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03mParses the MatrixIndicesMap to find the appropriate CIFTI-2 axis describing the rows or columns\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03maxis : subclass of :class:`Axis`\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m return_type \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFTI_INDEX_TYPE_SCALARS\u001b[39m\u001b[38;5;124m'\u001b[39m: ScalarAxis,\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFTI_INDEX_TYPE_LABELS\u001b[39m\u001b[38;5;124m'\u001b[39m: LabelAxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFTI_INDEX_TYPE_PARCELS\u001b[39m\u001b[38;5;124m'\u001b[39m: ParcelsAxis,\n\u001b[1;32m    147\u001b[0m }\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreturn_type\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices_map_to_data_type\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_index_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nibabel/cifti2/cifti2_axes.py:431\u001b[0m, in \u001b[0;36mBrainModelAxis.from_index_mapping\u001b[0;34m(cls, mim)\u001b[0m\n\u001b[1;32m    429\u001b[0m             shape \u001b[38;5;241m=\u001b[39m mim\u001b[38;5;241m.\u001b[39mvolume\u001b[38;5;241m.\u001b[39mvolume_dimensions\n\u001b[1;32m    430\u001b[0m             affine \u001b[38;5;241m=\u001b[39m mim\u001b[38;5;241m.\u001b[39mvolume\u001b[38;5;241m.\u001b[39mtransformation_matrix_voxel_indices_ijk_to_xyz\u001b[38;5;241m.\u001b[39mmatrix\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoxel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnvertices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nibabel/cifti2/cifti2_axes.py:297\u001b[0m, in \u001b[0;36mBrainModelAxis.__init__\u001b[0;34m(self, name, voxel, vertex, affine, volume_shape, nvertices)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    296\u001b[0m     name \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_cifti_brain_structure_name(name)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvertex\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(name, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nvertices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnvertices \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nibabel/cifti2/cifti2_axes.py:623\u001b[0m, in \u001b[0;36mBrainModelAxis.name\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;129m@name\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m, values):\n\u001b[0;32m--> 623\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_cifti_brain_structure_name(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nibabel/cifti2/cifti2_axes.py:623\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;129m@name\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m, values):\n\u001b[0;32m--> 623\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_cifti_brain_structure_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Set up for visualizing dual frequency tagging across each subject using fractional overlap\n",
    "\"\"\"\n",
    "PALETTE = \"power_surf\"\n",
    "f1_c = -.1 # red -.1\n",
    "f2_c = .82 # blue .82\n",
    "f1f2_c = .14 # white .88 yellow .1\n",
    "mask_c = .41 # .9 [green], .1 [goldish], .4 [black]\n",
    "\n",
    "cohort_roi_info_across_experiments = {}\n",
    "ROI_FOS = [.8,1.]\n",
    "corr_type = \"uncp\"\n",
    "\n",
    "\"\"\"Save png\n",
    "\"\"\"\n",
    "# 3T control under entrain condition (set this to get power measurements with entrain ROIs)\n",
    "label = \"3TNormal\"\n",
    "df_data = None\n",
    "for _roi_task_id in [\"entrain\"]:\n",
    "    for ROI_FO in ROI_FOS:\n",
    "        experiment_id = \"1_frequency_tagging\" \n",
    "        mri_id = \"3T\"\n",
    "        sub_ids = [\"000\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\"] \n",
    "        roi_task_ids = [_roi_task_id] * len(sub_ids)\n",
    "        roi_f_1s = [.125] * len(sub_ids)\n",
    "        roi_f_2s = [.2] * len(sub_ids)\n",
    "        df_data = generate_single_subject_maps(\n",
    "            label, experiment_id, mri_id, sub_ids, \n",
    "            roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO,\n",
    "            df_data=df_data,\n",
    "            corr_type=corr_type,\n",
    "            ROI_FO=ROI_FO, SUB_THRESHOLD=.5,\n",
    "            FORCE_TASK_ID=\"control\"\n",
    "        )\n",
    "# 3T normal\n",
    "label = \"3TNormal\"\n",
    "for _roi_task_id in [\"entrain\"]:\n",
    "    for ROI_FO in ROI_FOS:\n",
    "        experiment_id = \"1_frequency_tagging\" \n",
    "        mri_id = \"3T\"\n",
    "        sub_ids = [\"000\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\"] \n",
    "        roi_task_ids = [_roi_task_id] * len(sub_ids)\n",
    "        roi_f_1s = [.125] * len(sub_ids)\n",
    "        roi_f_2s = [.2] * len(sub_ids)\n",
    "        df_data = generate_single_subject_maps(\n",
    "            label, experiment_id, mri_id, sub_ids, \n",
    "            roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO,\n",
    "            df_data=df_data,\n",
    "            corr_type=corr_type,\n",
    "            ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "        )\n",
    "# 7T normal\n",
    "label = \"7TNormal\"\n",
    "for ROI_FO in ROI_FOS:\n",
    "    experiment_id = \"1_attention\" \n",
    "    mri_id = \"7T\"\n",
    "    sub_ids = [\"Pilot001\", \"Pilot009\", \"Pilot010\", \"Pilot011\"]\n",
    "    roi_task_ids = [\"AttendAway\"] * len(sub_ids)\n",
    "    roi_f_1s = [.125] * len(sub_ids)\n",
    "    roi_f_2s = [.2] * len(sub_ids)\n",
    "    df_data = generate_single_subject_maps(\n",
    "        label, experiment_id, mri_id, sub_ids, \n",
    "        roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO,\n",
    "        df_data=df_data,\n",
    "        corr_type=corr_type,\n",
    "        ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "    )\n",
    "# 3T vary\n",
    "label = \"3TVary\"\n",
    "for ROI_FO in ROI_FOS:\n",
    "    experiment_id = \"1_frequency_tagging\"\n",
    "    mri_id = \"3T\"\n",
    "    sub_ids = [\"020\"] * 3 + [\"021\"] * 3\n",
    "    roi_task_ids = [f\"entrain{i}\" for i in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]]\n",
    "    roi_f_1s = [.125] * 3 + [.125, .15, .175]\n",
    "    roi_f_2s = [.2, .175, .15] + [.2] * 3\n",
    "    df_data = generate_single_subject_maps(\n",
    "        label, experiment_id, mri_id, sub_ids, \n",
    "        roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO,\n",
    "        df_data=df_data,\n",
    "        corr_type=corr_type,\n",
    "        ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "    )\n",
    "# 7T vary\n",
    "label = \"7TVary\"\n",
    "for ROI_FO in ROI_FOS:\n",
    "    experiment_id = \"1_frequency_tagging\"\n",
    "    mri_id = \"7T\"\n",
    "    sub_ids = [\"020\"] * 3 + [\"021\"] * 3\n",
    "    roi_task_ids = [f\"entrain{i}\" for i in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]]\n",
    "    roi_f_1s = [.125] * 3 + [.125, .15, .175]\n",
    "    roi_f_2s = [.2, .175, .15] + [.2] * 3\n",
    "    df_data = generate_single_subject_maps(\n",
    "        label, experiment_id, mri_id, sub_ids, \n",
    "        roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO,\n",
    "        df_data=df_data,\n",
    "        corr_type=corr_type,\n",
    "        ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create geodesic distance ROI from V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlabel_dir = Path(\"/opt/app/notebooks/data/dlabels\")\n",
    "hcp_label = dlabel_dir / \"Q1-Q6_RelatedValidation210.CorticalAreas_dil_Final_Final_Areas_Group_Colors.32k_fs_LR.dlabel.nii\"\n",
    "\n",
    "_HCP_INFO = !wb_command -file-information {hcp_label}\n",
    "HCP_LABELS = []\n",
    "HCP_COUNTER = 0\n",
    "for i in _HCP_INFO:\n",
    "    if len(i) == 60 and any([\"L_\" in i, \"R_\" in i]):\n",
    "        hcp_colors = tuple([float(f\"0.{k}\") for k in [j.split(' ') [0] for j in i.split('0.')][-3:]] + [1])\n",
    "        if ' R_' in i:\n",
    "            roi = i.split(\"_ROI\")[0].split(' R_')[1]\n",
    "            HCP_LABELS.append(f\"R_{roi}_ROI\")\n",
    "        if ' L_' in i:\n",
    "            roi = i.split(\"_ROI\")[0].split(' L_')[1]\n",
    "            HCP_LABELS.append(f\"L_{roi}_ROI\")\n",
    "        HCP_COUNTER += 1\n",
    "\n",
    "\n",
    "dscalar_dir = Path(\"/opt/app/notebooks/data/dscalars\")\n",
    "tmpdir = Path(\"/tmp\")\n",
    "template_dscalar = dscalar_dir / \"S1200.MyelinMap_BC_MSMAll.32k_fs_LR.dscalar.nii\"\n",
    "\n",
    "hcp_mapping = {}\n",
    "for roi_label in HCP_LABELS:\n",
    "    out_dscalar = tmpdir / f\"{roi_label}.gd.dscalar.nii\"\n",
    "    if out_dscalar.exists():\n",
    "        print(f\"Skipping {roi_label}\")\n",
    "        hcp_mapping[roi_label] = out_dscalar\n",
    "        continue\n",
    "    !wb_command -cifti-label-to-roi {hcp_label} {out_dscalar} -name {roi_label}\n",
    "    !wb_command -cifti-create-dense-from-template {template_dscalar} {out_dscalar} -cifti {out_dscalar}\n",
    "    assert out_dscalar.exists(), f\"{out_dscalar.stem} does not exist.\"\n",
    "    hcp_mapping[roi_label] = out_dscalar\n",
    "hcp_rois = list(set([k.split('_')[1] for k in hcp_mapping.keys()]))\n",
    "\n",
    "surface_dir = Path(\"/opt/app/notebooks/data/surfaces\")\n",
    "tmpdir = Path(\"/tmp\")\n",
    "L_mid = surface_dir / \"S1200.L.midthickness_MSMAll.32k_fs_LR.surf.gii\"\n",
    "R_mid = surface_dir / \"S1200.R.midthickness_MSMAll.32k_fs_LR.surf.gii\"\n",
    "L_geo = tmpdir / \"L.dconn.nii\"\n",
    "R_geo = tmpdir / \"R.dconn.nii\"\n",
    "!wb_command -surface-geodesic-distance-all-to-all {L_mid} {L_geo}\n",
    "!wb_command -surface-geodesic-distance-all-to-all {R_mid} {R_geo}\n",
    "\n",
    "L_V1 = tmpdir / \"L_V1_ROI.gd.dscalar.nii\"\n",
    "L_V1_coords = nib.load(L_V1).get_fdata()[0,:32492]==1\n",
    "_L_geo = nib.load(L_geo).get_fdata()\n",
    "L_geo_arr = _L_geo[L_V1_coords,:].mean(0)\n",
    "del _L_geo\n",
    "R_V1 = tmpdir / \"R_V1_ROI.gd.dscalar.nii\"\n",
    "R_V1_coords = nib.load(R_V1).get_fdata()[0,32492:]==1\n",
    "_R_geo = nib.load(R_geo).get_fdata()\n",
    "R_geo_arr = _R_geo[R_V1_coords,:].mean(0)\n",
    "del _R_geo\n",
    "\n",
    "geo_arr = np.concatenate((L_geo_arr, R_geo_arr))\n",
    "\n",
    "geodesic_dscalar = tmpdir / \"geodesic_V1.dscalar.nii\"\n",
    "img = nib.load(L_V1)\n",
    "data = np.zeros(img.shape)\n",
    "data[0,:] = geo_arr\n",
    "geo_img = nib.Cifti2Image(data, header=img.header)\n",
    "nib.save(geo_img, geodesic_dscalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot histogram of %BOLD power\n",
    "- Get max voxel count for $f_1$ or $f_2$\n",
    "- Get ROI colours from `dfm:get_roi_colour_codes`\n",
    "1. Subset `df` by dataset and fractional overlap to generate `subset_df`\n",
    "    - `df.experiment_id`: `3TNormal`, `7TNormal`, `3TVary`, `7TVary`\n",
    "    - `df.roi_fo`: `.8`, `1.`\n",
    "2. Get unique ROIs (specify thresholding: % of subjects in dataset that has this ROI)\n",
    "    - Unique ROIs are selected by `subset_df.frequency_of_roi==\"f1\"`\n",
    "        - $f_1$ is more sensitive then $f_2$, therefore $f_2$ is a subset of $f_1$ ROIs\n",
    "    - ROIs contain contralateral **or** ipsilateral component\n",
    "3. Sort ROIs by a metric (**y-axis**)\n",
    "    - geodesic distance (order of regions along y-axis)\n",
    "    - length of region along y-axis varied by ROI size (contralateral+ipsilateral)\n",
    "    - get colours for each region based on HCP labels \n",
    "4. Subset `subset_df` by region (2) and extract metric of interest (3) to generate `region_df`\n",
    "5. Plot (for normal/vary experiments plot dots/line using **subject-level**/**task-level**, respectively)\n",
    "    - Note: Regions are selected using $f_1$ regions, as such $f_2$ counterpart\n",
    "        - Also, not all regions will have an ipsilateral counterpart as contralateral sensitivity > ipsilateral sensitivity\n",
    "    - Contralateral (`marker_style='o'`)\n",
    "        - (a) dots (mean **or** median) of $f_1$ and $f_2$,  (size of marker varied by voxel count)\n",
    "        - (b) line connecting $f_1$ and $f_2$\n",
    "    - Repeat for ipsilateral (`marker_style='^'`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_rois(subset_df, n_ids, frequency_of_roi=\"f1\", threshold=1.):\n",
    "    _df = subset_df[subset_df.frequency_of_roi==frequency_of_roi]\n",
    "    rois = [roi for roi in _df.hcp_roi]\n",
    "    unique_rois = list(set(rois))\n",
    "\n",
    "    roi_map = defaultdict(list)\n",
    "    for roi in unique_rois:\n",
    "        for _roi in rois:\n",
    "            if _roi == roi:\n",
    "                roi_map[roi].append(_roi)\n",
    "\n",
    "    thresholded_unique_rois = []\n",
    "    unique_rois = ( list(set([i.split(\"_\")[-1] for i in roi_map.keys()])) )\n",
    "    for roi in unique_rois:\n",
    "        contra_count = len(roi_map[f\"CONTRA_{roi}\"])\n",
    "        ipsi_count = len(roi_map[f\"IPSI_{roi}\"])\n",
    "        max_count = max(contra_count,ipsi_count)\n",
    "\n",
    "        _thr = max_count/n_ids\n",
    "        if _thr >= threshold:\n",
    "            print(roi, _thr, ipsi_count, contra_count, max_count, n_ids)\n",
    "            thresholded_unique_rois.append(roi)\n",
    "\n",
    "    return thresholded_unique_rois\n",
    "\n",
    "def sort_lists(list1, list2):\n",
    "    combined_lists = zip(list1, list2)\n",
    "    sorted_combined_lists = sorted(combined_lists, key=lambda x: x[0])\n",
    "    sorted_list1, sorted_list2 = zip(*sorted_combined_lists)\n",
    "    \n",
    "    return list(sorted_list1), list(sorted_list2)\n",
    "\n",
    "def sort_unique_rois_by_geodesic_distance(geodesic_dscalar, unique_rois, hcp_mapping):\n",
    "\n",
    "    if not geodesic_dscalar.exists():\n",
    "        raise ValueError(f\"{geodesic_dscalar} does not exist.\")\n",
    "    geo_data = nib.load(geodesic_dscalar).get_fdata()\n",
    "\n",
    "    mean_geodesic_distances, n_vertices = [], []\n",
    "    for roi_label in unique_rois:\n",
    "        L_ROI = hcp_mapping[f\"L_{roi_label}_ROI\"]\n",
    "        R_ROI = hcp_mapping[f\"R_{roi_label}_ROI\"]\n",
    "        if not L_ROI.exists():\n",
    "            raise ValueError(f\"{L_ROI} does not exist.\")\n",
    "        if not R_ROI.exists():\n",
    "            raise ValueError(f\"{R_ROI} does not exist.\")\n",
    "        L_data = nib.load(L_ROI).get_fdata()\n",
    "        R_data = nib.load(R_ROI).get_fdata()\n",
    "        L_geo = geo_data[L_data==1]\n",
    "        R_geo = geo_data[R_data==1]        \n",
    "        mean_geo = np.concatenate((L_geo, R_geo)).mean()\n",
    "        mean_geodesic_distances.append(mean_geo)\n",
    "        n_vertices.append(L_data.sum() + R_data.sum())\n",
    "\n",
    "    _, sorted_rois = sort_lists(mean_geodesic_distances, unique_rois)\n",
    "    _, vertex_per_roi = sort_lists(mean_geodesic_distances, n_vertices)\n",
    "\n",
    "    return sorted_rois, vertex_per_roi\n",
    "\n",
    "def remove_outliers(arr, threshold=1.2):\n",
    "    # Calculate mean and standard deviation\n",
    "    mean_val = np.mean(arr)\n",
    "    std_dev = np.std(arr)\n",
    "\n",
    "    # Define threshold for outliers\n",
    "    lower_bound = mean_val - threshold * std_dev\n",
    "    upper_bound = mean_val + threshold * std_dev\n",
    "\n",
    "    # Filter array to remove outliers\n",
    "    filtered_arr = arr[(arr >= lower_bound) & (arr <= upper_bound)]\n",
    "\n",
    "    return filtered_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get HCP label colours as RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_c_dict = dict()\n",
    "for roi in hcp_rois:\n",
    "    _HCP_INFO = !wb_command -file-information {hcp_label} | grep \"_{roi}_ROI\"\n",
    "    for i in _HCP_INFO:\n",
    "        #hcp_c_dict[roi] = np.array([int(float(j)*256) for j in i.split('   ')[-5:-1]])[np.newaxis,:]\n",
    "        hcp_c_dict[roi] = np.array([float(j) for j in i.split('   ')[-5:-1]])[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. Subset `subset_df` by region (2) and extract metric of interest (3) to generate `region_df`\n",
    "5. Plot (for normal/vary experiments plot dots/line using **subject-level**/**task-level**, respectively)\n",
    "    - Note: Regions are selected using $f_1$ regions, as such $f_2$ counterpart\n",
    "        - Also, not all regions will have an ipsilateral counterpart as contralateral sensitivity > ipsilateral sensitivity\n",
    "    - Contralateral (`marker_style='o'`)\n",
    "        - (a) dots (mean **or** median) of $f_1$ and $f_2$,  (size of marker varied by voxel count)\n",
    "        - (b) line connecting $f_1$ and $f_2$\n",
    "    - Repeat for ipsilateral (`marker_style='^'`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to get min/max vertex count across all experiment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_fo = .8\n",
    "max_vertex_count = []\n",
    "for experiment_id in df.experiment_id.unique():\n",
    "    _tmp_df = df[(df.experiment_id==experiment_id) & (df.roi_fo==roi_fo)]\n",
    "    for ix, i in _tmp_df.iterrows():\n",
    "        vertex_count = i[\"f1_BOLD_power\"].shape[0]\n",
    "        max_vertex_count.append(vertex_count)\n",
    "max_vertex_count = np.max(max_vertex_count)\n",
    "\n",
    "print(f\"Max vertex count HCP ROIs and experiments: {max_vertex_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ratios of ROI across `experiment_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = subset_df[subset_df.frequency_of_roi==frequency_of_roi]\n",
    "rois = [roi for roi in _df.hcp_roi]\n",
    "unique_rois = list(set(rois))\n",
    "\n",
    "roi_map = defaultdict(list)\n",
    "for roi in unique_rois:\n",
    "    for _roi in rois:\n",
    "        if _roi == roi:\n",
    "            roi_map[roi].append(_roi)\n",
    "\n",
    "thresholded_unique_rois = []\n",
    "unique_rois = ( list(set([i.split(\"_\")[-1] for i in roi_map.keys()])) )\n",
    "for roi in unique_rois:\n",
    "    contra_count = len(roi_map[f\"CONTRA_{roi}\"])\n",
    "    ipsi_count = len(roi_map[f\"IPSI_{roi}\"])\n",
    "    max_count = max(contra_count,ipsi_count)\n",
    "\n",
    "    _thr = max_count/n_ids\n",
    "    if _thr >= threshold:\n",
    "        print(roi, _thr, ipsi_count, contra_count, max_count, n_ids)\n",
    "        thresholded_unique_rois.append(roi)\n",
    "\n",
    "return thresholded_unique_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def sort_lists(list1, list2):\n",
    "    combined_lists = zip(list1, list2)\n",
    "    sorted_combined_lists = sorted(combined_lists, key=lambda x: x[0])\n",
    "    sorted_list1, sorted_list2 = zip(*sorted_combined_lists)\n",
    "    \n",
    "    return list(sorted_list1), list(sorted_list2)\n",
    "\n",
    "def sort_unique_rois_by_geodesic_distance(geodesic_dscalar, unique_rois, hcp_mapping):\n",
    "\n",
    "    if not geodesic_dscalar.exists():\n",
    "        raise ValueError(f\"{geodesic_dscalar} does not exist.\")\n",
    "    geo_data = nib.load(geodesic_dscalar).get_fdata()\n",
    "\n",
    "    mean_geodesic_distances, n_vertices = [], []\n",
    "    for roi_label in unique_rois:\n",
    "        L_ROI = hcp_mapping[f\"L_{roi_label}_ROI\"]\n",
    "        R_ROI = hcp_mapping[f\"R_{roi_label}_ROI\"]\n",
    "        if not L_ROI.exists():\n",
    "            raise ValueError(f\"{L_ROI} does not exist.\")\n",
    "        if not R_ROI.exists():\n",
    "            raise ValueError(f\"{R_ROI} does not exist.\")\n",
    "        L_data = nib.load(L_ROI).get_fdata()\n",
    "        R_data = nib.load(R_ROI).get_fdata()\n",
    "        L_geo = geo_data[L_data==1]\n",
    "        R_geo = geo_data[R_data==1]        \n",
    "        mean_geo = np.concatenate((L_geo, R_geo)).mean()\n",
    "        mean_geodesic_distances.append(mean_geo)\n",
    "        n_vertices.append(L_data.sum() + R_data.sum())\n",
    "\n",
    "    _, sorted_rois = sort_lists(mean_geodesic_distances, unique_rois)\n",
    "    _, vertex_per_roi = sort_lists(mean_geodesic_distances, n_vertices)\n",
    "\n",
    "    return sorted_rois, vertex_per_roi\n",
    "\n",
    "def get_unique_rois(df, experiment_id, roi_fo, frequency_of_roi, hemi_prefix, task_id, sub_threshold, geodesic_dscalar, hcp_mapping, VARY=False):\n",
    "    experiment_df = df[(df.experiment_id==experiment_id)]\n",
    "    if VARY:\n",
    "        n_sub_ids_in_experiment = experiment_df.roi_task_id.unique().shape[0]\n",
    "        subset_df = experiment_df[(experiment_df.roi_fo==roi_fo) & (experiment_df.frequency_of_roi==frequency_of_roi) & (experiment_df.hcp_roi.str.startswith(hemi_prefix))]\n",
    "    else:\n",
    "        n_sub_ids_in_experiment = experiment_df.sub_id.unique().shape[0]\n",
    "        subset_df = experiment_df[(experiment_df.roi_fo==roi_fo) & (experiment_df.task_id==task_id) & (experiment_df.frequency_of_roi==frequency_of_roi) & (experiment_df.hcp_roi.str.startswith(hemi_prefix))]\n",
    "\n",
    "    thresholded_unique_rois = []\n",
    "    not_thresholded_unique_rois = []\n",
    "    for roi in subset_df.hcp_roi.unique():\n",
    "        _subset_df = subset_df[(subset_df.hcp_roi == roi)]\n",
    "        n_sub_ids_per_roi = _subset_df.shape[0]\n",
    "        if n_sub_ids_per_roi > n_sub_ids_in_experiment:\n",
    "            raise ValueError()\n",
    "        _thr = n_sub_ids_per_roi / n_sub_ids_in_experiment\n",
    "        if _thr >= sub_threshold:\n",
    "            thresholded_unique_rois.append(roi)\n",
    "        else:\n",
    "            not_thresholded_unique_rois.append(roi)\n",
    "\n",
    "    if len(thresholded_unique_rois) != 0:\n",
    "        thresholded_unique_rois,_ = sort_unique_rois_by_geodesic_distance(\n",
    "            geodesic_dscalar,\n",
    "            [i.split(\"_\")[-1] for i in thresholded_unique_rois],\n",
    "            hcp_mapping\n",
    "        )\n",
    "    \n",
    "    if len(not_thresholded_unique_rois) != 0:\n",
    "        not_thresholded_unique_rois,_ = sort_unique_rois_by_geodesic_distance(\n",
    "            geodesic_dscalar,\n",
    "            [i.split(\"_\")[-1] for i in not_thresholded_unique_rois],\n",
    "            hcp_mapping\n",
    "        )\n",
    "\n",
    "    return thresholded_unique_rois, not_thresholded_unique_rois\n",
    "\n",
    "roi_fo = .8\n",
    "experiment_ids = [\"3TNormal\",\"7TNormal\",\"3TVary\",\"7TVary\"]\n",
    "task_ids = [\"entrain\",\"AttendAway\",\"\",\"\"]\n",
    "frequency_of_rois = [\"f1\",\"f2\",\"f1Uf2\"]\n",
    "hemi_prefices = [\"IPSI\",\"CONTRA\"]\n",
    "sub_threshold = 1.\n",
    "\n",
    "roi_dict = {}\n",
    "for experiment_id, task_id in zip(experiment_ids, task_ids):\n",
    "    for frequency_of_roi, hemi_prefix in itertools.product(frequency_of_rois, hemi_prefices):\n",
    "        roi_dict[(experiment_id,frequency_of_roi,hemi_prefix)] = get_unique_rois(df, experiment_id, roi_fo, frequency_of_roi, hemi_prefix, task_id, sub_threshold, geodesic_dscalar, hcp_mapping, VARY=\"Vary\" in experiment_id)\n",
    "\n",
    "for k,v in roi_dict.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_fo = .8\n",
    "hemi_prefix = \"CONTRA\"\n",
    "frequency_of_roi = \"f1\"\n",
    "use_common_across_datasets = True\n",
    "    \n",
    "def compute_mean(X):\n",
    "    X.sort\n",
    "    X=X[::-1]\n",
    "    half_index = len(X)//2\n",
    "    top_half_mean = sum(X[:half_index]) / half_index\n",
    "\n",
    "    #return top_half_mean\n",
    "    return np.mean(X)\n",
    "\n",
    "# Get common ROIs across all datasets\n",
    "all_lists = []\n",
    "for k, (thr_list, unthr_list) in roi_dict.items():\n",
    "    if frequency_of_roi in k and hemi_prefix in k:\n",
    "        all_lists.append(thr_list)\n",
    "common_list = all_lists[0]\n",
    "for lst in all_lists[1:]:\n",
    "    common_list = [item for item in common_list if item in lst]\n",
    "\n",
    "# Set up axes\n",
    "experiment_ids = [\"3TNormalC\",\"3TNormal\",\"7TNormal\",\"3TVary\",\"7TVary\"]\n",
    "fig,ax_dict = plt.subplot_mosaic([experiment_ids], layout=\"constrained\",figsize=(7,2),dpi=300)\n",
    "\n",
    "# Plot\n",
    "for experiment_ix, experiment_id in enumerate(experiment_ids):\n",
    "\n",
    "    if experiment_id == \"3TNormalC\":\n",
    "        roi_task_id = \"entrain\"\n",
    "        task_id = \"control\"\n",
    "    if experiment_id == \"3TNormal\":\n",
    "        roi_task_id = \"entrain\"\n",
    "        task_id = \"entrain\"\n",
    "    if experiment_id == \"7TNormal\":\n",
    "        roi_task_id = \"AttendAway\"\n",
    "        task_id = \"AttendAway\"\n",
    "\n",
    "    if use_common_across_datasets:\n",
    "        thresholded_rois = common_list\n",
    "    else:\n",
    "        thresholded_rois = roi_dict[(experiment_id,frequency_of_roi,hemi_prefix)][0]\n",
    "\n",
    "    ax = ax_dict[experiment_id]\n",
    "    if experiment_id == \"3TNormalC\":\n",
    "        experiment_id=\"3TNormal\"\n",
    "\n",
    "    for ix, roi in enumerate(thresholded_rois):\n",
    "        if experiment_id.endswith(\"Vary\"):\n",
    "            _df = df[(df.experiment_id==experiment_id) & (df.roi_fo==roi_fo) & (df.hcp_roi.str.startswith(hemi_prefix)) & (df.hcp_roi.str.endswith(f\"_{roi}\")) & (df.frequency_of_roi==frequency_of_roi)]#[[\"sub_id\", f\"{frequency_of_roi}_BOLD_power\"]]\n",
    "        else:\n",
    "            _df = df[(df.experiment_id==experiment_id) & (df.roi_task_id==roi_task_id) & (df.task_id==task_id) & (df.roi_fo==roi_fo) & (df.hcp_roi.str.startswith(hemi_prefix)) & (df.hcp_roi.str.endswith(f\"_{roi}\")) & (df.frequency_of_roi==frequency_of_roi)]#[[\"sub_id\", f\"{frequency_of_roi}_BOLD_power\"]]\n",
    "        _df[\"metric_mean\"] = _df[f\"{frequency_of_roi}_BOLD_power\"].apply(compute_mean)\n",
    "\n",
    "        _y = _df.metric_mean.values\n",
    "        _x = np.zeros_like(_y) + ix\n",
    "\n",
    "        ax.scatter(_x,_y,c=hcp_c_dict[roi])\n",
    "        ax.scatter(_x.mean(),_y.mean(),c='k',zorder=10)\n",
    "\n",
    "        if ix == 0:\n",
    "            _yline = _y\n",
    "        else:\n",
    "            _yline = np.vstack((_yline,_y))\n",
    "\n",
    "    for j in range(_yline.shape[-1]):\n",
    "        ax.plot(range(len(thresholded_rois)),_yline[:,j],lw=.2,c='k')\n",
    "\n",
    "\n",
    "    ax.set_xticks(range(len(thresholded_rois)))\n",
    "    ax.set_xticklabels(thresholded_rois,rotation=90, fontsize=6)\n",
    "    ax.set_title(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_yline.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack((_y,_y)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rois, vertex_per_roi = sort_unique_rois_by_geodesic_distance(geodesic_dscalar, unique_rois, hcp_mapping) # sorted by geodesic distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_dict[(\"3TVary\",\"f1\",\"CONTRA\")][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_unique_rois = .8\n",
    "experiment_ids = [\"3TNormal\",\"7TNormal\",\"3TVary\",\"7TVary\"]\n",
    "n_rois_per_experiment_id = {}\n",
    "for experiment_id in experiment_ids:\n",
    "    # 1\n",
    "    if experiment_id == \"3TNormal\":\n",
    "        subset_df = df[(df.experiment_id==experiment_id) & (df.roi_fo==roi_fo) & (df.task_id==\"entrain\")]\n",
    "    else:\n",
    "        subset_df = df[(df.experiment_id==experiment_id) & (df.roi_fo==roi_fo)]\n",
    "    # 2\n",
    "    if experiment_id.endswith(\"Normal\"):\n",
    "        col_id = \"sub_id\" \n",
    "    elif experiment_id.endswith(\"Vary\"):\n",
    "        col_id = \"roi_task_id\" \n",
    "    else:\n",
    "        raise ValueError(f\"{experiment_id} not implemented.\")\n",
    "    n_ids = len(subset_df[col_id].unique())\n",
    "    unique_rois = get_unique_rois(subset_df, n_ids, threshold=threshold_unique_rois)\n",
    "    n_rois_per_experiment_id[experiment_id] = len(unique_rois)\n",
    "\n",
    "max_rois_across_experiments = np.max([i for i in n_rois_per_experiment_id.values()])\n",
    "n_rois_per_experiment_id, max_rois_across_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_fo = .8\n",
    "threshold_unique_rois = 1.\n",
    "metric = \"BOLD_power\" # Or phase_delay\n",
    "summary_metric_type = \"mean\"\n",
    "\n",
    "mosaic = [\n",
    "    [\"3TNormal\"]*n_rois_per_experiment_id[\"3TNormal\"],\n",
    "    [\"7TNormal\"]*n_rois_per_experiment_id[\"7TNormal\"]+[\"7TNormalFILL\"]*(max_rois_across_experiments-n_rois_per_experiment_id[\"7TNormal\"]),\n",
    "    [\"3TVary\"]*n_rois_per_experiment_id[\"3TVary\"]+[\"3TVaryFILL\"]*(max_rois_across_experiments-n_rois_per_experiment_id[\"3TVary\"]),\n",
    "    [\"7TVary\"]*n_rois_per_experiment_id[\"7TVary\"],\n",
    "]\n",
    "fig, ax_dict = plt.subplot_mosaic(\n",
    "    mosaic, \n",
    "    figsize=(5,4),\n",
    "    dpi=200, \n",
    "    layout=\"constrained\"\n",
    ")\n",
    "\n",
    "experiment_unique_rois = {}\n",
    "experiment_ids = [\"3TNormal\",\"7TNormal\",\"3TVary\",\"7TVary\"]\n",
    "ylabels = [\"3T\",\"7T\",\"3TVary\",\"7TVary\"]\n",
    "for experiment_id,ylabel in zip(experiment_ids,ylabels):\n",
    "    # 1\n",
    "    subset_df = df[(df.experiment_id==experiment_id) & (df.roi_fo==roi_fo)]\n",
    "    # 2\n",
    "    if experiment_id.endswith(\"Normal\"):\n",
    "        col_id = \"sub_id\" \n",
    "    elif experiment_id.endswith(\"Vary\"):\n",
    "        col_id = \"roi_task_id\" \n",
    "    else:\n",
    "        raise ValueError(f\"{experiment_id} not implemented.\")\n",
    "    n_ids = len(subset_df[col_id].unique())\n",
    "    unique_rois = get_unique_rois(subset_df, n_ids, threshold=threshold_unique_rois)\n",
    "    # 3\n",
    "    unique_rois, vertex_per_roi = sort_unique_rois_by_geodesic_distance(geodesic_dscalar, unique_rois, hcp_mapping) # sorted by geodesic distance\n",
    "    experiment_unique_rois[experiment_id] = unique_rois\n",
    "    # 4. Plot\n",
    "    roi_y_coords = [.2, .8] # [f_1, f_2] where f_1 and f_2 is plotted between 0 and 1\n",
    "    roi_c_dict = get_roi_colour_codes()\n",
    "    FONTSIZE = 6\n",
    "    LINEWIDTH = .4\n",
    "    ax = ax_dict[experiment_id]\n",
    "\n",
    "    # Loop over laterality\n",
    "    all_metrics = []\n",
    "    all_line_metrics = []\n",
    "    for laterality in [\"CONTRA\",\"IPSI\"]:\n",
    "\n",
    "        marker = \"o\"\n",
    "        if laterality == \"IPSI\":\n",
    "            marker = \"^\"\n",
    "\n",
    "        # Loop across all filtered HCP ROIs\n",
    "        for hcp_ix, hcp_roi in enumerate(unique_rois):\n",
    "            if experiment_id == \"3TNormal\":\n",
    "                region_df = subset_df[(subset_df.hcp_roi.str.endswith(f\"_{hcp_roi}\")) & (subset_df.frequency_of_roi.isin([\"f1\",\"f2\"]) & (subset_df.task_id==\"entrain\"))]\n",
    "            else:\n",
    "                region_df = subset_df[(subset_df.hcp_roi.str.endswith(f\"_{hcp_roi}\")) & (subset_df.frequency_of_roi.isin([\"f1\",\"f2\"]))]\n",
    "            data_levels = region_df[col_id].unique() # sub_id or task_id\n",
    "            for data_level in data_levels:\n",
    "                _df = region_df[(region_df[col_id]==data_level) & (region_df.hcp_roi==f\"{laterality}_{hcp_roi}\")]\n",
    "                f1_data = _df[_df.frequency_of_roi==\"f1\"]\n",
    "                f2_data = _df[_df.frequency_of_roi==\"f2\"]\n",
    "\n",
    "                plot_line = False\n",
    "                if (f1_data.shape[0] == 1 and (f1_data[\"f1_BOLD_power\"].values[0]!=0).shape[0]) and (f2_data.shape[0] == 1 and (f2_data[\"f2_BOLD_power\"].values[0]!=0).shape[0]):\n",
    "                    plot_line = True\n",
    "                    line_x = []\n",
    "                    line_y = []\n",
    "\n",
    "                if f1_data.shape[0] > 1:\n",
    "                    raise ValueError(f\"Expect 0 or 1 row in the filtered dataframe `f1_data`\")\n",
    "                if f2_data.shape[0] > 1:\n",
    "                    raise ValueError(f\"Expect 0 or 1 row in the filtered dataframe `f2_data`\")\n",
    "\n",
    "                # Loop over f1 and f2 data\n",
    "                for plot_ix, f_data in enumerate([f1_data,f2_data]):\n",
    "                    if f_data.shape[0] > 1:\n",
    "                        raise ValueError(f\"Expect 0 or 1 row in the filtered dataframe\")\n",
    "\n",
    "                    if f_data.shape[0]==1: \n",
    "\n",
    "                        frequency_of_roi = f_data.frequency_of_roi.values[0]\n",
    "                        pos = (hcp_ix)+roi_y_coords[plot_ix]\n",
    "                        metric = f_data[f\"{frequency_of_roi}_BOLD_power\"].values[0]\n",
    "                        vertex_count = metric.shape[0]\n",
    "                        if vertex_count == 0:\n",
    "                            continue\n",
    "\n",
    "                        if summary_metric_type == \"median\":\n",
    "                            summary_metric = np.median(metric)\n",
    "                        elif summary_metric_type == \"mean\":\n",
    "                            summary_metric = np.mean(metric)\n",
    "                        else:\n",
    "                            raise ValueError(f\"summary metric: {summary_metric_type} not supported.\")\n",
    "\n",
    "                        # s= # need to set marker size based on voxel count\n",
    "                        marker_scale = (vertex_count*.9/444)+.1\n",
    "                        ax.scatter(pos, summary_metric, s=marker_scale*50, c=roi_c_dict[frequency_of_roi],edgecolor='k',linewidths=LINEWIDTH,zorder=10, marker=marker)\n",
    "                        if plot_line:\n",
    "                            line_y.append(summary_metric)\n",
    "                            line_x.append(pos)\n",
    "                    \n",
    "                    # Plot line\n",
    "                    if plot_line:\n",
    "                        ax.plot(line_x, line_y, lw=LINEWIDTH, c='k',zorder=5, linestyle='dotted')\n",
    "                        all_line_metrics += line_y\n",
    "\n",
    "                all_metrics.append(summary_metric)\n",
    "                #print(data_level, f1_data.shape, f2_data.shape, plot_line)\n",
    "        \n",
    "    # Y-axis\n",
    "    YTICKMAX = round(np.max(all_line_metrics),4)\n",
    "    _yticks = [0,YTICKMAX/2,YTICKMAX]\n",
    "    _yticklabels = [f\"{i:.1e}\" if i!=0 else \"0\" for i in _yticks]\n",
    "    ax.set_yticks(_yticks)\n",
    "    ax.set_yticklabels(_yticklabels,fontsize=FONTSIZE)\n",
    "    ax.tick_params(axis=\"y\",width=LINEWIDTH,pad=0.2,length=4)\n",
    "    ax.tick_params(axis=\"x\",width=LINEWIDTH,pad=0.4,length=0)\n",
    "    YMAX = np.array(all_line_metrics).max()\n",
    "    Y_OFFSET = YMAX*.05\n",
    "    X_TICKLABEL_POSITION = -(YMAX*.35)\n",
    "    ax.set_ylim(X_TICKLABEL_POSITION,YMAX+Y_OFFSET)\n",
    "    ax.plot([-.1]*2,[_yticks[0],_yticks[-1]],lw=LINEWIDTH,c='k',zorder=20)\n",
    "    \n",
    "    # X-axis\n",
    "    ax.set_xlim(-.1,len(unique_rois))\n",
    "    ax.set_xticks([])\n",
    "    XTICKS = [i-.5 for i in range(1, len(unique_rois)+1,1)]\n",
    "    for xtickpos, xticklabel in zip(XTICKS,unique_rois):\n",
    "        if xticklabel == \"TPOJ2\":\n",
    "            ax.text(xtickpos,X_TICKLABEL_POSITION+(YMAX*.15),xticklabel,fontsize=FONTSIZE-.5,ha=\"center\",va=\"center\",c='white',zorder=50)\n",
    "            continue\n",
    "        ax.text(xtickpos,X_TICKLABEL_POSITION+(YMAX*.15),xticklabel,fontsize=FONTSIZE,ha=\"center\",va=\"center\",c='white',zorder=50)\n",
    "    for start_ix, hcp_roi in enumerate(unique_rois):\n",
    "        end_ix = start_ix+1\n",
    "        ax.fill_between([start_ix, end_ix],[X_TICKLABEL_POSITION]*2,[-(YMAX*.05)]*2,color=hcp_c_dict[hcp_roi])\n",
    "\n",
    "    for spine_type in [\"left\",\"right\", \"top\", \"bottom\"]:\n",
    "        ax.spines[spine_type].set_visible(False)\n",
    "\n",
    "    _ylabel_pos = -X_TICKLABEL_POSITION / (-X_TICKLABEL_POSITION+YMAX+Y_OFFSET)\n",
    "    _ylabel_pos = _ylabel_pos + ( (1 - _ylabel_pos) / 2 )\n",
    "    ax.set_ylabel(ylabel,fontsize=FONTSIZE,y=_ylabel_pos)\n",
    "\n",
    "for k,ax in ax_dict.items():\n",
    "    if k.endswith(\"FILL\"):\n",
    "        ax.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"D.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: For all power plots below uses *mean* for points, and *median* across points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laterality dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_fo = .8\n",
    "subset_df = df[(df.roi_fo==roi_fo)]\n",
    "experiment_ids = subset_df.experiment_id.unique()\n",
    "\n",
    "data_dict = defaultdict(list)\n",
    "for experiment_id in experiment_ids:\n",
    "    _df = subset_df[(subset_df.experiment_id==experiment_id)]\n",
    "    sub_ids = _df.sub_id.unique()\n",
    "    for sub_id in sub_ids:\n",
    "        _df = subset_df[(subset_df.experiment_id==experiment_id) & (subset_df.sub_id==sub_id)]\n",
    "        roi_task_ids = _df.roi_task_id.unique()\n",
    "        for roi_task_id in roi_task_ids:\n",
    "            _df = subset_df[(subset_df.experiment_id==experiment_id) & (subset_df.sub_id==sub_id) & (subset_df.roi_task_id==roi_task_id)]\n",
    "            hcp_rois = [i for i in _df.hcp_roi.unique() if i.startswith(\"CONTRA\")]\n",
    "            for frequency_of_roi, f_metric_label in zip([\"f1\",\"f2\"],[\"f1_BOLD_power\",\"f2_BOLD_power\"]):\n",
    "                for hcp_roi in hcp_rois:\n",
    "                    _df = subset_df[(subset_df.experiment_id==experiment_id) & (subset_df.sub_id==sub_id) & (subset_df.roi_task_id==roi_task_id) & (subset_df.frequency_of_roi==frequency_of_roi) & (subset_df.hcp_roi.str.contains(hcp_roi.split(\"_\")[-1]))]\n",
    "                    if _df.shape[0]!=2:\n",
    "                        continue\n",
    "                    hcp_roi_stripped = hcp_roi.split(\"_\")[-1]\n",
    "                    ipsi_data = _df[(_df.hcp_roi==f\"IPSI_{hcp_roi_stripped}\")]\n",
    "                    contra_data = _df[(_df.hcp_roi==f\"CONTRA_{hcp_roi_stripped}\")]\n",
    "                    if ipsi_data.shape[0]==0 or contra_data.shape[0]==0:\n",
    "                        continue\n",
    "                    n_contra_vertices = contra_data[f_metric_label].values[0].shape[0]\n",
    "                    n_ipsi_vertices = ipsi_data[f_metric_label].values[0].shape[0]\n",
    "                    if n_contra_vertices == 0 or n_ipsi_vertices == 0:\n",
    "                        continue\n",
    "\n",
    "                    data_dict[\"experiment_id\"].append(ipsi_data.experiment_id.values[0])\n",
    "                    data_dict[\"roi_task_id\"].append(ipsi_data.roi_task_id.values[0])\n",
    "                    data_dict[\"sub_id\"].append(ipsi_data.sub_id.values[0])\n",
    "                    data_dict[\"frequency_of_roi\"].append(ipsi_data.frequency_of_roi.values[0])\n",
    "                    data_dict[\"hcp_roi\"].append(ipsi_data.hcp_roi.values[0].split(\"_\")[-1])\n",
    "                    # Control for vertex size for contra/ipsilateral ROIs\n",
    "                    if True:\n",
    "                        contra_metric = np.sort(contra_data[f_metric_label].values[0])[::-1]\n",
    "                        ipsi_metric = np.sort(ipsi_data[f_metric_label].values[0])[::-1]\n",
    "                        _n_vertices = np.min([contra_metric.shape[0],ipsi_metric.shape[0]])\n",
    "                        contra_metric = contra_metric[:_n_vertices]\n",
    "                        ipsi_metric = ipsi_metric[:_n_vertices]\n",
    "                        pvalue = stats.mannwhitneyu(\n",
    "                            contra_metric,\n",
    "                            ipsi_metric,\n",
    "                        ).pvalue\n",
    "                        contra_metric = np.mean(contra_metric)\n",
    "                        ipsi_metric = np.mean(ipsi_metric)\n",
    "                    else:\n",
    "                        pvalue = stats.mannwhitneyu(\n",
    "                            contra_data[f_metric_label].values[0],\n",
    "                            ipsi_data[f_metric_label].values[0],\n",
    "                        ).pvalue\n",
    "                        contra_metric = np.mean(contra_data[f_metric_label].values[0])\n",
    "                        ipsi_metric = np.mean(ipsi_data[f_metric_label].values[0])\n",
    "                    laterality_difference = contra_metric-ipsi_metric # CONTRA > IPSI -> laterality_difference > 0\n",
    "                    data_dict[\"metric\"].append(laterality_difference)\n",
    "                    data_dict[\"p_value\"].append(pvalue)\n",
    "\n",
    "laterality_df = pd.DataFrame(data_dict)\n",
    "laterality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_dict = plt.subplot_mosaic([[\"X\"]], dpi=300, figsize=(.75,1.5),layout=\"constrained\")\n",
    "\n",
    "ax=ax_dict[\"X\"]\n",
    "s = 20\n",
    "store_data = {}\n",
    "for ix, frequency in enumerate([\"f1\",\"f2\"]):\n",
    "    _laterality_df = laterality_df[(laterality_df.frequency_of_roi==frequency)]\n",
    "    c = [hcp_c_dict[hcp_roi] for hcp_roi in _laterality_df.hcp_roi.values]\n",
    "    y = _laterality_df.metric.values\n",
    "    store_data[frequency] = y\n",
    "    x = np.zeros_like(y.shape) + ix\n",
    "    x_jitter = np.random.uniform(low=-.35,high=.35,size=y.shape)\n",
    "    ax.scatter(x+x_jitter,y,c=c,s=s,edgecolor='k',linewidth=LINEWIDTH)\n",
    "    ax.plot([ix-.5,ix+.35],[np.median(y)]*2,lw=LINEWIDTH,c='r')\n",
    "    ax.plot([-.5,1.5],[0]*2,lw=LINEWIDTH,c='grey',linestyle='dotted')\n",
    "\n",
    "    print(stats.ttest_1samp(y,0))\n",
    "\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_xticklabels([\"$f_1$\",\"$f_2$\"],fontsize=FONTSIZE)\n",
    "ax.set_yticks(ax.get_yticks())\n",
    "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=FONTSIZE)\n",
    "ax.set_ylim(-.0005,.001)\n",
    "ax.set_ylabel(\"$\\Delta$Power ($C$-$I$)\",fontsize=FONTSIZE,y=.42,ha=\"center\",va=\"center\")\n",
    "\n",
    "for spine_type in [\"left\",\"right\", \"top\", \"bottom\"]:\n",
    "    ax.spines[spine_type].set_visible(False)\n",
    "ax.tick_params(\"both\",width=LINEWIDTH,pad=0.4)\n",
    "\n",
    "fig.savefig(\"E.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=store_data[\"f1\"]\n",
    "weights = np.ones_like(y)/len(y)\n",
    "\n",
    "fig,ax=plt.subplots(dpi=200,figsize=(2,1))\n",
    "ax.hist(y,weights=weights,bins=600)\n",
    "ax.set_ylim(0,.5)\n",
    "ax.plot([0,0],[0,.1],linewidth=.4,c='r')\n",
    "ax.set_xlim(-.001,.001)\n",
    "print(np.median(y), np.mean(y))\n",
    "\n",
    "fig.savefig(\"F.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_mapping = {\n",
    "    \"entrain\": (.125,.2),\n",
    "    \"AttendAway\": (.125,.2),\n",
    "    \"entrainA\": (.125,.2),\n",
    "    \"entrainB\": (.125,.175),\n",
    "    \"entrainC\": (.125,.15),\n",
    "    \"entrainD\": (.125,.2),\n",
    "    \"entrainE\": (.15,.2),\n",
    "    \"entrainF\": (.175,.2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_fo = .8\n",
    "subset_df = df[(df.roi_fo==roi_fo)]\n",
    "experiment_ids = subset_df.experiment_id.unique()\n",
    "\n",
    "\n",
    "data_dict = defaultdict(list)\n",
    "for experiment_id in experiment_ids:\n",
    "    _df = subset_df[(subset_df.experiment_id==experiment_id)]\n",
    "    sub_ids = _df.sub_id.unique()\n",
    "    for sub_id in sub_ids:\n",
    "        _df = subset_df[(subset_df.experiment_id==experiment_id) & (subset_df.sub_id==sub_id)]\n",
    "        roi_task_ids = _df.roi_task_id.unique()\n",
    "        for roi_task_id in roi_task_ids:\n",
    "            _df = subset_df[(subset_df.experiment_id==experiment_id) & (subset_df.sub_id==sub_id) & (subset_df.roi_task_id==roi_task_id)]\n",
    "            hcp_rois = _df.hcp_roi.unique()\n",
    "            for hcp_roi in hcp_rois:\n",
    "                _df = subset_df[(subset_df.experiment_id==experiment_id) & (subset_df.sub_id==sub_id) & (subset_df.roi_task_id==roi_task_id) & (subset_df.hcp_roi==hcp_roi)]\n",
    "                f1_frequency = frequency_mapping[_df.roi_task_id.values[0]][0]\n",
    "                f2_frequency = frequency_mapping[_df.roi_task_id.values[0]][1]\n",
    "                f_difference = round(f2_frequency-f1_frequency,3)\n",
    "                f1_data = _df[(_df.frequency_of_roi==\"f1\")]\n",
    "                f2_data = _df[(_df.frequency_of_roi==\"f2\")]\n",
    "                if f1_data.shape[0] != 1 or f2_data.shape[0] != 1:\n",
    "                    continue\n",
    "                f1_data = f1_data[\"f1_BOLD_power\"].values[0]\n",
    "                f2_data = f2_data[\"f2_BOLD_power\"].values[0]\n",
    "                if f1_data.shape[0]==0 or f2_data.shape[0]==0:\n",
    "                    continue\n",
    "\n",
    "                if True:\n",
    "                    _f1_data = np.sort(f1_data)[::-1]\n",
    "                    _f2_data = np.sort(f2_data)[::-1]\n",
    "                    _n_vertices = np.min([_f1_data.shape[0],_f2_data.shape[0]])\n",
    "                    _f1_data = _f1_data[:_n_vertices]\n",
    "                    _f2_data = _f2_data[:_n_vertices]\n",
    "                    pvalue = stats.mannwhitneyu(\n",
    "                        _f1_data,\n",
    "                        _f2_data,\n",
    "                    ).pvalue\n",
    "                    metric = np.mean(_f1_data) - np.mean(_f2_data)\n",
    "                else:\n",
    "                    metric = np.mean(f1_data) - np.mean(f2_data) # f1>f2\n",
    "                    \n",
    "                data_dict[\"experiment_id\"].append(experiment_id)\n",
    "                data_dict[\"roi_task_id\"].append(roi_task_id)\n",
    "                data_dict[\"sub_id\"].append(sub_id)\n",
    "                data_dict[\"frequency_difference\"].append(f_difference)\n",
    "                data_dict[\"hcp_roi\"].append(hcp_roi)\n",
    "                data_dict[\"metric\"].append(metric)\n",
    "\n",
    "frequency_difference_dependence_df = pd.DataFrame(data_dict)\n",
    "frequency_difference_dependence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_dict = plt.subplot_mosaic([[\"ALL\",\"3TNormal\",\"7TNormal\",\"3TVary\",\"7TVary\"]], dpi=300, figsize=(7,1.5),layout=\"constrained\",gridspec_kw={\"width_ratios\": [1,.33,.33,1,1]})\n",
    "s = 20\n",
    "\n",
    "for key, ax in ax_dict.items():\n",
    "    if key == \"ALL\":\n",
    "        unique_frequencies = frequency_difference_dependence_df.frequency_difference.unique()\n",
    "    else:\n",
    "        unique_frequencies = frequency_difference_dependence_df[(frequency_difference_dependence_df.experiment_id==key)].frequency_difference.unique()\n",
    "    for ix, f_difference in enumerate(unique_frequencies):\n",
    "        if key == \"ALL\":\n",
    "            _frequency_difference_dependence_df = frequency_difference_dependence_df[(frequency_difference_dependence_df.frequency_difference==f_difference)]\n",
    "        else:\n",
    "            _frequency_difference_dependence_df = frequency_difference_dependence_df[(frequency_difference_dependence_df.frequency_difference==f_difference)&(frequency_difference_dependence_df.experiment_id==key)]\n",
    "        c = [hcp_c_dict[hcp_roi.split(\"_\")[-1]] for hcp_roi in _frequency_difference_dependence_df.hcp_roi.values]\n",
    "        y = _frequency_difference_dependence_df.metric.values\n",
    "        print(stats.ttest_1samp(y,0))\n",
    "        x = np.zeros_like(y.shape) + ix\n",
    "        x_jitter = np.random.uniform(low=-.35,high=.35,size=y.shape)\n",
    "        ax.scatter(x+x_jitter,y,c=c,s=s,edgecolor='k',linewidth=LINEWIDTH)\n",
    "        ax.plot([ix-.5,ix+.35],[np.median(y)]*2,lw=LINEWIDTH,c='r')\n",
    "        ax.plot([-.5,len(unique_frequencies)-.5],[0]*2,lw=LINEWIDTH,c='grey',linestyle='dotted')\n",
    "\n",
    "        ax.set_xticks([i for i in range(len(unique_frequencies))])\n",
    "        xticklabels = [f\"{round(i,4)}\" for i in unique_frequencies]\n",
    "        ax.set_xticklabels(xticklabels, fontsize=FONTSIZE)\n",
    "        ax.set_yticks(ax.get_yticks())\n",
    "        ax.set_yticklabels([f\"{i:.0e}\" for i in ax.get_yticks()],fontsize=FONTSIZE)\n",
    "        ax.set_ylim(-.00025,.001)\n",
    "        if key == \"ALL\":\n",
    "            ax.set_ylabel(\"$\\Delta$Power ($f_1$-$f_2$)\",fontsize=FONTSIZE,y=.42,ha=\"center\",va=\"center\")\n",
    "        ax.set_title(key, fontsize=FONTSIZE)\n",
    "\n",
    "        for spine_type in [\"left\",\"right\", \"top\", \"bottom\"]:\n",
    "            ax.spines[spine_type].set_visible(False)\n",
    "        ax.tick_params(\"both\",width=LINEWIDTH,pad=0.4)\n",
    "\n",
    "fig.savefig(\"G.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRI dependence on power\n",
    "    - only plots ROIs that are common across experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "experiment_suffices = [\"Normal\",\"Vary\"]\n",
    "lateralities = [\"CONTRA\",\"IPSI\"]\n",
    "\n",
    "mosaic_grid = [\n",
    "    [\n",
    "        \"f1_CONTRA_Normal\",\n",
    "        \"f1_IPSI_Normal\",\n",
    "        \"f2_CONTRA_Normal\",\n",
    "        \"f2_IPSI_Normal\",\n",
    "        \"f1_CONTRA_Vary\",\n",
    "        \"f1_IPSI_Vary\",\n",
    "        \"f2_CONTRA_Vary\",\n",
    "        \"f2_IPSI_Vary\",\n",
    "    ]\n",
    "]\n",
    "fig, ax_dict = plt.subplot_mosaic(mosaic_grid, dpi=300, figsize=(7.2,1.5),layout=\"constrained\")\n",
    "for experiment_suffix, laterality in itertools.product(experiment_suffices,lateralities):\n",
    "\n",
    "    rois_to_keep = [f\"{laterality}_{i}\" for i in list(set(experiment_unique_rois[f\"3T{experiment_suffix}\"]+experiment_unique_rois[f\"7T{experiment_suffix}\"]))]\n",
    "    s = 20\n",
    "    for f_type, f_metric in zip([\"f1\",\"f2\"],[\"f1_BOLD_power\",\"f2_BOLD_power\"]):\n",
    "        key = f\"{f_type}_{laterality}_{experiment_suffix}\"\n",
    "        ax = ax_dict[f\"{f_type}_{laterality}_{experiment_suffix}\"]\n",
    "        data_for_testing = {}\n",
    "        for ix, mri_id in enumerate([\"3T\",\"7T\"]):\n",
    "            mri_data = df[(df.experiment_id==f\"{mri_id}{experiment_suffix}\") & (df.frequency_of_roi==f_type) & (df.hcp_roi.isin(rois_to_keep))]\n",
    "            c = [hcp_c_dict[hcp_roi.split(\"_\")[-1]] for hcp_roi,f_data in zip(mri_data.hcp_roi.values, mri_data[f_metric]) if not np.isnan(np.mean(f_data))]\n",
    "            y = np.array([np.mean(i) for i in mri_data[f_metric] if not np.isnan(np.mean(i))])\n",
    "            data_for_testing[mri_id] = y\n",
    "            x = np.zeros_like(y.shape[0]) + ix\n",
    "            x_jitter = np.random.uniform(low=-.35,high=.35,size=y.shape)\n",
    "            ax.scatter(x+x_jitter,y,c=c,s=s,edgecolor='k',linewidth=LINEWIDTH)\n",
    "            ax.plot([ix-.5,ix+.35],[np.median(y)]*2,lw=LINEWIDTH,c='r')\n",
    "            ax.plot([-.5,1.5],[0]*2,lw=LINEWIDTH,c='grey',linestyle='dotted')\n",
    "\n",
    "\n",
    "        pvalue = stats.mannwhitneyu(\n",
    "            data_for_testing[\"3T\"],\n",
    "            data_for_testing[\"7T\"],\n",
    "        ).pvalue\n",
    "        print(experiment_suffix,laterality,f_type,pvalue)\n",
    "        ax.set_xticks([i for i in range(2)])\n",
    "        xticklabels = [\"3T\",\"7T\"]\n",
    "        ax.set_xticklabels(xticklabels, fontsize=FONTSIZE)\n",
    "        ax.set_yticks(ax.get_yticks())\n",
    "        ax.set_yticklabels([f\"{i:.0e}\" for i in ax.get_yticks()],fontsize=FONTSIZE)\n",
    "        ax.set_ylim(-.0005,.002)\n",
    "        ax.set_title(key, fontsize=FONTSIZE)\n",
    "\n",
    "        for spine_type in [\"left\",\"right\", \"top\", \"bottom\"]:\n",
    "            ax.spines[spine_type].set_visible(False)\n",
    "        ax.tick_params(\"both\",width=LINEWIDTH,pad=0.4)\n",
    "ax_dict[\"f1_CONTRA_Normal\"].set_ylabel(\"Power\",fontsize=FONTSIZE,y=.42,ha=\"center\",va=\"center\")\n",
    "\n",
    "fig.savefig(\"H.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersection dependence (f1&f2 vs f1/f2 alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_fo = .8\n",
    "\n",
    "data_dict = defaultdict(list)\n",
    "experiment_ids = df.experiment_id.unique()\n",
    "for experiment_id in experiment_ids:\n",
    "    _df = df[(df.roi_fo==roi_fo) & (df.experiment_id==experiment_id)]\n",
    "    roi_task_ids = _df.roi_task_id.unique()\n",
    "    for roi_task_id in roi_task_ids:\n",
    "        _df = df[(df.roi_fo==roi_fo) & (df.experiment_id==experiment_id) & (df.roi_task_id==roi_task_id)]\n",
    "        sub_ids = _df.sub_id.unique()\n",
    "        for sub_id in sub_ids:\n",
    "            _df = df[(df.roi_fo==roi_fo) & (df.experiment_id==experiment_id) & (df.roi_task_id==roi_task_id) & (df.sub_id==sub_id)]\n",
    "            hcp_rois = _df.hcp_roi.unique()\n",
    "            for hcp_roi in hcp_rois:\n",
    "                _df = df[(df.roi_fo==roi_fo) & (df.experiment_id==experiment_id) & (df.roi_task_id==roi_task_id) & (df.sub_id==sub_id) & (df.hcp_roi==hcp_roi)]\n",
    "                for f_type in [\"f1\",\"f2\"]:\n",
    "                    _df = df[(df.roi_fo==roi_fo) & (df.experiment_id==experiment_id) & (df.roi_task_id==roi_task_id) & (df.sub_id==sub_id) & (df.hcp_roi==hcp_roi)]\n",
    "                    f_data = _df[_df.frequency_of_roi==f_type]\n",
    "                    f_inter_data = _df[_df.frequency_of_roi==\"f1Uf2\"]\n",
    "\n",
    "                    if f_data.shape[0]+f_inter_data.shape[0] != 2:\n",
    "                        continue\n",
    "                    if f_data[f\"{f_type}_BOLD_power\"].values[0].shape[0]==0:\n",
    "                        continue\n",
    "                    if f_inter_data[f\"{f_type}_BOLD_power\"].values[0].shape[0]==0:\n",
    "                        continue\n",
    "                    # Get intersection coordinates\n",
    "                    f_data_vertices = f_data[\"vertex_coordinates\"].values[0][0]\n",
    "                    f_inter_data_vertices = f_inter_data[\"vertex_coordinates\"].values[0][0]\n",
    "                    intersection_bools = []\n",
    "                    for f_data_vertex in f_data_vertices:\n",
    "                        intersection_bool = False\n",
    "                        if f_data_vertex in f_inter_data_vertices:\n",
    "                            intersection_bool = True\n",
    "                        intersection_bools.append(intersection_bool)\n",
    "                    intersection_bools = np.array(intersection_bools)\n",
    "                    # Get power of non-intersection and intersection data\n",
    "                    _f_not_inter_data = f_data[f\"{f_type}_BOLD_power\"].values[0][intersection_bools==0]\n",
    "                    _f_inter_data = f_data[f\"{f_type}_BOLD_power\"].values[0][intersection_bools==1]\n",
    "                    if _f_not_inter_data.shape[0] == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    pvalue = stats.mannwhitneyu(\n",
    "                        _f_inter_data,\n",
    "                        _f_not_inter_data,\n",
    "                    ).pvalue\n",
    "\n",
    "                    \n",
    "                    metric = np.mean(_f_inter_data) - np.mean(_f_not_inter_data) # power @ intersection of f1&f2 > outside\n",
    "                    print(hcp_roi,metric,pvalue)\n",
    "                        \n",
    "                    data_dict[\"experiment_id\"].append(experiment_id)\n",
    "                    data_dict[\"roi_task_id\"].append(roi_task_id)\n",
    "                    data_dict[\"sub_id\"].append(sub_id)\n",
    "                    data_dict[\"hcp_roi\"].append(hcp_roi)\n",
    "                    data_dict[\"f_type\"].append(f_type)\n",
    "                    data_dict[\"metric\"].append(metric)\n",
    "                    data_dict[\"p_value\"].append(pvalue)\n",
    "\n",
    "intersection_df = pd.DataFrame(data_dict)\n",
    "intersection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_df[intersection_df.p_value<.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax_dict = plt.subplot_mosaic([[\"X\"]], dpi=300, figsize=(.75,1.5),layout=\"constrained\")\n",
    "\n",
    "ax=ax_dict[\"X\"]\n",
    "s = 20\n",
    "#store_data = {}\n",
    "for ix, frequency in enumerate([\"f1\",\"f2\"]):\n",
    "    _intersection_df = intersection_df[(intersection_df.f_type==frequency)]\n",
    "    c = [hcp_c_dict[hcp_roi.split(\"_\")[-1]] for hcp_roi in _intersection_df.hcp_roi.values]\n",
    "    y = _intersection_df.metric.values\n",
    "    print(stats.ttest_1samp(y,0))\n",
    "    #store_data[frequency] = y\n",
    "    x = np.zeros_like(y.shape) + ix\n",
    "    x_jitter = np.random.uniform(low=-.35,high=.35,size=y.shape)\n",
    "    ax.scatter(x+x_jitter,y,c=c,s=s,edgecolor='k',linewidth=LINEWIDTH)\n",
    "    ax.plot([ix-.5,ix+.35],[np.median(y)]*2,lw=LINEWIDTH,c='r')\n",
    "    ax.plot([-.5,1.5],[0]*2,lw=LINEWIDTH,c='grey',linestyle='dotted')\n",
    "\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_xticklabels([\"$f_1$\",\"$f_2$\"],fontsize=FONTSIZE)\n",
    "ax.set_yticks([0])\n",
    "ax.set_yticklabels([\"0\"],fontsize=FONTSIZE)\n",
    "ax.set_ylabel(\"$\\Delta$Power ($f_{both}$-$f_{single}$)\",fontsize=FONTSIZE,y=.42,ha=\"center\",va=\"center\")\n",
    "\n",
    "for spine_type in [\"left\",\"right\", \"top\", \"bottom\"]:\n",
    "    ax.spines[spine_type].set_visible(False)\n",
    "ax.tick_params(\"both\",width=LINEWIDTH,pad=0.4)\n",
    "\n",
    "fig.savefig(\"H.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare contralateral, ipsilateral\n",
    "    - Consider only subjects, regions with a contralateral and ipsilateral component\n",
    "    - x-axis: L, R\n",
    "    - y-axis: power\n",
    "- Compare stimulation frequencies\n",
    "    - Consider only subjects, regions with $f_1$ and $f_2$ component\n",
    "    - x-axis: stimulation frequency\n",
    "    - y-axis: power\n",
    "- MRI dependence\n",
    "    - x-axis: MRI strength (3T, 7T)\n",
    "    - y-axis: power\n",
    "- Control experiment\n",
    "    - Plot across all 3T subjects, regions\n",
    "    - x-axis: 3T control, 3T entrain\n",
    "    - y-axis: power\n",
    "- Compare (lower, higher visual hierarchy)\n",
    "    - How to define this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare single subject maps to HCP retinotopic polar angle maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create group-level retinotopy maps (from HCP) \n",
    "- normalize metrics' coordinates to the `template_dscalar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retinotopy_dir = Path(\"/opt/app/notebooks/data/S1200_7T_Retinotopy_Pr_9Zkk/S1200_7T_Retinotopy181/MNINonLinear/fsaverage_LR32k\")\n",
    "retinotopy_dscalars = {k: retinotopy_dir / f\"S1200_7T_Retinotopy181.Fit1_{k}_MSMAll.32k_fs_LR.dscalar.nii\" for k in [\"PolarAngle\",\"Eccentricity\",\"ReceptiveFieldSize\"]}\n",
    "tmp_retinotopy_dscalars = {k: tmpdir / f\"S1200_7T_Retinotopy181.Fit1_{k}_MSMAll.32k_fs_LR.dscalar.nii\" for k in [\"PolarAngle\",\"Eccentricity\",\"ReceptiveFieldSize\"]}\n",
    "for retino_type, raw_retino in retinotopy_dscalars.items():\n",
    "    if not raw_retino.exists():\n",
    "        raise ValueError(f\"{retino_type}: {raw_retino} not found.\")\n",
    "\n",
    "    tmp_retino = tmp_retinotopy_dscalars[retino_type]\n",
    "    !wb_command -cifti-create-dense-from-template {template_dscalar} {tmp_retino} -cifti {raw_retino}\n",
    "    assert tmp_retino.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing extraction of retinotopy information with a row of the dataframe (or `df`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinotopy_metrics = {k: read_roi_path(v) for k,v in tmp_retinotopy_dscalars.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"ComputeCanada/frequency_tagging\")\n",
    "from dfm import (\n",
    "    get_roi_colour_codes,\n",
    "    change_font,\n",
    ")\n",
    "change_font()\n",
    "\n",
    "def circular_median(vals):\n",
    "    median_cos = np.mean([np.cos(i) for i in vals if not np.isnan(i)])\n",
    "    median_sin = np.mean([np.sin(i) for i in vals if not np.isnan(i)])\n",
    "    x = np.arctan2(median_sin,median_cos)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTSIZE = 6\n",
    "\n",
    "roi_fo = .8\n",
    "hcp_roi = \"CONTRA_V1\"\n",
    "metric = \"power\"\n",
    "\n",
    "roi_c_dict = get_roi_colour_codes()\n",
    "for experiment_id in df.experiment_id.unique():\n",
    "\n",
    "    sub_ids = df[(df.experiment_id==experiment_id)].sub_id.unique()\n",
    "\n",
    "    for sub_id in sub_ids:\n",
    "\n",
    "        roi_task_ids = df[(df.experiment_id==experiment_id) & (df.sub_id==sub_id)].roi_task_id.unique()\n",
    "\n",
    "        for roi_task_id in roi_task_ids:\n",
    "\n",
    "            subset_df = df[(df.roi_fo==roi_fo) & (df.hcp_roi==hcp_roi) & (df.experiment_id==experiment_id) & (df.sub_id==sub_id) & (df.roi_task_id==roi_task_id)]\n",
    "            max_metric = max([i.max() for i in subset_df['f1_BOLD_power']])\n",
    "            assert subset_df.shape[0]<=3, subset_df\n",
    "\n",
    "            fig, ax_dict = plt.subplot_mosaic([[\"f1\",\"f2\",\"f1Uf2\"]],figsize=(2,1),subplot_kw={'projection':'polar'}, dpi=300)\n",
    "\n",
    "            f_types = [\"f1\",\"f2\",\"f1Uf2\"]\n",
    "\n",
    "            for _, r in subset_df.iterrows():\n",
    "                _ = [f_types.pop(ix) for ix,i in enumerate(f_types) if r.frequency_of_roi == i]\n",
    "                ax = ax_dict[r.frequency_of_roi]\n",
    "                polar_angle = retinotopy_metrics[\"PolarAngle\"][r.vertex_coordinates] * np.pi/180\n",
    "\n",
    "                metric_map = {}\n",
    "                if metric==\"power\":\n",
    "                    if r.frequency_of_roi == \"f1\":\n",
    "                        metric_map[\"f1\"] = r.f1_BOLD_power\n",
    "                    elif r.frequency_of_roi == \"f2\":\n",
    "                        metric_map[\"f2\"] = r.f2_BOLD_power\n",
    "                    elif r.frequency_of_roi == \"f1Uf2\":\n",
    "                        metric_map[\"f1\"] = r.f1_BOLD_power\n",
    "                        metric_map[\"f2\"] = r.f2_BOLD_power\n",
    "                    else:\n",
    "                        raise ValueError()\n",
    "                for k,_metric in metric_map.items():\n",
    "                    ax.scatter(polar_angle, _metric, c=roi_c_dict[k],s=.2,zorder=1)\n",
    "                    ax.plot([circular_median(polar_angle)]*2, [0,max_metric/5],linewidth=.5,c='k',zorder=2)\n",
    "                ax.set_xticks([0])\n",
    "                ax.set_xticklabels([\"0\"],fontsize=FONTSIZE-2)\n",
    "                ax.set_yticks([max_metric/5])\n",
    "                ax.set_yticklabels([f\"{max_metric:.4f}\"],fontsize=FONTSIZE-3)\n",
    "                ax.tick_params(axis=\"both\",pad=0,zorder=3)\n",
    "                ax.grid(linewidth=.5)\n",
    "                ax.spines.polar.set_visible(False)\n",
    "                ax.set_title(f\"{r.frequency_of_roi} [{circular_median(polar_angle)*180/np.pi:.1f}]\",fontsize=FONTSIZE)\n",
    "            for k in f_types:\n",
    "                ax_dict[k].set_visible(False)\n",
    "\n",
    "            fig.suptitle(f\"[{experiment_id}] sub-{sub_id} roi-task-{roi_task_id}{r.quadrant_id}\", fontsize=FONTSIZE-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

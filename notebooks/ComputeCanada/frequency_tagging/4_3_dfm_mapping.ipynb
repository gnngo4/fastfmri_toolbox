{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/opt/wbplot\")\n",
    "\n",
    "from wbplot import dscalar\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "\n",
    "sys.path.append(\"ComputeCanada/frequency_tagging\")\n",
    "from dfm import (\n",
    "    get_roi_colour_codes,\n",
    "    change_font,\n",
    ")\n",
    "change_font()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get HCP info\n",
    "- `hcp_mappings`: dict of ROI: dscalars\n",
    "- `hcp_rois`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get HCP labels\n",
    "\"\"\"\n",
    "dlabel_dir = Path(\"/opt/app/notebooks/data/dlabels\")\n",
    "hcp_label = dlabel_dir / \"Q1-Q6_RelatedValidation210.CorticalAreas_dil_Final_Final_Areas_Group_Colors.32k_fs_LR.dlabel.nii\"\n",
    "\n",
    "_HCP_INFO = !wb_command -file-information {hcp_label}\n",
    "HCP_LABELS = []\n",
    "HCP_COUNTER = 0\n",
    "for i in _HCP_INFO:\n",
    "    if len(i) == 60 and any([\"L_\" in i, \"R_\" in i]):\n",
    "        hcp_colors = tuple([float(f\"0.{k}\") for k in [j.split(' ') [0] for j in i.split('0.')][-3:]] + [1])\n",
    "        if ' R_' in i:\n",
    "            roi = i.split(\"_ROI\")[0].split(' R_')[1]\n",
    "            HCP_LABELS.append(f\"R_{roi}_ROI\")\n",
    "        if ' L_' in i:\n",
    "            roi = i.split(\"_ROI\")[0].split(' L_')[1]\n",
    "            HCP_LABELS.append(f\"L_{roi}_ROI\")\n",
    "        HCP_COUNTER += 1\n",
    "\n",
    "\"\"\"Get HCP label coordinates\n",
    "\"\"\"\n",
    "dscalar_dir = Path(\"/opt/app/notebooks/data/dscalars\")\n",
    "tmpdir = Path(\"/tmp\")\n",
    "template_dscalar = dscalar_dir / \"S1200.MyelinMap_BC_MSMAll.32k_fs_LR.dscalar.nii\"\n",
    "\n",
    "hcp_mapping = {}\n",
    "for roi_label in HCP_LABELS:\n",
    "    out_dscalar = tmpdir / f\"{roi_label}.dscalar.nii\"\n",
    "    if out_dscalar.exists():\n",
    "        hcp_mapping[roi_label] = out_dscalar\n",
    "        continue\n",
    "    !wb_command -cifti-label-to-roi {hcp_label} {out_dscalar} -name {roi_label}\n",
    "    assert out_dscalar.exists(), f\"{out_dscalar.stem} does not exist.\"\n",
    "    hcp_mapping[roi_label] = out_dscalar\n",
    "hcp_rois = list(set([k.split('_')[1] for k in hcp_mapping.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_fractional_overlap(data):\n",
    "\n",
    "    return data.sum(0) / data.shape[0]\n",
    "\n",
    "def map_data_to_value(data_list):\n",
    "\n",
    "    for ix, (k,v) in enumerate(data_list):\n",
    "\n",
    "        if ix == 0:\n",
    "            new_data = k.copy() * v\n",
    "        else:\n",
    "            new_data += k * v\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def combine_f1_f2(f1,f2,fo=1.,mask=None, f1_c=.01, f2_c=.82, f1f2_c=.28, mask_c=.01):\n",
    "    f1_data = convert_to_fractional_overlap(nib.load(f1).get_fdata())\n",
    "    f2_data = convert_to_fractional_overlap(nib.load(f2).get_fdata())\n",
    "    f1_data = (f1_data >= fo).astype(int)\n",
    "    f2_data = (f2_data >= fo).astype(int)\n",
    "    f1f2_data = ((f1_data + f2_data) == 2).astype(int)\n",
    "    f1_data -= f1f2_data\n",
    "    f2_data -= f1f2_data\n",
    "    if mask:\n",
    "        mask_data = convert_to_fractional_overlap(nib.load(mask).get_fdata())\n",
    "        mask_data = (mask_data >= 1.).astype(int)\n",
    "        mask_data -= f1f2_data\n",
    "        mask_data -= f1_data\n",
    "        mask_data -= f2_data\n",
    "    data_dict = [(f1_data, f1_c), (f2_data, f2_c), (f1f2_data, f1f2_c)]\n",
    "    if mask:\n",
    "        data_dict.append((mask_data,mask_c))\n",
    "\n",
    "    return map_data_to_value(data_dict)\n",
    "\n",
    "def get_quadrant_id(mask_path):\n",
    "    rel_mask_path = mask_path.split(\"/\")[-1]\n",
    "    idx_start = rel_mask_path.find(\"Q\")\n",
    "    quadrant_id = rel_mask_path[idx_start:idx_start+2]\n",
    "    assert quadrant_id in ['Q1', 'Q2'], f\"{quadrant_id} not Q1 or Q2\"\n",
    "\n",
    "    return quadrant_id\n",
    "\n",
    "def merge_and_binarize_mask(data, f1_c, f2_c, f1f2_c, mask_c):\n",
    "    data_dict = {\n",
    "        \"f1\": data.copy(),\n",
    "        \"f2\": data.copy(),\n",
    "        \"f1Uf2\": data.copy(),\n",
    "    }\n",
    "    data_dict[\"f1\"][(data_dict[\"f1\"]==f1_c) | (data_dict[\"f1\"]==f1f2_c)] = 1\n",
    "    data_dict[\"f1\"][(data_dict[\"f1\"]==f2_c)] = 0\n",
    "    data_dict[\"f2\"][(data_dict[\"f2\"]==f2_c) | (data_dict[\"f2\"]==f1f2_c)] = 1\n",
    "    data_dict[\"f2\"][(data_dict[\"f2\"]==f1_c)] = 0\n",
    "    data_dict[\"f1Uf2\"][(data_dict[\"f1Uf2\"]==f1f2_c)] = 1\n",
    "    data_dict[\"f1Uf2\"][(data_dict[\"f1Uf2\"]==f1_c) | (data_dict[\"f1Uf2\"]==f2_c)] = 0\n",
    "    for v in data_dict.values():\n",
    "        v[v==mask_c] = 0\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "@lru_cache(maxsize=360)\n",
    "def read_roi_path(roi_path):\n",
    "    return nib.load(roi_path).get_fdata()[0,:]\n",
    "\n",
    "def append_data(\n",
    "    df_data,\n",
    "    hcp_mapping,\n",
    "    map_data,\n",
    "    power_f1_data,\n",
    "    power_f2_data,\n",
    "    pd_f1_data, \n",
    "    pd_f2_data, \n",
    "    q_id,\n",
    "    experiment_label, \n",
    "    sub_id, \n",
    "    roi_fo,\n",
    "    roi_task_id,\n",
    "    task_id,\n",
    "):\n",
    "    \"\"\"Create function to store vertex level data for each HCP ROI:\n",
    "    - columns: [cohort_id, sub_ids, quadrant_id, hcp_roi, frequency_of_roi, vertex_count, vertex_coordinates, f1_BOLD_power, f2_BOLD_power, f1_phase_delay, f2_phase_delay]\n",
    "        - roi_fo = region fractional overlap threshold\n",
    "        - cohort_id = cohort_id of each dataset [3T/7T Normal/Vary]\n",
    "            - sub_ids = sub_id of all ROIs in cohort\n",
    "                - quadrant_id = each subject will have a quadrant_id (corresponding to quadrant stimulation)\n",
    "                - hcp_roi = all HCP ROIs, convert L/R to express laterality\n",
    "                    - CONTRA/IPSI\n",
    "                    - frequency_of_roi = each `hcp_roi` will have a ROI corresponding to f1, f2 or both (f1Uf2)\n",
    "                        - vertex_count = each `frequency_of_roi` will have a vertex_count\n",
    "                        - vertex_coordinates = each `frequency_of_roi` will have coordinates to all its vertices\n",
    "                        - f1_BOLD_power = each `frequency_of_roi` will have a np.array of power values corresponding to each vertex\n",
    "                        - f2_BOLD_power = each `frequency_of_roi` will have a np.array of power values corresponding to each vertex\n",
    "                        - f1_phase_delay = each `frequency_of_roi` will have a np.array of phase delay values corresponding to each vertex\n",
    "                        - f2_phase_delay = each `frequency_of_roi` will have a np.array of phase delay values corresponding to each vertex\n",
    "    \"\"\"\n",
    "    for frequency_of_roi, f_data in map_data.items():\n",
    "        for roi_label, roi_path in hcp_mapping.items():\n",
    "            if q_id == \"Q1\":\n",
    "                contra = \"L_\"\n",
    "            elif q_id == \"Q2\":\n",
    "                contra = \"R_\"\n",
    "            else:\n",
    "                raise ValueError(f\"{q_id} not supported.\")\n",
    "\n",
    "            if roi_label.startswith(contra):\n",
    "                roi_label = f\"CONTRA_{roi_label[2:-4]}\"\n",
    "            else:\n",
    "                roi_label = f\"IPSI_{roi_label[2:-4]}\"\n",
    "\n",
    "            roi_mask = read_roi_path(roi_path)\n",
    "            assert roi_mask.shape == f_data.shape\n",
    "\n",
    "            hcp_and_f_roi = roi_mask * f_data\n",
    "            vertex_coordinates = np.where(hcp_and_f_roi == 1)\n",
    "            vertex_count = hcp_and_f_roi.sum()\n",
    "            if vertex_count == 0:\n",
    "                continue\n",
    "\n",
    "            if roi_task_id == \"control\":\n",
    "                f1_BOLD_power = None\n",
    "                f2_BOLD_power = None\n",
    "            else:\n",
    "                f1_BOLD_power = power_f1_data[hcp_and_f_roi==1]\n",
    "                f2_BOLD_power = power_f2_data[hcp_and_f_roi==1]\n",
    "            if task_id != roi_task_id:\n",
    "                f1_phase_delay = None\n",
    "                f2_phase_delay = None\n",
    "            else:\n",
    "                f1_phase_delay = pd_f1_data[hcp_and_f_roi==1]\n",
    "                f2_phase_delay = pd_f2_data[hcp_and_f_roi==1]\n",
    "\n",
    "            df_data[\"roi_task_id\"].append(roi_task_id)\n",
    "            df_data[\"task_id\"].append(task_id)\n",
    "            df_data[\"roi_fo\"].append(roi_fo)\n",
    "            df_data[\"experiment_id\"].append(experiment_label)\n",
    "            df_data[\"sub_id\"].append(sub_id)\n",
    "            df_data[\"quadrant_id\"].append(q_id)\n",
    "            df_data[\"hcp_roi\"].append(roi_label)\n",
    "            df_data[\"frequency_of_roi\"].append(frequency_of_roi)\n",
    "            df_data[\"vertex_count\"].append(vertex_count)\n",
    "            df_data[\"vertex_coordinates\"].append(vertex_coordinates)\n",
    "            df_data[\"f1_BOLD_power\"].append(f1_BOLD_power)\n",
    "            df_data[\"f2_BOLD_power\"].append(f2_BOLD_power)\n",
    "            df_data[\"f1_phase_delay\"].append(f1_phase_delay)\n",
    "            df_data[\"f2_phase_delay\"].append(f2_phase_delay)\n",
    "\n",
    "    return df_data\n",
    "\n",
    "def contains_all_strings(input_str, string_list):\n",
    "    for string in string_list:\n",
    "        if string not in input_str:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_activations(experiment_id, mri_id, roi_task_id, roi_f_1, fo, sub_id, data_split_id=\"train\", match_str=\"activations.dtseries.nii\", additional_match_strs=None, additional_match_str=None, corr_type=\"uncp\"):\n",
    "    import os\n",
    "    directory = f\"/scratch/fastfmri/experiment-{experiment_id}_mri-{mri_id}_smooth-0_truncate-39-219_n-200_batch-merged_desc-basic_roi-{roi_task_id}-{roi_f_1}_pval-{corr_type}_fo-{fo}_bootstrap/sub-{sub_id}/bootstrap/\"\n",
    "    if additional_match_strs is not None:\n",
    "        match_str = additional_match_strs + [match_str, f\"data-{data_split_id}\"]\n",
    "        activations_files = []\n",
    "        for file in os.listdir(directory):\n",
    "            if contains_all_strings(file, match_str):\n",
    "                activations_files.append(file)\n",
    "    else:\n",
    "        activations_files = [file for file in os.listdir(directory) if f'data-{data_split_id}' in file and match_str in file]\n",
    "\n",
    "    return [f\"{directory}{i}\" for i in activations_files]\n",
    "\n",
    "def set_base_dir(basedir):\n",
    "    basedir = Path(basedir)\n",
    "    if not basedir.exists():\n",
    "        basedir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    return basedir\n",
    "\n",
    "def load_mean_dtseries(dtseries):\n",
    "    mean_power = nib.load(dtseries).get_fdata().mean(0)\n",
    "    return mean_power\n",
    "\n",
    "def generate_single_subject_maps(\n",
    "    label, experiment_id, mri_id, sub_ids, \n",
    "    roi_task_ids, roi_f_1s, roi_f_2s, roi_fo,\n",
    "    df_data=None,\n",
    "    corr_type=\"uncp\",\n",
    "    ROI_FO=.8, SUB_THRESHOLD=.5,\n",
    "    LEFT=590, TOP=80, RIGHT=1140, BOTTOM=460, VERTEX_TO = 59412,\n",
    "    FORCE_TASK_ID=None,\n",
    "):\n",
    "\n",
    "    if df_data is None:\n",
    "        df_data = defaultdict(list)\n",
    "\n",
    "    for ix, (sub_id, roi_task_id, roi_f_1, roi_f_2) in enumerate(zip(\n",
    "        sub_ids,\n",
    "        roi_task_ids, \n",
    "        roi_f_1s,\n",
    "        roi_f_2s,\n",
    "    )):\n",
    "\n",
    "        if FORCE_TASK_ID is None:\n",
    "            _roi_task_id = roi_task_id\n",
    "        else:\n",
    "            _roi_task_id = FORCE_TASK_ID\n",
    "\n",
    "        png_out = Path(set_base_dir(f\"./ComputeCanada/frequency_tagging/figures/dual_frequency_mapping\")) / f\"label-{label}_mri-{mri_id}_sub-{sub_id}_task-{roi_task_id}_f-{roi_f_1}-{roi_f_2}_corr-{corr_type}_fo-{ROI_FO}.png\"\n",
    "        dscalar_out = Path(set_base_dir(f\"./ComputeCanada/frequency_tagging/figures/dual_frequency_mapping_cifti\")) / f\"label-{label}_mri-{mri_id}_sub-{sub_id}_task-{roi_task_id}_f-{roi_f_1}-{roi_f_2}_corr-{corr_type}_fo-{ROI_FO}.dtseries.nii\"\n",
    "        if png_out.exists():\n",
    "            #continue\n",
    "            pass\n",
    "\n",
    "        f1 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_1, .8, sub_id, match_str=\"activations.dtseries.nii\", corr_type=corr_type)\n",
    "        f2 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_2, .8, sub_id, match_str=\"activations.dtseries.nii\", corr_type=corr_type)\n",
    "        mask = find_activations(experiment_id, mri_id, roi_task_id, roi_f_1, .8, sub_id, match_str=\"mask.dtseries.nii\", corr_type=corr_type)\n",
    "        pd_f1 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_1, .8, sub_id, data_split_id = \"train\", match_str=\"phasedelay.dtseries.nii\", additional_match_strs=[roi_task_id,f\"f-{roi_f_1}\"], corr_type=corr_type)\n",
    "        pd_f2 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_2, .8, sub_id, data_split_id = \"train\", match_str=\"phasedelay.dtseries.nii\", additional_match_strs=[roi_task_id,f\"f-{roi_f_2}\"], corr_type=corr_type)\n",
    "        power_f1 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_1, .8, sub_id, data_split_id = \"test\", match_str=\"power.dtseries.nii\", additional_match_strs=[_roi_task_id,f\"f-{roi_f_1}\"], corr_type=corr_type)\n",
    "        power_f2 = find_activations(experiment_id, mri_id, roi_task_id, roi_f_2, .8, sub_id, data_split_id = \"test\", match_str=\"power.dtseries.nii\", additional_match_strs=[_roi_task_id,f\"f-{roi_f_2}\"], corr_type=corr_type)\n",
    "        for f_label, f in zip([\"f1\",\"f2\",\"mask\",\"pd_f1\",\"pd_f2\",\"power_f1\",\"power_f2\"], [f1,f2, mask, pd_f1, pd_f2, power_f1, power_f2]):\n",
    "            if roi_task_id == \"control\" and experiment_id == \"1_frequency_tagging\":\n",
    "                if f_label in [\"f1\", \"f2\", \"mask\"]:\n",
    "                    assert len(f) == 1, f\"{sub_id}, {f_label} - {f}\"\n",
    "            else:\n",
    "                assert len(f) == 1, f\"{sub_id}, {f_label} - {f}, {experiment_id} {roi_task_id}\"\n",
    "\n",
    "        f1, f2 = f1[0], f2[0]\n",
    "        data = combine_f1_f2(f1, f2, fo=ROI_FO, mask=mask[0], f1_c=f1_c,f2_c=f2_c,f1f2_c=f1f2_c,mask_c=mask_c)\n",
    "        data = data[:VERTEX_TO]\n",
    "\n",
    "        map_data = merge_and_binarize_mask(data,f1_c,f2_c,f1f2_c,mask_c)\n",
    "        pd_f1_data = load_mean_dtseries(pd_f1[0])[:VERTEX_TO]\n",
    "        pd_f2_data = load_mean_dtseries(pd_f2[0])[:VERTEX_TO]\n",
    "        if roi_task_id == \"control\" and experiment_id == \"1_frequency_tagging\":\n",
    "            power_f1_data = None\n",
    "            power_f2_data = None\n",
    "        # Power metrics were not calculated for control task condition (no voxels allocated to task-control ROIs)\n",
    "        else:\n",
    "            power_f1_data = load_mean_dtseries(power_f1[0])[:VERTEX_TO]\n",
    "            power_f2_data = load_mean_dtseries(power_f2[0])[:VERTEX_TO]\n",
    "        q_id = get_quadrant_id(mask[0])\n",
    "        df_data = append_data(\n",
    "            df_data, \n",
    "            hcp_mapping, \n",
    "            map_data, \n",
    "            power_f1_data, \n",
    "            power_f2_data, \n",
    "            pd_f1_data, \n",
    "            pd_f2_data, \n",
    "            q_id,\n",
    "            label, \n",
    "            sub_id,\n",
    "            roi_fo,\n",
    "            roi_task_id,\n",
    "            _roi_task_id, \n",
    "        )\n",
    "\n",
    "        palette_params = {\n",
    "            \"disp-zero\": False,\n",
    "            \"disp-neg\": True,\n",
    "            \"disp-pos\": True,\n",
    "            \"pos-user\": (0, 1.),\n",
    "            \"neg-user\": (-1,0),\n",
    "            \"interpolate\": True,\n",
    "        }\n",
    "        # Save f1f2 map as dtseries\n",
    "        f1_img = nib.load(f1)\n",
    "        dscalar_to_save_as_cifti = np.zeros((1,f1_img.shape[-1]))\n",
    "        dscalar_to_save_as_cifti[0,:VERTEX_TO] = data\n",
    "        f1f2_img = nib.Cifti2Image(dscalar_to_save_as_cifti, header=f1_img.header)\n",
    "        f1f2_img.header.matrix[0].number_of_series_points = 1\n",
    "        nib.save(f1f2_img, dscalar_out)\n",
    "        dscalar(\n",
    "            png_out, data, \n",
    "            orientation=\"portrait\", \n",
    "            hemisphere='right',\n",
    "            palette=PALETTE, \n",
    "            palette_params=palette_params,\n",
    "            transparent=False,\n",
    "            flatmap=True,\n",
    "            flatmap_style='plain',\n",
    "        )\n",
    "        crop_and_save(png_out, str(png_out).replace(\"png\", \"cropped.png\"), LEFT, TOP, RIGHT, BOTTOM)\n",
    "        \n",
    "        track = [len(v) for k,v in df_data.items()]\n",
    "        print(track)\n",
    "\n",
    "    return df_data\n",
    "\n",
    "def crop_and_save(input_file, output_file, left, top, right, bottom):\n",
    "    from PIL import Image\n",
    "    try:\n",
    "        # Open the input image\n",
    "        with Image.open(input_file) as img:\n",
    "            # Crop the image\n",
    "            cropped_img = img.crop((left, top, right, bottom))\n",
    "            # Save the cropped image\n",
    "            cropped_img.save(output_file)\n",
    "            print(\"Cropped image saved successfully as\", output_file)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save visualizations and create DataFrame storing data for simple analysis\n",
    "- Datasets\n",
    "    - `3TNormal` ($f_1$=.125, $f_2$=.2)\n",
    "    - `7TNormal` ($f_1$=.125, $f_2$=.2)\n",
    "    - `3TVary` varying frequencies \n",
    "    - `7TVary` varying frequencies\n",
    "- Region info `df.frequency_of_roi` and `df.hcp_roi`\n",
    "    - Region data of $f_1$ *include* $f_1$&$f_2$ intersected vertices, same goes for $f_2$\n",
    "    - roi choice includes `[\"f1\",\"f2\",\"f1Uf2\"]`, where `f1Uf2` denotes intersected vertices\n",
    "- Data includes\n",
    "    - Note: to load `fdrp` (from `uncp`) corrected data change the `corr_type` variable in the cell below\n",
    "    - `df.roi_task_id` task used to generate the ROI\n",
    "    - `df.roi_fo` fractional overlap used to generate the ROI\n",
    "    - ...\n",
    "    - `df.hcp_roi` HCP ROI used to filter data from\n",
    "    - `df.frequency_of_roi` frequency of ROI used to filter data from (related to the frequency of `df.roi_task_id`)\n",
    "    - `df.vertex_count` total vertices in the HCP ROI & identified with the frequency of the task\n",
    "    - `df.vertex_coordinates` coordinates on a 32k_fs_LR surface (consistent with file structure of `template_dscalar`)\n",
    "    - `df.f1_[BOLD_power,phase_delay]` vertex-wise power extracted from $f_1$ region (this is set to 0 for $f_2$-only regions)\n",
    "    - `df.f2_[BOLD_power,phase_delay]` vertex-wise power extracted from $f_2$ region (this is set to 0 for $f_1$-only regions)\n",
    "        - `f1Uf2` regions contain both $f_1$ and $f_2$ metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set up for visualizing dual frequency tagging across each subject using fractional overlap\n",
    "\"\"\"\n",
    "PALETTE = \"power_surf\"\n",
    "f1_c = -.1 # red -.1\n",
    "f2_c = .82 # blue .82\n",
    "f1f2_c = .14 # white .88 yellow .1\n",
    "mask_c = .41 # .9 [green], .1 [goldish], .4 [black]\n",
    "\n",
    "cohort_roi_info_across_experiments = {}\n",
    "ROI_FOS = [.8,1.]\n",
    "corr_type = \"uncp\"\n",
    "\n",
    "\"\"\"Save png\n",
    "\"\"\"\n",
    "# 3T control under entrain condition (set this to get power measurements with entrain ROIs)\n",
    "label = \"3TNormal\"\n",
    "df_data = None\n",
    "for _roi_task_id in [\"entrain\"]:\n",
    "    for ROI_FO in ROI_FOS:\n",
    "        experiment_id = \"1_frequency_tagging\" \n",
    "        mri_id = \"3T\"\n",
    "        sub_ids = [\"000\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\"] \n",
    "        roi_task_ids = [_roi_task_id] * len(sub_ids)\n",
    "        roi_f_1s = [.125] * len(sub_ids)\n",
    "        roi_f_2s = [.2] * len(sub_ids)\n",
    "        df_data = generate_single_subject_maps(\n",
    "            label, experiment_id, mri_id, sub_ids, \n",
    "            roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO,\n",
    "            df_data=df_data,\n",
    "            corr_type=corr_type,\n",
    "            ROI_FO=ROI_FO, SUB_THRESHOLD=.5,\n",
    "            FORCE_TASK_ID=\"control\"\n",
    "        )\n",
    "# 3T normal\n",
    "label = \"3TNormal\"\n",
    "for _roi_task_id in [\"entrain\"]:\n",
    "    for ROI_FO in ROI_FOS:\n",
    "        experiment_id = \"1_frequency_tagging\" \n",
    "        mri_id = \"3T\"\n",
    "        sub_ids = [\"000\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\"] \n",
    "        roi_task_ids = [_roi_task_id] * len(sub_ids)\n",
    "        roi_f_1s = [.125] * len(sub_ids)\n",
    "        roi_f_2s = [.2] * len(sub_ids)\n",
    "        df_data = generate_single_subject_maps(\n",
    "            label, experiment_id, mri_id, sub_ids, \n",
    "            roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO,\n",
    "            df_data=df_data,\n",
    "            corr_type=corr_type,\n",
    "            ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "        )\n",
    "# 7T normal\n",
    "label = \"7TNormal\"\n",
    "for ROI_FO in ROI_FOS:\n",
    "    experiment_id = \"1_attention\" \n",
    "    mri_id = \"7T\"\n",
    "    sub_ids = [\"Pilot001\", \"Pilot009\", \"Pilot010\", \"Pilot011\"]\n",
    "    roi_task_ids = [\"AttendAway\"] * len(sub_ids)\n",
    "    roi_f_1s = [.125] * len(sub_ids)\n",
    "    roi_f_2s = [.2] * len(sub_ids)\n",
    "    df_data = generate_single_subject_maps(\n",
    "        label, experiment_id, mri_id, sub_ids, \n",
    "        roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO,\n",
    "        df_data=df_data,\n",
    "        corr_type=corr_type,\n",
    "        ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "    )\n",
    "# 3T vary\n",
    "label = \"3TVary\"\n",
    "for ROI_FO in ROI_FOS:\n",
    "    experiment_id = \"1_frequency_tagging\"\n",
    "    mri_id = \"3T\"\n",
    "    sub_ids = [\"020\"] * 3 + [\"021\"] * 3\n",
    "    roi_task_ids = [f\"entrain{i}\" for i in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]]\n",
    "    roi_f_1s = [.125] * 3 + [.125, .15, .175]\n",
    "    roi_f_2s = [.2, .175, .15] + [.2] * 3\n",
    "    df_data = generate_single_subject_maps(\n",
    "        label, experiment_id, mri_id, sub_ids, \n",
    "        roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO,\n",
    "        df_data=df_data,\n",
    "        corr_type=corr_type,\n",
    "        ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "    )\n",
    "# 7T vary\n",
    "label = \"7TVary\"\n",
    "for ROI_FO in ROI_FOS:\n",
    "    experiment_id = \"1_frequency_tagging\"\n",
    "    mri_id = \"7T\"\n",
    "    sub_ids = [\"020\"] * 3 + [\"021\"] * 3\n",
    "    roi_task_ids = [f\"entrain{i}\" for i in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]]\n",
    "    roi_f_1s = [.125] * 3 + [.125, .15, .175]\n",
    "    roi_f_2s = [.2, .175, .15] + [.2] * 3\n",
    "    df_data = generate_single_subject_maps(\n",
    "        label, experiment_id, mri_id, sub_ids, \n",
    "        roi_task_ids, roi_f_1s, roi_f_2s, ROI_FO,\n",
    "        df_data=df_data,\n",
    "        corr_type=corr_type,\n",
    "        ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

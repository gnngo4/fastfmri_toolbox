{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/opt/wbplot\")\n",
    "\n",
    "from wbplot import dscalar\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from scipy.stats import wilcoxon\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append(\"ComputeCanada/frequency_tagging\")\n",
    "from dfm import (\n",
    "    get_frequency_text_codes,\n",
    "    get_roi_colour_codes,\n",
    "    change_font,\n",
    ")\n",
    "change_font()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get HCP info\n",
    "- `hcp_mappings`: dict of ROI: dscalars\n",
    "- `hcp_rois`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Get HCP labels\n",
    "\"\"\"\n",
    "dlabel_dir = Path(\"/opt/app/notebooks/data/dlabels\")\n",
    "hcp_label = dlabel_dir / \"Q1-Q6_RelatedValidation210.CorticalAreas_dil_Final_Final_Areas_Group_Colors.32k_fs_LR.dlabel.nii\"\n",
    "\n",
    "_HCP_INFO = !wb_command -file-information {hcp_label}\n",
    "HCP_LABELS = []\n",
    "HCP_COUNTER = 0\n",
    "for i in _HCP_INFO:\n",
    "    if len(i) == 60 and any([\"L_\" in i, \"R_\" in i]):\n",
    "        hcp_colors = tuple([float(f\"0.{k}\") for k in [j.split(' ') [0] for j in i.split('0.')][-3:]] + [1])\n",
    "        if ' R_' in i:\n",
    "            roi = i.split(\"_ROI\")[0].split(' R_')[1]\n",
    "            HCP_LABELS.append(f\"R_{roi}_ROI\")\n",
    "        if ' L_' in i:\n",
    "            roi = i.split(\"_ROI\")[0].split(' L_')[1]\n",
    "            HCP_LABELS.append(f\"L_{roi}_ROI\")\n",
    "        HCP_COUNTER += 1\n",
    "\n",
    "\"\"\"Get HCP label coordinates\n",
    "\"\"\"\n",
    "dscalar_dir = Path(\"/opt/app/notebooks/data/dscalars\")\n",
    "tmpdir = Path(\"/tmp\")\n",
    "\n",
    "hcp_mapping = {}\n",
    "for roi_label in HCP_LABELS:\n",
    "    out_dscalar = tmpdir / f\"{roi_label}.dscalar.nii\"\n",
    "    if out_dscalar.exists():\n",
    "        hcp_mapping[roi_label] = out_dscalar\n",
    "        continue\n",
    "    !wb_command -cifti-label-to-roi {hcp_label} {out_dscalar} -name {roi_label}\n",
    "    assert out_dscalar.exists(), f\"{out_dscalar.stem} does not exist.\"\n",
    "    hcp_mapping[roi_label] = out_dscalar\n",
    "hcp_rois = list(set([k.split('_')[1] for k in hcp_mapping.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_dir = Path(\"/opt/app/notebooks/data/surfaces\")\n",
    "tmpdir = Path(\"/tmp\")\n",
    "L_mid = surface_dir / \"S1200.L.midthickness_MSMAll.32k_fs_LR.surf.gii\"\n",
    "R_mid = surface_dir / \"S1200.R.midthickness_MSMAll.32k_fs_LR.surf.gii\"\n",
    "L_geo = tmpdir / \"L.dconn.nii\"\n",
    "R_geo = tmpdir / \"R.dconn.nii\"\n",
    "!wb_command -surface-geodesic-distance-all-to-all {L_mid} {L_geo}\n",
    "!wb_command -surface-geodesic-distance-all-to-all {R_mid} {R_geo}\n",
    "\n",
    "template_dscalar = dscalar_dir / \"S1200.MyelinMap_BC_MSMAll.32k_fs_LR.dscalar.nii\"\n",
    "L_V1_ROI = tmpdir / \"L_V1_ROI.dscalar.nii\"\n",
    "R_V1_ROI = tmpdir / \"R_V1_ROI.dscalar.nii\"\n",
    "L_V1_GD = tmpdir / \"L_V1_ROI.gd.dscalar.nii\"\n",
    "R_V1_GD = tmpdir / \"R_V1_ROI.gd.dscalar.nii\"\n",
    "    \n",
    "!wb_command -cifti-create-dense-from-template {template_dscalar} {L_V1_GD} -cifti {L_V1_ROI}\n",
    "!wb_command -cifti-create-dense-from-template {template_dscalar} {R_V1_GD} -cifti {R_V1_ROI}\n",
    "\n",
    "L_V1_coords = nib.load(L_V1_GD).get_fdata()[0,:32492]==1\n",
    "_L_geo = nib.load(L_geo).get_fdata()\n",
    "L_geo_arr = _L_geo[L_V1_coords,:].mean(0)\n",
    "del _L_geo\n",
    "R_V1_coords = nib.load(R_V1_GD).get_fdata()[0,32492:]==1\n",
    "_R_geo = nib.load(R_geo).get_fdata()\n",
    "R_geo_arr = _R_geo[R_V1_coords,:].mean(0)\n",
    "del _R_geo\n",
    "\n",
    "geo_arr = np.concatenate((L_geo_arr, R_geo_arr))\n",
    "\n",
    "geodesic_dscalar = tmpdir / \"geodesic_V1.dscalar.nii\"\n",
    "img = nib.load(L_V1_GD)\n",
    "data = np.zeros(img.shape)\n",
    "data[0,:] = geo_arr\n",
    "geo_img = nib.Cifti2Image(data, header=img.header)\n",
    "nib.save(geo_img, geodesic_dscalar)\n",
    "!wb_command -cifti-create-dense-from-template {L_V1_ROI} {geodesic_dscalar} -cifti {geodesic_dscalar}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_f_im_with_f1_f2(f_im,f_1,f_2,fo=1.,mask=None,f_1_c=-.1,f_2_c=.82,f1f2_c=.1,f_im_c=.9,mask_c=.4,):\n",
    "    f_1_data = convert_to_fractional_overlap(nib.load(f_1).get_fdata())\n",
    "    f_1_data = (f_1_data >= fo).astype(int)\n",
    "    f_2_data = convert_to_fractional_overlap(nib.load(f_2).get_fdata())\n",
    "    f_2_data = (f_2_data >= fo).astype(int)\n",
    "    f1f2_data = ((f_1_data + f_2_data) == 2).astype(int) # Intersection of f1 & f2\n",
    "    f_1_data -= f1f2_data # f1 only\n",
    "    f_2_data -= f1f2_data # f2 only\n",
    "    f_im_data = convert_to_fractional_overlap(nib.load(f_im).get_fdata())\n",
    "    f_im_data = (f_im_data >= fo).astype(int) # f_im\n",
    "    \n",
    "    # Recolour f_im_data with f_1, f_2 and f1f2 (show f1, f2 and f1f2 that appears in fim) \n",
    "    if mask:\n",
    "        mask_data = convert_to_fractional_overlap(nib.load(mask).get_fdata())\n",
    "        mask_data = (mask_data >= 1.).astype(int)\n",
    "    data_dict = []\n",
    "    for f_data,f_c in zip([f_1_data, f_2_data, f1f2_data],[f_1_c, f_2_c,f1f2_c]):\n",
    "        _f_data = ((f_data+f_im_data)==2).astype(int) # Intersection of f_im and f (f_data)\n",
    "        f_im_data -= _f_data\n",
    "        if mask:\n",
    "            mask_data -= _f_data\n",
    "        data_dict.append((_f_data,f_c))\n",
    "    data_dict.append((f_im_data,f_im_c))\n",
    "    if mask:\n",
    "        mask_data -= f_im_data\n",
    "        data_dict.append((mask_data,mask_c))\n",
    "    X = map_data_to_value(data_dict)\n",
    "    \n",
    "    # Recolour f_im_data with f_1, f_2 and f1f2 (show f1, f2 and f1f2 that does not appears in fim) \n",
    "    f_im_data = convert_to_fractional_overlap(nib.load(f_im).get_fdata())\n",
    "    f_im_data = (f_im_data >= fo).astype(int) # f_im\n",
    "    if mask:\n",
    "        mask_data = convert_to_fractional_overlap(nib.load(mask).get_fdata())\n",
    "        mask_data = (mask_data >= 1.).astype(int)\n",
    "    data_dict = []\n",
    "    for f_data,f_c in zip([f_1_data, f_2_data, f1f2_data],[f_1_c, f_2_c,f1f2_c]):\n",
    "        _f_data = ((f_data+f_im_data)==2).astype(int) # Intersection of f_im and f (f_data)\n",
    "        _f_data_only = f_data.copy() # f-only, and does not include any IM vertices\n",
    "        _f_data_only -= _f_data\n",
    "        if mask:\n",
    "            mask_data -= _f_data_only\n",
    "        data_dict.append((_f_data_only,f_c))\n",
    "    data_dict.append((f_im_data,f_im_c))\n",
    "    if mask:\n",
    "        mask_data -= f_im_data\n",
    "        data_dict.append((mask_data,mask_c))\n",
    "    Y = map_data_to_value(data_dict)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def binarize_mask(data, f_im_c, mask_c, im_key):\n",
    "    data_dict = {\n",
    "        im_key: data.copy(),\n",
    "    }\n",
    "    data_dict[im_key][(data_dict[im_key]==f_im_c)] = 1\n",
    "    for v in data_dict.values():\n",
    "        v[v==mask_c] = 0\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def append_data(\n",
    "    df_data,\n",
    "    hcp_mapping,\n",
    "    map_data,\n",
    "    power_im_data,\n",
    "    pd_im_data, \n",
    "    q_id,\n",
    "    experiment_label, \n",
    "    sub_id, \n",
    "    roi_fo,\n",
    "    roi_task_id,\n",
    "    task_id,\n",
    "    im_str,\n",
    "    im_f,\n",
    "):\n",
    "    \"\"\"Create function to store vertex level data for each HCP ROI:\n",
    "    - columns: [cohort_id, sub_ids, quadrant_id, hcp_roi, im_code, vertex_count, vertex_coordinates, f_im_BOLD_power, f_im_phase_delay]\n",
    "        - roi_fo = region fractional overlap threshold\n",
    "        - cohort_id = cohort_id of each dataset [3T/7T Normal/Vary]\n",
    "            - sub_ids = sub_id of all ROIs in cohort\n",
    "                - quadrant_id = each subject will have a quadrant_id (corresponding to quadrant stimulation)\n",
    "                - hcp_roi = all HCP ROIs, convert L/R to express laterality\n",
    "                    - CONTRA/IPSI\n",
    "                    - im_code = each `hcp_roi` will have a ROI corresponding to f1, f2 or both (f1Uf2)\n",
    "                        - vertex_count = each `im_code` will have a vertex_count\n",
    "                        - vertex_coordinates = each `im_code` will have coordinates to all its vertices\n",
    "                        - f_im_BOLD_power = each `im_code` will have a np.array of power values corresponding to each vertex\n",
    "                        - f_im_phase_delay = each `im_code` will have a np.array of phase delay values corresponding to each vertex\n",
    "    \"\"\"\n",
    "    for im_code, f_data in map_data.items():\n",
    "        for roi_label, roi_path in hcp_mapping.items():\n",
    "            if q_id == \"Q1\":\n",
    "                contra = \"L_\"\n",
    "            elif q_id == \"Q2\":\n",
    "                contra = \"R_\"\n",
    "            else:\n",
    "                raise ValueError(f\"{q_id} not supported.\")\n",
    "\n",
    "            if roi_label.startswith(contra):\n",
    "                roi_label = f\"CONTRA_{roi_label[2:-4]}\"\n",
    "            else:\n",
    "                roi_label = f\"IPSI_{roi_label[2:-4]}\"\n",
    "\n",
    "            roi_mask = read_roi_path(roi_path)\n",
    "            assert roi_mask.shape == f_data.shape\n",
    "\n",
    "            hcp_and_f_roi = roi_mask * f_data\n",
    "            vertex_coordinates = np.where(hcp_and_f_roi == 1)\n",
    "            vertex_count = hcp_and_f_roi.sum()\n",
    "            if vertex_count == 0:\n",
    "                continue\n",
    "\n",
    "            if roi_task_id == \"control\":\n",
    "                f_im_BOLD_power = None\n",
    "            elif power_im_data is None:\n",
    "                f_im_BOLD_power = None\n",
    "            else:\n",
    "                f_im_BOLD_power = power_im_data[hcp_and_f_roi==1]\n",
    "            if task_id != roi_task_id:\n",
    "                f_im_phase_delay = None\n",
    "            else:\n",
    "                f_im_phase_delay = pd_im_data[hcp_and_f_roi==1]\n",
    "\n",
    "            df_data[\"roi_task_id\"].append(roi_task_id)\n",
    "            df_data[\"task_id\"].append(task_id)\n",
    "            df_data[\"roi_fo\"].append(roi_fo)\n",
    "            df_data[\"experiment_id\"].append(experiment_label)\n",
    "            df_data[\"sub_id\"].append(sub_id)\n",
    "            df_data[\"quadrant_id\"].append(q_id)\n",
    "            df_data[\"hcp_roi\"].append(roi_label)\n",
    "            df_data[\"im_code\"].append(im_code)\n",
    "            df_data[\"vertex_count\"].append(vertex_count)\n",
    "            df_data[\"vertex_coordinates\"].append(vertex_coordinates)\n",
    "            df_data[\"f_im_BOLD_power\"].append(f_im_BOLD_power)\n",
    "            df_data[\"f_im_phase_delay\"].append(f_im_phase_delay)\n",
    "            df_data[\"im_type\"].append(im_str)\n",
    "            df_data[\"im_f\"].append(im_f)\n",
    "\n",
    "    return df_data\n",
    "\n",
    "def calculate_percentile(values, percentile):\n",
    "    if not values:\n",
    "        print(values)\n",
    "        raise ValueError(\"The list of values cannot be empty.\")\n",
    "    if not (0 <= percentile <= 1):\n",
    "        raise ValueError(\"Percentile must be between 0 and 1.\")\n",
    "    \n",
    "    sorted_values = sorted(values)\n",
    "    N = len(sorted_values)\n",
    "    R = percentile * (N - 1)\n",
    "    \n",
    "    lower_index = int(R)\n",
    "    upper_index = min(lower_index + 1, N - 1)\n",
    "    fraction = R - lower_index\n",
    "    \n",
    "    return sorted_values[lower_index] + (sorted_values[upper_index] - sorted_values[lower_index]) * fraction\n",
    "\n",
    "\n",
    "def generate_single_subject_maps(\n",
    "    label, experiment_id, mri_id, sub_ids, \n",
    "    roi_task_ids, im_frequencies, hcp_labels,\n",
    "    df_data=None,\n",
    "    proportion_data=None,\n",
    "    corr_type=\"uncp\", \n",
    "    ROI_FO=.8, SUB_THRESHOLD=.5,\n",
    "    LEFT=590, TOP=80, RIGHT=1140, BOTTOM=460, VERTEX_TO = 59412,\n",
    "    FORCE_TASK_ID=None,\n",
    "    mask_c = .41,\n",
    "    PALETTE=\"power_surf\"\n",
    "):\n",
    "    \n",
    "    if df_data is None:\n",
    "        df_data = defaultdict(list)\n",
    "\n",
    "    if proportion_data is None:\n",
    "        proportion_data = defaultdict(list)\n",
    "\n",
    "    for ix, (sub_id, roi_task_id) in enumerate(zip(sub_ids,roi_task_ids)):\n",
    "\n",
    "        if FORCE_TASK_ID is None:\n",
    "            _roi_task_id = roi_task_id\n",
    "        else:\n",
    "            _roi_task_id = FORCE_TASK_ID\n",
    "        \"\"\" \n",
    "        im_frequencies = {\n",
    "            \"first_order\": [(\"f1\",.125), ...],\n",
    "            \"second_order\": [(\"f2-f1\",.075), ...],\n",
    "            \"third_order\": [(\"2f1-f2\",.05), ...],\n",
    "        }\n",
    "        \"\"\"\n",
    "        for im_order, v in im_frequencies.items():\n",
    "            im_strs = [i[0] for i in v]\n",
    "            im_fs = [i[1] for i in v]\n",
    "            for im_str, im_f in zip(im_strs, im_fs):\n",
    "                png_out = Path(set_base_dir(f\"./ComputeCanada/frequency_tagging/figures/im_frequency_mapping\")) / f\"label-{label}_mri-{mri_id}_sub-{sub_id}_task-{roi_task_id}_f-{im_order}-{im_str}-{im_f}_corr-{corr_type}_fo-{ROI_FO}.png\"\n",
    "                dscalar_out = Path(set_base_dir(f\"./ComputeCanada/frequency_tagging/figures/im_frequency_mapping_cifti\")) / f\"label-{label}_mri-{mri_id}_sub-{sub_id}_task-{roi_task_id}_f-{im_order}-{im_str}-{im_f}_corr-{corr_type}_fo-{ROI_FO}.dtseries.nii\"\n",
    "                if png_out.exists():\n",
    "                    pass\n",
    "\n",
    "                f_1_str = im_frequencies[\"first_order\"][0][0]\n",
    "                f_2_str = im_frequencies[\"first_order\"][1][0]\n",
    "                f_1 = im_frequencies[\"first_order\"][0][1]\n",
    "                f_2 = im_frequencies[\"first_order\"][1][1]\n",
    "                assert f_2 > f_1, f\"{f_2} <= {f_1}\"\n",
    "                f_1 = find_activations(experiment_id, mri_id, roi_task_id, f_1, .8, sub_id, match_str=\"activations.dtseries.nii\", corr_type=corr_type)\n",
    "                f_2 = find_activations(experiment_id, mri_id, roi_task_id, f_2, .8, sub_id, match_str=\"activations.dtseries.nii\", corr_type=corr_type)\n",
    "                f_im = find_activations(experiment_id, mri_id, roi_task_id, im_f, .8, sub_id, match_str=\"activations.dtseries.nii\", corr_type=corr_type)\n",
    "                mask = find_activations(experiment_id, mri_id, roi_task_id, im_f, .8, sub_id, match_str=\"mask.dtseries.nii\", corr_type=corr_type)\n",
    "                pd_im = find_activations(experiment_id, mri_id, roi_task_id, im_f, .8, sub_id, data_split_id = \"train\", match_str=\"phasedelay.dtseries.nii\", additional_match_strs=[roi_task_id,f\"f-{im_f}\"], corr_type=corr_type)\n",
    "                power_im = find_activations(experiment_id, mri_id, roi_task_id, im_f, .8, sub_id, data_split_id = \"test\", match_str=\"power.dtseries.nii\", additional_match_strs=[_roi_task_id,f\"f-{im_f}\"], corr_type=corr_type)\n",
    "                for f_label, f in zip([f_1_str,f_2_str,im_str,\"mask\",f\"pd_{im_str}\",f\"power_{im_str}\"], [f_1,f_2,f_im,mask,pd_im,power_im]):\n",
    "                    if roi_task_id == \"control\" and experiment_id == \"1_frequency_tagging\":\n",
    "                        if f_label in [im_str,\"mask\"]:\n",
    "                            assert len(f) == 1, f\"{sub_id}, {f_label} - {f}\"\n",
    "                    else: # AssertionError: 2f2/0.4, 002, power_2f2 - [], 1_frequency_tagging entrain\n",
    "                        if not f_label.startswith(\"power\"):\n",
    "                            assert len(f) == 1, f\"{im_str}/{im_f}, {sub_id}, {f_label} - {f}, {experiment_id} {roi_task_id}\"\n",
    "                f_1 = f_1[0] # f_1 path\n",
    "                f_2 = f_2[0] # f_2 path\n",
    "                f_im = f_im[0] # f_im path\n",
    "                # Create image for im only\n",
    "                data = convert_f_im(f_im, fo=ROI_FO, mask=mask[0], f_im_c=f_im_c, mask_c=mask_c)\n",
    "                data = data[:VERTEX_TO]\n",
    "                # Create image for im, contextualized by f1 and f2\n",
    "                data_contextualized_1, data_contextualized_2 = convert_f_im_with_f1_f2(f_im, f_1, f_2, fo=ROI_FO, mask=mask[0], f_1_c=f_1_c, f_2_c=f_2_c, f1f2_c=f1f2_c, f_im_c=f_im_c, mask_c=mask_c)\n",
    "                data_contextualized_1 = data_contextualized_1[:VERTEX_TO]\n",
    "                data_contextualized_2 = data_contextualized_2[:VERTEX_TO]\n",
    "\n",
    "                # Get total vertex count\n",
    "                c_per_label = [f_1_c,f_2_c,f1f2_c,f_im_c]\n",
    "                for hcp_label in hcp_labels:\n",
    "                    hcp_roi_mask = read_roi_path(f\"/tmp/{hcp_label}.dscalar.nii\")\n",
    "                    vertex_in_hcp_roi = hcp_roi_mask.sum()\n",
    "                    _data_contextualized_1 = data_contextualized_1 * hcp_roi_mask\n",
    "                    total_vertex = 0\n",
    "                    for _f_c in c_per_label:\n",
    "                        total_vertex += (_data_contextualized_1 == _f_c).sum()\n",
    "                    total_vertex_with_slab = total_vertex + (_data_contextualized_1 == mask_c).sum()\n",
    "                    # Get proportion per label\n",
    "                    if total_vertex > 0:\n",
    "                        vertex_label_dict = {}\n",
    "                        f_labels = [\"f1\",\"f2\",\"f1&f2\",\"fim\"]\n",
    "                        for _f_label, _f_c in zip(f_labels, c_per_label):\n",
    "                            vertex_label_dict[_f_label] = (_data_contextualized_1 == _f_c).sum() / total_vertex\n",
    "                        vertex_label_dict[\"hcp_label\"] = hcp_label\n",
    "                        vertex_label_dict[\"activated_vertex_count\"] = total_vertex\n",
    "                        vertex_label_dict[\"slab_vertex_count\"] = total_vertex_with_slab\n",
    "                        vertex_label_dict[\"hcp_vertex_count\"] = vertex_in_hcp_roi\n",
    "                        vertex_label_dict[\"sub_id\"] = sub_id\n",
    "                        vertex_label_dict[\"im_code\"] = im_str\n",
    "                        vertex_label_dict[\"roi_task_id\"] = roi_task_id\n",
    "                        vertex_label_dict[\"experiment\"] = label\n",
    "                        for k,v in vertex_label_dict.items():\n",
    "                            proportion_data[k].append(v)\n",
    "\n",
    "                map_data = binarize_mask(data,f_im_c,mask_c,im_str)\n",
    "                pd_im_data = load_mean_dtseries(pd_im[0])[:VERTEX_TO]\n",
    "                # Power metrics were not calculated for control task condition (no voxels allocated to task-control ROIs)\n",
    "                if roi_task_id == \"control\" and experiment_id == \"1_frequency_tagging\":\n",
    "                    power_im_data = None\n",
    "                # Power metrics were not calculated for some IM frequencies where voxels were not allocated\n",
    "                elif len(power_im)==0:\n",
    "                    power_im_data = None\n",
    "                else:\n",
    "                    power_im_data = load_mean_dtseries(power_im[0])[:VERTEX_TO]\n",
    "                q_id = get_quadrant_id(mask[0])\n",
    "                df_data = append_data(\n",
    "                    df_data, \n",
    "                    hcp_mapping, \n",
    "                    map_data, \n",
    "                    power_im_data, \n",
    "                    pd_im_data, \n",
    "                    q_id,\n",
    "                    label, \n",
    "                    sub_id,\n",
    "                    ROI_FO,\n",
    "                    roi_task_id,\n",
    "                    _roi_task_id,\n",
    "                    im_str,\n",
    "                    im_f,\n",
    "                )\n",
    "                palette_params = {\n",
    "                    \"disp-zero\": False,\n",
    "                    \"disp-neg\": True,\n",
    "                    \"disp-pos\": True,\n",
    "                    \"pos-user\": (0, 1.),\n",
    "                    \"neg-user\": (-1,0),\n",
    "                    \"interpolate\": True,\n",
    "                }\n",
    "                # Save f1f2 map as dtseries\n",
    "                f_im_img = nib.load(f_im)\n",
    "                dscalar_to_save_as_cifti = np.zeros((1,f_im_img.shape[-1]))\n",
    "                dscalar_to_save_as_cifti[0,:VERTEX_TO] = data\n",
    "                f_im_img = nib.Cifti2Image(dscalar_to_save_as_cifti, header=f_im_img.header)\n",
    "                f_im_img.header.matrix[0].number_of_series_points = 1\n",
    "                nib.save(f_im_img, dscalar_out)\n",
    "                dscalar(\n",
    "                    png_out, data, \n",
    "                    orientation=\"portrait\", \n",
    "                    hemisphere='right',\n",
    "                    palette=PALETTE, \n",
    "                    palette_params=palette_params,\n",
    "                    transparent=False,\n",
    "                    flatmap=True,\n",
    "                    flatmap_style='plain',\n",
    "                )\n",
    "                #crop_and_save(png_out, str(png_out).replace(\"png\", \"cropped.png\"), LEFT, TOP, RIGHT, BOTTOM)\n",
    "                # Save f1f2 map (contextualized) as dtseries\n",
    "                png_out = str(png_out).replace(\".png\", \"_contextualized.png\")\n",
    "                dscalar(\n",
    "                    png_out, data_contextualized_1, \n",
    "                    orientation=\"portrait\", \n",
    "                    hemisphere='right',\n",
    "                    palette=PALETTE, \n",
    "                    palette_params=palette_params,\n",
    "                    transparent=False,\n",
    "                    flatmap=True,\n",
    "                    flatmap_style='plain',\n",
    "                )\n",
    "                png_out = str(png_out).replace(\"_contextualized.png\", \"_contextualized_include_missing.png\")\n",
    "                dscalar(\n",
    "                    png_out, data_contextualized_2,\n",
    "                    orientation=\"portrait\", \n",
    "                    hemisphere='right',\n",
    "                    palette=PALETTE, \n",
    "                    palette_params=palette_params,\n",
    "                    transparent=False,\n",
    "                    flatmap=True,\n",
    "                    flatmap_style='plain',\n",
    "                )\n",
    "                if power_im_data is not None:\n",
    "                    if len([i for i in power_im_data[power_im_data>0]])==0:\n",
    "                        continue\n",
    "                    png_out = str(png_out).replace(\"_contextualized_include_missing.png\", \"_bold_power.png\")\n",
    "                    percentile = .8\n",
    "                    ub = calculate_percentile([i for i in power_im_data[power_im_data>0]], percentile)\n",
    "                    print(f\"Percentile [{percentile}]: {ub}\")\n",
    "                    palette_params = {\n",
    "                        \"disp-zero\": False,\n",
    "                        \"disp-neg\": False,\n",
    "                        \"disp-pos\": True,\n",
    "                        \"pos-user\": (0, ub),\n",
    "                        \"neg-user\": (-1,0),\n",
    "                        \"interpolate\": True,\n",
    "                    }\n",
    "                    dscalar(\n",
    "                        png_out, power_im_data,\n",
    "                        orientation=\"portrait\", \n",
    "                        hemisphere='right',\n",
    "                        palette=\"videen_style\", \n",
    "                        palette_params=palette_params,\n",
    "                        transparent=False,\n",
    "                        flatmap=True,\n",
    "                        flatmap_style='plain',\n",
    "                    )\n",
    "                    png_out = str(png_out).replace(\"_bold_power.png\", \"_phase_delay.png\")\n",
    "                    ub = 1/im_f\n",
    "                    palette_params = {\n",
    "                        \"disp-zero\": False,\n",
    "                        \"disp-neg\": False,\n",
    "                        \"disp-pos\": True,\n",
    "                        \"pos-user\": (0, ub),\n",
    "                        \"neg-user\": (-1,0),\n",
    "                        \"interpolate\": True,\n",
    "                    }\n",
    "                    pd_im_data[power_im_data <= 0] = 0\n",
    "                    dscalar(\n",
    "                        png_out, pd_im_data,\n",
    "                        orientation=\"portrait\", \n",
    "                        hemisphere='right',\n",
    "                        palette=\"videen_style\", \n",
    "                        palette_params=palette_params,\n",
    "                        transparent=False,\n",
    "                        flatmap=True,\n",
    "                        flatmap_style='plain',\n",
    "                    )\n",
    "                #crop_and_save(png_out, str(png_out).replace(\"png\", \"cropped.png\"), LEFT, TOP, RIGHT, BOTTOM)\n",
    "                \n",
    "                track = [len(v) for k,v in df_data.items()]\n",
    "                print(track)\n",
    "\n",
    "    return df_data, proportion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "scratch_dir = Path(\"/scratch/fastfmri\")\n",
    "immapdir = scratch_dir / \"im_map\"\n",
    "if not immapdir.exists():\n",
    "    immapdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save visualizations\n",
    "- 3T normal (.125/.2)\n",
    "- 7T normal (.125/.2)\n",
    "- 3T varying frequencies\n",
    "- 7T varying frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set up for visualizing dual frequency tagging across each subject using fractional overlap\n",
    "\"\"\"\n",
    "PALETTE = \"power_surf\"\n",
    "f_1_c = -.1 # red\n",
    "f_2_c = .82 # blue\n",
    "f1f2_c = .14 # gold\n",
    "f_im_c = .52 # .52 cyan\n",
    "f_1_c_not_im = .6 # orange\n",
    "f_2_c_not_im = .65 # purple\n",
    "f1f2_c_not_im = .75 # limegreen\n",
    "mask_c = .41 # black\n",
    "\n",
    "ROI_FOS = [.8]\n",
    "corr_type = \"uncp\"\n",
    "FORCE_RUN = False\n",
    "\n",
    "\"\"\"Save png\n",
    "\"\"\"\n",
    "immap_pkl = immapdir / f\"im_map_corr-{corr_type}.pkl\"\n",
    "immap_proportion_pkl = immapdir / f\"im_map_proportion_corr-{corr_type}.pkl\"\n",
    "if immap_pkl.exists() and immap_proportion_pkl.exists() and not FORCE_RUN:\n",
    "    df = pd.read_pickle(immap_pkl)\n",
    "    proportion_df = pd.read_pickle(immap_proportion_pkl)\n",
    "else:\n",
    "    # 3T control under entrain condition (set this to get power measurements with entrain ROIs)\n",
    "    label = \"3TNormal\"\n",
    "    df_data = None\n",
    "    proportion_data = None\n",
    "    for _roi_task_id in [\"entrain\"]:\n",
    "        for ROI_FO in ROI_FOS:\n",
    "            experiment_id = \"1_frequency_tagging\" \n",
    "            mri_id = \"3T\"\n",
    "            sub_ids = [\"000\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\"] \n",
    "            roi_task_ids = [_roi_task_id] * len(sub_ids)\n",
    "            roi_f_1s = [.125] * len(sub_ids)\n",
    "            roi_f_2s = [.2] * len(sub_ids)\n",
    "            for sub_id, roi_task_id, roi_f_1, roi_f_2 in zip(sub_ids, roi_task_ids, roi_f_1s, roi_f_2s):\n",
    "                im_frequencies = get_im_frequencies(roi_f_1, roi_f_2)\n",
    "                df_data, proportion_data = generate_single_subject_maps(\n",
    "                    label, experiment_id, mri_id, [sub_id], \n",
    "                    [roi_task_id], im_frequencies, HCP_LABELS,\n",
    "                    df_data=df_data,\n",
    "                    proportion_data=proportion_data,\n",
    "                    corr_type=corr_type,\n",
    "                    ROI_FO=ROI_FO, SUB_THRESHOLD=.5,\n",
    "                    FORCE_TASK_ID=\"control\"\n",
    "                )\n",
    "                import pdb; pdb.set_trace()\n",
    "    # 3T normal\n",
    "    label = \"3TNormal\"\n",
    "    for _roi_task_id in [\"entrain\"]:\n",
    "        for ROI_FO in ROI_FOS:\n",
    "            experiment_id = \"1_frequency_tagging\" \n",
    "            mri_id = \"3T\"\n",
    "            sub_ids = [\"000\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\"] \n",
    "            roi_task_ids = [_roi_task_id] * len(sub_ids)\n",
    "            roi_f_1s = [.125] * len(sub_ids)\n",
    "            roi_f_2s = [.2] * len(sub_ids)\n",
    "            for sub_id, roi_task_id, roi_f_1, roi_f_2 in zip(sub_ids, roi_task_ids, roi_f_1s, roi_f_2s):\n",
    "                im_frequencies = get_im_frequencies(roi_f_1, roi_f_2)\n",
    "                df_data, proportion_data = generate_single_subject_maps(\n",
    "                    label, experiment_id, mri_id, [sub_id],\n",
    "                    [roi_task_id], im_frequencies, HCP_LABELS,\n",
    "                    df_data=df_data,\n",
    "                    proportion_data=proportion_data,\n",
    "                    corr_type=corr_type,\n",
    "                    ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "                )\n",
    "    # 7T normal\n",
    "    label = \"7TNormal\"\n",
    "    for ROI_FO in ROI_FOS:\n",
    "        experiment_id = \"1_attention\" \n",
    "        mri_id = \"7T\"\n",
    "        sub_ids = [\"Pilot001\", \"Pilot009\", \"Pilot010\", \"Pilot011\"]\n",
    "        roi_task_ids = [\"AttendAway\"] * len(sub_ids)\n",
    "        roi_f_1s = [.125] * len(sub_ids)\n",
    "        roi_f_2s = [.2] * len(sub_ids)\n",
    "        for sub_id, roi_task_id, roi_f_1, roi_f_2 in zip(sub_ids, roi_task_ids, roi_f_1s, roi_f_2s):\n",
    "            im_frequencies = get_im_frequencies(roi_f_1, roi_f_2)\n",
    "            df_data, proportion_data = generate_single_subject_maps(\n",
    "                label, experiment_id, mri_id, [sub_id],\n",
    "                [roi_task_id], im_frequencies, HCP_LABELS,\n",
    "                df_data=df_data,\n",
    "                proportion_data=proportion_data,\n",
    "                corr_type=corr_type,\n",
    "                ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "            )\n",
    "    # 3T vary\n",
    "    label = \"3TVary\"\n",
    "    for ROI_FO in ROI_FOS:\n",
    "        experiment_id = \"1_frequency_tagging\"\n",
    "        mri_id = \"3T\"\n",
    "        sub_ids = [\"020\"] * 3 + [\"021\"] * 3\n",
    "        roi_task_ids = [f\"entrain{i}\" for i in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]]\n",
    "        roi_f_1s = [.125] * 3 + [.125, .15, .175]\n",
    "        roi_f_2s = [.2, .175, .15] + [.2] * 3\n",
    "        for sub_id, roi_task_id, roi_f_1, roi_f_2 in zip(sub_ids, roi_task_ids, roi_f_1s, roi_f_2s):\n",
    "            im_frequencies = get_im_frequencies(roi_f_1, roi_f_2)\n",
    "            df_data, proportion_data = generate_single_subject_maps(\n",
    "                label, experiment_id, mri_id, [sub_id],\n",
    "                [roi_task_id], im_frequencies, HCP_LABELS,\n",
    "                df_data=df_data,\n",
    "                proportion_data=proportion_data,\n",
    "                corr_type=corr_type,\n",
    "                ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "            )\n",
    "    # 7T vary\n",
    "    label = \"7TVary\"\n",
    "    for ROI_FO in ROI_FOS:\n",
    "        experiment_id = \"1_frequency_tagging\"\n",
    "        mri_id = \"7T\"\n",
    "        sub_ids = [\"020\"] * 3 + [\"021\"] * 3\n",
    "        roi_task_ids = [f\"entrain{i}\" for i in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]]\n",
    "        roi_f_1s = [.125] * 3 + [.125, .15, .175]\n",
    "        roi_f_2s = [.2, .175, .15] + [.2] * 3\n",
    "        for sub_id, roi_task_id, roi_f_1, roi_f_2 in zip(sub_ids, roi_task_ids, roi_f_1s, roi_f_2s):\n",
    "            im_frequencies = get_im_frequencies(roi_f_1, roi_f_2)\n",
    "            df_data, proportion_data = generate_single_subject_maps(\n",
    "                label, experiment_id, mri_id, [sub_id],\n",
    "                [roi_task_id], im_frequencies, HCP_LABELS, \n",
    "                df_data=df_data,\n",
    "                proportion_data=proportion_data,\n",
    "                corr_type=corr_type,\n",
    "                ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "    df.to_pickle(immap_pkl)\n",
    "    proportion_df = pd.DataFrame(proportion_data)\n",
    "    proportion_df.to_pickle(immap_proportion_pkl)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_codes = [i for i in df.im_code.unique()]\n",
    "FONTSIZE = 6\n",
    "LINEWIDTH = 1.\n",
    "roi_c_dict = get_roi_colour_codes()\n",
    "text_dict = get_frequency_text_codes()\n",
    "\n",
    "fig, ax_dict = plt.subplot_mosaic(\n",
    "    [[i] for i in im_codes],\n",
    "    layout=\"constrained\",\n",
    "    figsize=(6,10),\n",
    "    dpi=600,\n",
    ")\n",
    "\n",
    "for im_code, ax in ax_dict.items():\n",
    "    _df = proportion_df[(proportion_df.im_code==im_code)]\n",
    "    _df.drop_duplicates(subset=[\"im_code\",\"hcp_label\",\"sub_id\",\"roi_task_id\",\"experiment\"], inplace=True)\n",
    "\n",
    "    _df[\"f1_count\"] = _df[\"activated_vertex_count\"] * _df[\"f1\"]\n",
    "    _df[\"f2_count\"] = _df[\"activated_vertex_count\"] * _df[\"f2\"]\n",
    "    _df[\"f1&f2_count\"] = _df[\"activated_vertex_count\"] * _df[\"f1&f2\"]\n",
    "    _df[\"fim_count\"] = _df[\"activated_vertex_count\"] * _df[\"fim\"]\n",
    "    _df = _df.groupby([\"sub_id\",\"experiment\",\"roi_task_id\"])[[\"f1_count\",\"f2_count\",\"f1&f2_count\",\"fim_count\"]].sum().reset_index()\n",
    "    _df[\"all_count\"] = _df[\"f1_count\"] + _df[\"f2_count\"] + _df[\"f1&f2_count\"] + _df[\"fim_count\"]\n",
    "    for col_id in [\"f1_count\",\"f2_count\",\"f1&f2_count\",\"fim_count\"]:\n",
    "        _df[col_id] = _df[col_id] / _df[\"all_count\"]\n",
    "    \n",
    "    sub_ids_info = _df.sub_id.values\n",
    "    roi_task_info = _df.roi_task_id.values\n",
    "    experiment_info = _df.experiment.values\n",
    "\n",
    "    for i, (row_ix, row) in enumerate(_df[[\"f1_count\",\"f2_count\",\"f1&f2_count\",\"fim_count\"]].iterrows()):\n",
    "        p_f1 = row[\"f1_count\"]\n",
    "        p_f2 = row[\"f2_count\"]\n",
    "        p_fim = row[\"fim_count\"]\n",
    "        p_f1f2 = row[\"f1&f2_count\"]\n",
    "\n",
    "        end_i = i+1\n",
    "\n",
    "        p_cumu_1 = 0\n",
    "        p_cumu_2 = 0\n",
    "        for ix, (_p, c) in enumerate(zip([p_f1, p_f2, p_f1f2, p_fim],[roi_c_dict[\"f1\"],roi_c_dict[\"f2\"],roi_c_dict[\"f1f2\"],\"cyan\"])):\n",
    "            p_cumu_2 += _p\n",
    "            ax.fill_between([i, end_i], [p_cumu_1]*2, [p_cumu_2]*2, color=c, alpha=1.)\n",
    "            p_cumu_1 += _p\n",
    "            \n",
    "        for ix in range(_df.shape[0]):\n",
    "            if ix == 0:\n",
    "                ax.plot([0,0],[0,1],c='k',linestyle='-',linewidth=LINEWIDTH,zorder=1)\n",
    "            ax.plot([ix+1,ix+1],[0,1],c='k',linestyle='-',linewidth=LINEWIDTH,zorder=1)\n",
    "        ax.plot([0,_df.shape[0]],[0,0],c='k',linestyle='-',linewidth=LINEWIDTH,zorder=1)\n",
    "        ax.plot([0,_df.shape[0]],[1,1],c='k',linestyle='-',linewidth=LINEWIDTH,zorder=1)\n",
    "        for _spine in [\"right\",\"bottom\",\"top\",\"left\"]:\n",
    "            ax.spines[_spine].set_visible(False)\n",
    "\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.set_xlim(0,_df.shape[0])\n",
    "\n",
    "        ax.set_xticks([i+.5 for i in range(_df.shape[0])])\n",
    "        ax.set_xticklabels([f\"{sub_id}/{roi_task_id}\\n{experiment_id}\" for sub_id, roi_task_id, experiment_id in zip(sub_ids_info, roi_task_info, experiment_info)], fontsize=FONTSIZE-2, rotation=90)\n",
    "        ax.set_yticks([0,1])\n",
    "        ax.set_yticklabels([0,1], fontsize=FONTSIZE-2)\n",
    "        ax.set_title(text_dict[im_code], fontsize=FONTSIZE)\n",
    "\n",
    "        ax.tick_params(\"both\",pad=0,width=LINEWIDTH,length=2)\n",
    "\n",
    "fig.savefig(\"im_mapping_proportion.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate control ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "immap_control_pkl = immapdir / f\"im_map_control_corr-{corr_type}.pkl\"\n",
    "immap_control_proportion_pkl = immapdir / f\"im_map_control_proportion_corr-{corr_type}.pkl\"\n",
    "if immap_control_pkl.exists() and immap_control_proportion_pkl.exists():\n",
    "    control_df = pd.read_pickle(immap_control_pkl)\n",
    "    control_proportion_df = pd.read_pickle(immap_control_proportion_pkl)\n",
    "else:\n",
    "    label = \"3TControl\"\n",
    "    control_df_data = None\n",
    "    control_proportion_data = None\n",
    "    for _roi_task_id in [\"control\"]:\n",
    "        for ROI_FO in ROI_FOS:\n",
    "            experiment_id = \"1_frequency_tagging\" \n",
    "            mri_id = \"3T\"\n",
    "            sub_ids = [\"000\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\"] \n",
    "            roi_task_ids = [_roi_task_id] * len(sub_ids)\n",
    "            roi_f_1s = [.125] * len(sub_ids)\n",
    "            roi_f_2s = [.2] * len(sub_ids)\n",
    "            for sub_id, roi_task_id, roi_f_1, roi_f_2 in zip(sub_ids, roi_task_ids, roi_f_1s, roi_f_2s):\n",
    "                im_frequencies = get_im_frequencies(roi_f_1, roi_f_2)\n",
    "                control_df_data, control_proportion_data = generate_single_subject_maps(\n",
    "                    label, experiment_id, mri_id, [sub_id],\n",
    "                    [roi_task_id], im_frequencies, HCP_LABELS,\n",
    "                    df_data=control_df_data,\n",
    "                    proportion_data=control_proportion_data,\n",
    "                    corr_type=corr_type,\n",
    "                    ROI_FO=ROI_FO, SUB_THRESHOLD=.5\n",
    "                )\n",
    "            \n",
    "    control_df = pd.DataFrame(control_df_data)\n",
    "    control_df.to_pickle(immap_control_pkl)\n",
    "    control_proportion_df = pd.DataFrame(control_proportion_data)\n",
    "    control_proportion_df.to_pickle(immap_control_proportion_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "geodesic_arr = np.array(nib.load(geodesic_dscalar).get_fdata())\n",
    "@lru_cache(maxsize=180)\n",
    "def _get_geodesic_distance(hcp_label):\n",
    "    hcp_label = hcp_label.split(\"_\")[1]\n",
    "    l_hcp_path = f\"/tmp/L_{hcp_label}_ROI.dscalar.nii\"\n",
    "    r_hcp_path = f\"/tmp/R_{hcp_label}_ROI.dscalar.nii\"\n",
    "    geo_l_data = geodesic_arr[nib.load(l_hcp_path).get_fdata()==1].mean()\n",
    "    geo_r_data = geodesic_arr[nib.load(r_hcp_path).get_fdata()==1].mean()\n",
    "\n",
    "    return np.mean([geo_l_data, geo_r_data])\n",
    "\n",
    "def get_laterality(df,sub_id, experiment_id, hcp_label):\n",
    "\n",
    "    quadrant_id = df[(df.sub_id==sub_id) & (df.experiment_id==experiment_id)].quadrant_id.unique()[0]\n",
    "    if quadrant_id == \"Q1\" and hcp_label.startswith(\"L_\"):\n",
    "        return hcp_label.replace(\"L_\",\"CONTRA_\",1)\n",
    "    elif quadrant_id == \"Q1\" and hcp_label.startswith(\"R_\"):\n",
    "        return hcp_label.replace(\"R_\",\"IPSI_\",1)\n",
    "    elif quadrant_id == \"Q2\" and hcp_label.startswith(\"L_\"):\n",
    "        return hcp_label.replace(\"L_\",\"IPSI_\",1)\n",
    "    elif quadrant_id == \"Q2\" and hcp_label.startswith(\"R_\"):\n",
    "        return hcp_label.replace(\"R_\",\"CONTRA_\",1)\n",
    "    else:\n",
    "        raise ValueError(f\"{quadrant_id} / {hcp_label} not supported.\")\n",
    "\n",
    "def plot_pie(ax,values,x,y,s,colors): \n",
    "    ax.pie(values, center=(x,y), radius=s, colors=colors)\n",
    "\n",
    "def get_pie_info_per_hcp_and_im_label(sub_proportion_df,im_code, hcp_label_normalized):\n",
    "    X = sub_proportion_df[(sub_proportion_df.im_code==im_code) & (sub_proportion_df.hcp_label_normalized==hcp_label_normalized)]\n",
    "    nrows = X.shape[0]\n",
    "    if nrows in [1,2]:\n",
    "        if nrows == 2:\n",
    "            X = X.iloc[[0]]\n",
    "        f1_proportion = X[\"f1\"].values[0]\n",
    "        f2_proportion = X[\"f2\"].values[0]\n",
    "        f1_and_f2_proportion = X[\"f1&f2\"].values[0]\n",
    "        fim_proportion = X[\"fim\"].values[0]\n",
    "        activated_vertex_count = X.activated_vertex_count.values[0]\n",
    "        hcp_vertex_count = X.hcp_vertex_count.values[0]\n",
    "        slab_vertex_count = X.slab_vertex_count.values[0]\n",
    "        assert hcp_vertex_count >= slab_vertex_count\n",
    "        not_slab_vertex_count = hcp_vertex_count - slab_vertex_count\n",
    "        not_activated_vertex_count = slab_vertex_count - activated_vertex_count\n",
    "        slab_vertex_count = slab_vertex_count - activated_vertex_count\n",
    "        assert activated_vertex_count + slab_vertex_count + not_slab_vertex_count == hcp_vertex_count, f\"{activated_vertex_count} + {slab_vertex_count} + {not_slab_vertex_count} != {hcp_vertex_count}\"\n",
    "        f1_vertex_count = f1_proportion * activated_vertex_count\n",
    "        f2_vertex_count = f2_proportion * activated_vertex_count\n",
    "        f1_and_f2_vertex_count = f1_and_f2_proportion * activated_vertex_count\n",
    "        fim_vertex_count = fim_proportion * activated_vertex_count\n",
    "        assert activated_vertex_count == f1_vertex_count + f2_vertex_count + f1_and_f2_vertex_count + fim_vertex_count\n",
    "        \n",
    "        return [f1_vertex_count, f2_vertex_count, f1_and_f2_vertex_count, fim_vertex_count, slab_vertex_count, not_slab_vertex_count], hcp_vertex_count, X.geodesic_distance.values[0]\n",
    "\n",
    "    elif nrows == 0:\n",
    "        return 0\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"{hcp_label} {im_code} has {nrows} rows\")\n",
    "\n",
    "def plot_bar(ax, hcp_ix, x_offset, bar_width, values, colors,fractionate=False):\n",
    "\n",
    "    assert len(values) == len(colors)\n",
    "\n",
    "    sum_all = np.sum(values)\n",
    "\n",
    "    if fractionate:\n",
    "        values = [i/sum_all for i in values]\n",
    "    \n",
    "    bottom_value = 0\n",
    "    for v, c in zip(values, colors):\n",
    "        top_value = v + bottom_value\n",
    "        x_start = hcp_ix + x_offset\n",
    "        x_end = x_start + bar_width\n",
    "        ax.fill_between([x_start,x_end], [bottom_value]*2, [top_value]*2, color=c, alpha=.8,linewidth=0)\n",
    "        bottom_value += v\n",
    "\n",
    "def plot_proportions(ax, proportion_df,experiment, sub_id, hemi_starts_with, im_codes_to_include, pie_colors, FONTSIZE, unique_hcp_rois=None, ymax=None):\n",
    "\n",
    "    sub_proportion_df = proportion_df[(proportion_df.sub_id == sub_id) & (proportion_df.experiment == experiment) & (proportion_df.hcp_label_normalized.str.startswith(hemi_starts_with)) & (proportion_df.im_code.isin(im_codes_to_include))]\n",
    "\n",
    "    if unique_hcp_rois is None:\n",
    "        unique_hcp_rois = proportion_df[(proportion_df.hcp_label_normalized.str.startswith(hemi_starts_with))].hcp_label_normalized.unique()\n",
    "    if ymax is None:\n",
    "        ymax = sub_proportion_df.activated_vertex_count.max() * .8\n",
    "    for hcp_ix, hcp_label_normalized in enumerate(unique_hcp_rois):\n",
    "        x_offset = 0\n",
    "        x_width = .2\n",
    "        ax.plot([hcp_ix, hcp_ix+(x_width*4)],[0,0], color=\"k\", linewidth=.4, alpha=1.,zorder=10)\n",
    "        ax.text(hcp_ix+(x_width*2), ymax*1.05, hcp_label_normalized.split(\"_\")[1], ha=\"center\", fontsize=FONTSIZE-3,rotation=90,fontweight=\"bold\")\n",
    "        for ypos, line_c, im_code in zip([0,1,2,3],[\"k\",\"k\",\"r\",\"k\"], im_codes_to_include):\n",
    "            pie_info = get_pie_info_per_hcp_and_im_label(sub_proportion_df,im_code, hcp_label_normalized)\n",
    "            if pie_info == 0:\n",
    "                x_offset += x_width\n",
    "                continue\n",
    "            \n",
    "            ymax_plot = np.sum(pie_info[0][:4])\n",
    "            ax.plot([hcp_ix+((x_width*ypos)+x_width/2)]*2,[0,ymax_plot], color=line_c, linewidth=.4, alpha=1.,zorder=10, linestyle=\"dotted\")\n",
    "            if im_code == \"f1\":\n",
    "                ax.scatter(hcp_ix+((x_width*ypos)+x_width/2), ymax_plot + (5*ymax/100), marker='o', s=5, c='r', linewidths=0, zorder=20)\n",
    "            if im_code == \"f2\":\n",
    "                ax.scatter(hcp_ix+((x_width*ypos)+x_width/2), ymax_plot + (5*ymax/100), marker='o', s=5, c='b', linewidths=0, zorder=20)\n",
    "            if im_code == \"f2-f1\":\n",
    "                ax.scatter(hcp_ix+((x_width*ypos)+x_width/2), ymax_plot + (5*ymax/100), marker='v', s=15, c='g', linewidths=0, zorder=20)\n",
    "            pie_values, hcp_vertex_count, gd = pie_info\n",
    "            assert len(pie_values) == len(pie_colors)\n",
    "\n",
    "            plot_bar(ax, hcp_ix, x_offset, x_width, pie_values, pie_colors)\n",
    "\n",
    "            x_offset += x_width\n",
    "\n",
    "    # style plot\n",
    "    for _spine in [\"top\",\"right\",\"bottom\"]:\n",
    "        ax.spines[_spine].set_visible(False)\n",
    "    ax.spines[\"left\"].set_bounds(0,ymax)\n",
    "    ax.spines['left'].set_position(('data',-.4))\n",
    "    ax.spines[\"left\"].set_linewidth(.2)\n",
    "    ax.set_ylim(-ymax*.01,ymax*1.2)\n",
    "    ax.set_yticks([0,ymax])\n",
    "    ax.set_yticklabels([f\"{i:.1f}\" for i in [0,ymax]], fontsize=FONTSIZE)\n",
    "    ax.set_xticks([])\n",
    "    ax.tick_params(axis=\"y\",width=.2,pad=0,length=2,labelsize=FONTSIZE)\n",
    "    ax.set_title(sub_id, fontsize=FONTSIZE)\n",
    "    \n",
    "    return ymax\n",
    "\n",
    "proportion_df[\"hcp_label_normalized\"] = proportion_df.apply(lambda row: get_laterality(df, row[\"sub_id\"], row[\"experiment\"], row[\"hcp_label\"]), axis=1)\n",
    "proportion_df[\"geodesic_distance\"] = proportion_df[\"hcp_label\"].apply(_get_geodesic_distance)\n",
    "proportion_df = proportion_df.sort_values(by=\"geodesic_distance\",ascending=True)\n",
    "\n",
    "control_proportion_df[\"hcp_label_normalized\"] = control_proportion_df.apply(lambda row: get_laterality(control_df, row[\"sub_id\"], row[\"experiment\"], row[\"hcp_label\"]), axis=1)\n",
    "control_proportion_df[\"geodesic_distance\"] = control_proportion_df[\"hcp_label\"].apply(_get_geodesic_distance)\n",
    "control_proportion_df = control_proportion_df.sort_values(by=\"geodesic_distance\",ascending=True)\n",
    "\n",
    "FONTSIZE = 6\n",
    "im_codes_to_include = [\"f1\",\"f2\",\"f2-f1\",\"2f1\"]\n",
    "pie_colors = [\"r\",\"b\",\"yellow\",\"cyan\",\"grey\",\"k\"]\n",
    "\n",
    "experiment = \"3TNormal\"\n",
    "sub_ids = [\"000\",\"002\",\"003\",\"004\",\"005\",\"006\",\"007\",\"008\",\"009\"]\n",
    "for hemi_starts_with in [\"CONTRA_\",\"IPSI_\"]:\n",
    "    unique_hcp_rois = proportion_df[(proportion_df.hcp_label_normalized.str.startswith(hemi_starts_with)) & (proportion_df.im_code.isin(im_codes_to_include))].hcp_label_normalized.unique()\n",
    "    entrain_fig, entrain_ax_dict = plt.subplot_mosaic(mosaic=[[i] for i in sub_ids],layout=\"constrained\", figsize=(4.5,6.6), dpi=300)\n",
    "    control_fig, control_ax_dict = plt.subplot_mosaic(mosaic=[[i] for i in sub_ids],layout=\"constrained\", figsize=(4.5,6.6), dpi=300)\n",
    "    for sub_id in sub_ids:\n",
    "        ax = entrain_ax_dict[sub_id]\n",
    "        ymax = plot_proportions(ax, proportion_df, \"3TNormal\", sub_id, hemi_starts_with, im_codes_to_include, pie_colors, FONTSIZE,unique_hcp_rois=unique_hcp_rois)\n",
    "        ax = control_ax_dict[sub_id]\n",
    "        _ = plot_proportions(ax, control_proportion_df, \"3TControl\", sub_id, hemi_starts_with, im_codes_to_include, pie_colors, FONTSIZE,unique_hcp_rois=unique_hcp_rois,ymax=ymax)\n",
    "\n",
    "    entrain_fig.savefig(f\"{experiment}_{hemi_starts_with}entrain_composition.png\",dpi=600)\n",
    "    control_fig.savefig(f\"{experiment}_{hemi_starts_with}control_composition.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertex count 3T data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_task_id = \"entrain\"\n",
    "task_id = \"entrain\"\n",
    "control_task_id = \"control\"\n",
    "roi_fo = .8\n",
    "FONTSIZE = 6\n",
    "\n",
    "str_mapping = get_frequency_text_codes()\n",
    "\n",
    "def count_vertices_from_df(df):\n",
    "    vertex_count = 0\n",
    "    for i in df.vertex_coordinates:\n",
    "        vertex_count += i[0].shape[0]\n",
    "\n",
    "    return vertex_count\n",
    "\n",
    "def round_up_to_nearest_hundred(num, round_to=100):\n",
    "    import math\n",
    "    return math.ceil(num / round_to) * round_to\n",
    "\n",
    "im_codes = [i for i in df.im_code.unique()]\n",
    "sub_ids_3T = df[(df.experiment_id=='3TNormal')].sub_id.unique()\n",
    "\n",
    "fig, ax_dict = plt.subplot_mosaic([im_codes], layout=\"constrained\", figsize=(5, 1.), dpi=300)\n",
    "\n",
    "for im_code, ax in ax_dict.items():\n",
    "    _entrain_data = []\n",
    "    _control_data = []\n",
    "    for sub_id in sub_ids_3T:\n",
    "        _df = df[(df.sub_id==sub_id) & (df.roi_task_id==roi_task_id) & (df.task_id==task_id) & (df.roi_fo==roi_fo) & (df.im_code==im_code)]\n",
    "        _control_df = control_df[(control_df.sub_id==sub_id) & (control_df.roi_task_id==control_task_id) & (control_df.task_id==control_task_id) & (control_df.roi_fo==roi_fo) & (control_df.im_code==im_code)]\n",
    "        entrain_count = count_vertices_from_df(_df)\n",
    "        control_count = count_vertices_from_df(_control_df)\n",
    "        ax.scatter(1, entrain_count, color='cyan', s=20, edgecolors='k',linewidths=.4,zorder=12)\n",
    "        ax.scatter(0, control_count, color='grey', s=20, edgecolors='k',linewidths=.4,zorder=12)\n",
    "        ax.plot([0,1], [control_count, entrain_count], color='k', linestyle='dotted', linewidth=.4, zorder=10)\n",
    "        _entrain_data.append(entrain_count)\n",
    "        _control_data.append(control_count)\n",
    "    _data = _entrain_data + _control_data\n",
    "\n",
    "    wilcoxon_fail = False\n",
    "    try:\n",
    "        test = wilcoxon(_entrain_data, _control_data, alternative=\"greater\")\n",
    "        print(im_code, test)\n",
    "    except:\n",
    "        wilcoxon_fail = True\n",
    "\n",
    "    ax.set_title(str_mapping[im_code], fontsize=FONTSIZE)\n",
    "    ax.set_xlim(-.4,1.4)\n",
    "    ax.set_xticks([0,1])\n",
    "    ax.set_xticklabels([\"Control\",\"Entrain\"], fontsize=FONTSIZE,rotation=90)\n",
    "\n",
    "    ymax = round_up_to_nearest_hundred(max(_data))\n",
    "    ax.set_ylim(-ymax*.2,ymax*1.2)\n",
    "    ax.set_yticks([0,ymax/2,ymax])\n",
    "    ax.set_yticklabels([0,int(ymax/2),ymax],fontsize=FONTSIZE)\n",
    "    ax.tick_params(\"both\",pad=.2,width=.4)\n",
    "    if test.pvalue < .05 and not wilcoxon_fail:\n",
    "        ax.text(.5, max(_entrain_data)*1.15, \"$*$\", fontsize=FONTSIZE, ha=\"center\", va=\"center\")\n",
    "\n",
    "    for _spine in [\"right\",\"bottom\",\"top\"]:\n",
    "        ax.spines[_spine].set_visible(False)\n",
    "    ax.spines.left.set_linewidth(.4)\n",
    "    ax.spines.left.set_bounds(0,ymax)\n",
    "\n",
    "ax_dict[\"f1\"].set_ylabel(\"Vertex count\", fontsize=FONTSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertex count vary entrainDEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_dict = plt.subplot_mosaic([im_codes], layout=\"constrained\", figsize=(5, 1.), dpi=300)\n",
    "\n",
    "for mri_id in [\"3T\",\"7T\"]:\n",
    "    marker_style = 'o'\n",
    "    if mri_id==\"7T\":\n",
    "        marker_style=\"^\"\n",
    "    for im_code, ax in ax_dict.items():\n",
    "        entrain1_data = []\n",
    "        entrain2_data = []\n",
    "        entrain3_data = []\n",
    "        entrain1 = df[(df.experiment_id==f\"{mri_id}Vary\") & (df.roi_task_id==\"entrainD\") & (df.im_code==im_code)]\n",
    "        entrain2 = df[(df.experiment_id==f\"{mri_id}Vary\") & (df.roi_task_id==\"entrainE\") & (df.im_code==im_code)]\n",
    "        entrain3 = df[(df.experiment_id==f\"{mri_id}Vary\") & (df.roi_task_id==\"entrainF\") & (df.im_code==im_code)]\n",
    "        entrain1_data.append(count_vertices_from_df(entrain1))\n",
    "        entrain2_data.append(count_vertices_from_df(entrain2))\n",
    "        entrain3_data.append(count_vertices_from_df(entrain3))\n",
    "        all_entrain_data = entrain1_data+entrain2_data+entrain3_data\n",
    "        \n",
    "        ax.scatter(np.zeros_like(entrain1_data)+0,entrain1_data,c=\"cyan\",s=20,zorder=12,edgecolors='k',linewidths=.4,marker=marker_style)\n",
    "        ax.scatter(np.zeros_like(entrain2_data)+1,entrain2_data,c=\"cyan\",s=20,zorder=12,edgecolors='k',linewidths=.4,marker=marker_style)\n",
    "        ax.scatter(np.zeros_like(entrain3_data)+2,entrain3_data,c=\"cyan\",s=20,zorder=12,edgecolors='k',linewidths=.4,marker=marker_style)\n",
    "    \n",
    "        ax.set_title(str_mapping[im_code], fontsize=FONTSIZE)\n",
    "        _max = np.max(all_entrain_data)\n",
    "        if _max <= 100:\n",
    "            round_to = 10\n",
    "        else:\n",
    "            round_to = 100\n",
    "        ymax = round_up_to_nearest_hundred(np.max(all_entrain_data),round_to=round_to)\n",
    "        ax.set_ylim(-ymax*.2,ymax*1.2)\n",
    "        ax.set_yticks([0,ymax/2,ymax])\n",
    "        ax.set_yticklabels([0,int(ymax/2),ymax],fontsize=FONTSIZE)\n",
    "        ax.tick_params(\"both\",pad=.2,width=.4)\n",
    "        ax.set_xlim(-.4,2.4)\n",
    "        ax.set_xticks([0,1,2])\n",
    "        ax.set_xticklabels([\"1\",\"2\",\"3\"], fontsize=FONTSIZE,rotation=90)\n",
    "        ax.tick_params(\"both\",pad=.2,width=.4,labelsize=FONTSIZE)\n",
    "        for _spine in [\"right\",\"bottom\",\"top\"]:\n",
    "            ax.spines[_spine].set_visible(False)\n",
    "        ax.spines.left.set_linewidth(.4)\n",
    "        ax.spines.left.set_bounds(0,ymax)\n",
    "\n",
    "ax_dict[\"f1\"].set_ylabel(\"Voxel count\", fontsize=FONTSIZE)\n",
    "\n",
    "\n",
    "fig.savefig(\"K.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertex count vary entrainABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax_dict = plt.subplot_mosaic([im_codes], layout=\"constrained\", figsize=(5, 1.), dpi=300)\n",
    "\n",
    "for mri_id in [\"3T\",\"7T\"]:\n",
    "    marker_style = 'o'\n",
    "    if mri_id==\"7T\":\n",
    "        marker_style=\"^\"\n",
    "    for im_code, ax in ax_dict.items():\n",
    "        entrain1_data = []\n",
    "        entrain2_data = []\n",
    "        entrain3_data = []\n",
    "        entrain1 = df[(df.experiment_id==f\"{mri_id}Vary\") & (df.roi_task_id==\"entrainA\") & (df.im_code==im_code)]\n",
    "        entrain2 = df[(df.experiment_id==f\"{mri_id}Vary\") & (df.roi_task_id==\"entrainB\") & (df.im_code==im_code)]\n",
    "        entrain3 = df[(df.experiment_id==f\"{mri_id}Vary\") & (df.roi_task_id==\"entrainC\") & (df.im_code==im_code)]\n",
    "        entrain1_data.append(count_vertices_from_df(entrain1))\n",
    "        entrain2_data.append(count_vertices_from_df(entrain2))\n",
    "        entrain3_data.append(count_vertices_from_df(entrain3))\n",
    "        all_entrain_data = entrain1_data+entrain2_data+entrain3_data\n",
    "        \n",
    "        ax.scatter(np.zeros_like(entrain1_data)+0,entrain1_data,c=\"cyan\",s=20,zorder=12,edgecolors='k',linewidths=.4,marker=marker_style)\n",
    "        ax.scatter(np.zeros_like(entrain2_data)+1,entrain2_data,c=\"cyan\",s=20,zorder=12,edgecolors='k',linewidths=.4,marker=marker_style)\n",
    "        ax.scatter(np.zeros_like(entrain3_data)+2,entrain3_data,c=\"cyan\",s=20,zorder=12,edgecolors='k',linewidths=.4,marker=marker_style)\n",
    "    \n",
    "        ax.set_title(str_mapping[im_code], fontsize=FONTSIZE)\n",
    "        _max = np.max(all_entrain_data)\n",
    "        if _max <= 100:\n",
    "            round_to = 10\n",
    "        else:\n",
    "            round_to = 100\n",
    "        ymax = round_up_to_nearest_hundred(np.max(all_entrain_data),round_to=round_to)\n",
    "        ax.set_ylim(-ymax*.2,ymax*1.2)\n",
    "        ax.set_yticks([0,ymax/2,ymax])\n",
    "        ax.set_yticklabels([0,int(ymax/2),ymax],fontsize=FONTSIZE)\n",
    "        ax.tick_params(\"both\",pad=.2,width=.4)\n",
    "        ax.set_xlim(-.4,2.4)\n",
    "        ax.set_xticks([0,1,2])\n",
    "        ax.set_xticklabels([\"1\",\"2\",\"3\"], fontsize=FONTSIZE,rotation=90)\n",
    "        ax.tick_params(\"both\",pad=.2,width=.4,labelsize=FONTSIZE)\n",
    "        for _spine in [\"right\",\"bottom\",\"top\"]:\n",
    "            ax.spines[_spine].set_visible(False)\n",
    "        ax.spines.left.set_linewidth(.4)\n",
    "        ax.spines.left.set_bounds(0,ymax)\n",
    "\n",
    "ax_dict[\"f1\"].set_ylabel(\"Voxel count\", fontsize=FONTSIZE)\n",
    "\n",
    "\n",
    "fig.savefig(\"K.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESIGN MATRIX \n",
    "\n",
    "im_types = f1, f2, f2-f1, f1+f2, 2f1, 2f2, 2f1-f2, 2f2-f1\n",
    "- VERTEX_ID [row], sub_id, bold_power_[im_types], binary_[im_types], geodesic_distance_from_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.roi_fo.unique().shape[0] == 1\n",
    "\n",
    "roi_task_ids = []\n",
    "task_ids = []\n",
    "experiment_ids = []\n",
    "sub_ids = []\n",
    "quadrant_ids = []\n",
    "hcp_rois = []\n",
    "im_codes = []\n",
    "im_fs = []\n",
    "vertex_coordinates = []\n",
    "vertex_powers = []\n",
    "\n",
    "for row_ix, row in df.iterrows():\n",
    "    vertex_coords = row.vertex_coordinates[0]\n",
    "    power = row.f_im_BOLD_power\n",
    "    if power is None or len(power) == 0:\n",
    "        continue\n",
    "    assert vertex_coords.shape[0] == power.shape[0]\n",
    "\n",
    "    for _vertex, _power in zip(vertex_coords, power):\n",
    "        vertex_powers.append(_power)\n",
    "        vertex_coordinates.append(_vertex)\n",
    "        im_fs.append(row.im_f)\n",
    "        im_codes.append(row.im_code)\n",
    "        hcp_rois.append(row.hcp_roi)\n",
    "        quadrant_ids.append(row.quadrant_id)\n",
    "        sub_ids.append(row.sub_id)\n",
    "        experiment_ids.append(row.experiment_id)\n",
    "        task_ids.append(row.task_id)\n",
    "        roi_task_ids.append(row.roi_task_id)\n",
    "\n",
    "expanded_df = pd.DataFrame(\n",
    "    {\n",
    "        \"roi_task_id\": roi_task_ids,\n",
    "        \"task_id\": task_ids,\n",
    "        \"roi_fo\": df.roi_fo.values[0],\n",
    "        \"experiment_id\": experiment_ids,\n",
    "        \"sub_id\": sub_ids,\n",
    "        \"quadrant_id\": quadrant_ids,\n",
    "        \"hcp_roi\": hcp_rois,\n",
    "        \"im_code\": im_codes,\n",
    "        \"im_f\": im_fs,\n",
    "        \"power\": vertex_powers,\n",
    "        \"vertex_id\": vertex_coordinates\n",
    "    }\n",
    ")\n",
    "# Add hemi column\n",
    "expanded_df[\"hemi\"] = np.where(expanded_df.hcp_roi.str.startswith(\"CONTRA\"), 1,0)\n",
    "# Add geodesic distance column\n",
    "from functools import lru_cache\n",
    "@lru_cache(maxsize=180)\n",
    "def _get_geodesic_distance(hcp_label):\n",
    "    l_hcp_path = f\"/tmp/L_{hcp_label}_ROI.dscalar.nii\"\n",
    "    r_hcp_path = f\"/tmp/R_{hcp_label}_ROI.dscalar.nii\"\n",
    "    geo_l_data = geodesic_arr[nib.load(l_hcp_path).get_fdata()==1].mean()\n",
    "    geo_r_data = geodesic_arr[nib.load(r_hcp_path).get_fdata()==1].mean()\n",
    "\n",
    "    return np.mean([geo_l_data, geo_r_data])\n",
    "\n",
    "def get_geodesic_distance(row):\n",
    "    return _get_geodesic_distance(row.hcp_roi.split(\"_\")[-1])\n",
    "\n",
    "\"\"\"\n",
    "def get_geodesic_distance(row):\n",
    "    return geodesic_arr[0,row.vertex_id]\n",
    "\"\"\"\n",
    "\n",
    "geodesic_arr = np.array(nib.load(geodesic_dscalar).get_fdata())\n",
    "expanded_df[\"geodesic_distance\"] = expanded_df.apply(get_geodesic_distance, axis=1,)\n",
    "expanded_df[\"experiment_sub_id\"] = expanded_df.experiment_id + \"_\" + expanded_df.sub_id + \"_\" + expanded_df.task_id\n",
    "\n",
    "expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert control_df.roi_fo.unique().shape[0] == 1\n",
    "\n",
    "roi_task_ids = []\n",
    "task_ids = []\n",
    "experiment_ids = []\n",
    "sub_ids = []\n",
    "quadrant_ids = []\n",
    "hcp_rois = []\n",
    "im_codes = []\n",
    "im_fs = []\n",
    "vertex_coordinates = []\n",
    "\n",
    "for row_ix, row in control_df.iterrows():\n",
    "    vertex_coords = row.vertex_coordinates[0]\n",
    "\n",
    "    for _vertex in vertex_coords:\n",
    "        vertex_coordinates.append(_vertex)\n",
    "        im_fs.append(row.im_f)\n",
    "        im_codes.append(row.im_code)\n",
    "        hcp_rois.append(row.hcp_roi)\n",
    "        quadrant_ids.append(row.quadrant_id)\n",
    "        sub_ids.append(row.sub_id)\n",
    "        experiment_ids.append(row.experiment_id)\n",
    "        task_ids.append(row.task_id)\n",
    "        roi_task_ids.append(row.roi_task_id)\n",
    "\n",
    "control_expanded_df = pd.DataFrame(\n",
    "    {\n",
    "        \"roi_task_id\": roi_task_ids,\n",
    "        \"task_id\": task_ids,\n",
    "        \"roi_fo\": control_df.roi_fo.values[0],\n",
    "        \"experiment_id\": experiment_ids,\n",
    "        \"sub_id\": sub_ids,\n",
    "        \"quadrant_id\": quadrant_ids,\n",
    "        \"hcp_roi\": hcp_rois,\n",
    "        \"im_code\": im_codes,\n",
    "        \"im_f\": im_fs,\n",
    "        \"vertex_id\": vertex_coordinates\n",
    "    }\n",
    ")\n",
    "# Add hemi column\n",
    "control_expanded_df[\"hemi\"] = np.where(control_expanded_df.hcp_roi.str.startswith(\"CONTRA\"), 1,0)\n",
    "\n",
    "geodesic_arr = np.array(nib.load(geodesic_dscalar).get_fdata())\n",
    "control_expanded_df[\"geodesic_distance\"] = control_expanded_df.apply(get_geodesic_distance, axis=1,)\n",
    "control_expanded_df[\"experiment_sub_id\"] = control_expanded_df.experiment_id + \"_\" + control_expanded_df.sub_id + \"_\" + control_expanded_df.task_id\n",
    "\n",
    "control_expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import mixedlm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_formula(bold_power_type, bold_f_type_strs):\n",
    "    formula_y = f\"bold_power_{bold_power_type} ~ \"\n",
    "    all_variables = [f\"binary_{i}\" for i in bold_f_type_strs] + [\"hemi\",\"geodesic_distance\"]\n",
    "    all_variables = [i for i in all_variables if i != f\"binary_{bold_power_type}\"]\n",
    "    assert len(all_variables)-1 == len(bold_f_type_strs)\n",
    "    all_variables = \" + \".join(all_variables)\n",
    "    formula = f\"{formula_y}{all_variables}\"\n",
    "\n",
    "    return formula\n",
    "\n",
    "\n",
    "experiment_ids = [\"3TNormal\"]*18+[\"7TNormal\"]*4+[\"3TVary\"]*6+[\"7TVary\"]*6\n",
    "sub_ids = [\"000\",\"002\",\"003\",\"004\",\"005\",\"006\",\"007\",\"008\",\"009\"]*2+[\"Pilot001\",\"Pilot009\",\"Pilot010\",\"Pilot011\"]+[\"020\"]*3+[\"021\"]*3+[\"020\"]*3+[\"021\"]*3\n",
    "roi_task_ids = [\"entrain\"]*18+[\"AttendAway\"]*4+[f\"entrain{i}\" for i in [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]]*2\n",
    "task_ids = [\"control\"]*9+[\"entrain\"]*9+[\"AttendAway\"]*4+[f\"entrain{i}\" for i in [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]]*2\n",
    "bold_f_type_strs = [\"f1\",\"f2\",\"f2_sub_f1\",\"2f1\"]\n",
    "\n",
    "n_sub_ids = len(sub_ids)\n",
    "n_bold_f_types = len(bold_f_type_strs)\n",
    "\n",
    "fig, ax_dict = plt.subplot_mosaic([bold_f_type_strs], layout=\"constrained\", figsize=(8,2),dpi=400)\n",
    "predict_fig, predict_ax_dict = plt.subplot_mosaic([bold_f_type_strs], layout=\"constrained\", figsize=(8,1),dpi=400)\n",
    "geo_fig, geo_ax_dict = plt.subplot_mosaic([bold_f_type_strs], layout=\"constrained\", figsize=(8,1),dpi=400)\n",
    "hemi_fig, hemi_ax_dict = plt.subplot_mosaic([bold_f_type_strs], layout=\"constrained\", figsize=(8,1),dpi=400)\n",
    "spatial_f1_fig, spatial_f1_ax_dict = plt.subplot_mosaic([bold_f_type_strs], layout=\"constrained\", figsize=(8,1),dpi=400)\n",
    "spatial_f2_fig, spatial_f2_ax_dict = plt.subplot_mosaic([bold_f_type_strs], layout=\"constrained\", figsize=(8,1),dpi=400)\n",
    "spatial_f2_sub_f1_fig, spatial_f2_sub_f1_ax_dict = plt.subplot_mosaic([bold_f_type_strs], layout=\"constrained\", figsize=(8,1),dpi=400)\n",
    "spatial_2f1_fig, spatial_2f1_ax_dict = plt.subplot_mosaic([bold_f_type_strs], layout=\"constrained\", figsize=(8,1),dpi=400)\n",
    "\n",
    "mixed_glm_cohort_pvalues = np.zeros((len(bold_f_type_strs),n_bold_f_types+2))\n",
    "im_code_df_dict = {}\n",
    "for bold_power_type_ix, bold_power_type in enumerate(bold_f_type_strs):\n",
    "\n",
    "    ax = ax_dict[bold_power_type]\n",
    "    predict_ax = predict_ax_dict[bold_power_type]\n",
    "    hemi_ax = hemi_ax_dict[bold_power_type]\n",
    "    geo_ax = geo_ax_dict[bold_power_type]\n",
    "    spatial_f1_ax = spatial_f1_ax_dict[bold_power_type]\n",
    "    spatial_f2_ax = spatial_f2_ax_dict[bold_power_type]\n",
    "    spatial_f2_sub_f1_ax = spatial_f2_sub_f1_ax_dict[bold_power_type]\n",
    "    spatial_2f1_ax = spatial_2f1_ax_dict[bold_power_type]\n",
    "\n",
    "    group_filtered_df = None\n",
    "    cohort_pvalues = np.zeros((n_sub_ids,n_bold_f_types+2)) # +1 add hemi column, +1 add gd column\n",
    "    for sub_ix, (experiment_id,sub_id,roi_task_id,task_id) in enumerate(zip(experiment_ids,sub_ids,roi_task_ids,task_ids)):\n",
    "\n",
    "        # Reorganize\n",
    "        sub_expanded_df = expanded_df[(expanded_df.sub_id==sub_id) & (expanded_df.experiment_id==experiment_id) & (expanded_df.roi_task_id==expanded_df.roi_task_id) & (expanded_df.task_id==task_id)]\n",
    "        data_dict = defaultdict(list)\n",
    "        for vertex_id in sub_expanded_df.vertex_id.unique():\n",
    "            _df = sub_expanded_df[sub_expanded_df.vertex_id==vertex_id]\n",
    "            bold_power_f1 = np.nan\n",
    "            bold_power_f2 = np.nan\n",
    "            bold_power_f2_sub_f1 = np.nan\n",
    "            bold_power_2f1 = np.nan\n",
    "            binary_f1 = 0\n",
    "            binary_f2 = 0\n",
    "            binary_f2_sub_f1 = 0\n",
    "            binary_2f1 = 0\n",
    "            for _, row in _df.iterrows():\n",
    "                if row.im_code == \"f1\":\n",
    "                    binary_f1 = 1\n",
    "                    bold_power_f1 = row.power\n",
    "                elif row.im_code == \"f2\":\n",
    "                    binary_f2 = 1\n",
    "                    bold_power_f2 = row.power\n",
    "                elif row.im_code == \"f2-f1\":\n",
    "                    binary_f2_sub_f1 = 1\n",
    "                    bold_power_f2_sub_f1 = row.power\n",
    "                elif row.im_code == \"2f1\":\n",
    "                    binary_2f1 = 1\n",
    "                    bold_power_2f1 = row.power\n",
    "                else:\n",
    "                    continue\n",
    "                #else:\n",
    "                    #raise ValueError(f\"{row.im_code} not supported.\")\n",
    "            data_dict[\"bold_power_f1\"].append(bold_power_f1)\n",
    "            data_dict[\"bold_power_f2\"].append(bold_power_f2)\n",
    "            data_dict[\"bold_power_f2_sub_f1\"].append(bold_power_f2_sub_f1)\n",
    "            data_dict[\"bold_power_2f1\"].append(bold_power_2f1)\n",
    "            data_dict[\"binary_f1\"].append(binary_f1)\n",
    "            data_dict[\"binary_f2\"].append(binary_f2)\n",
    "            data_dict[\"binary_f2_sub_f1\"].append(binary_f2_sub_f1)\n",
    "            data_dict[\"binary_2f1\"].append(binary_2f1)\n",
    "            data_dict[\"im_count\"].append(_df.shape[0])\n",
    "            data_dict[\"vertex_id\"].append(vertex_id)\n",
    "            data_dict[\"sub_id\"].append(sub_id)\n",
    "            data_dict[\"hcp_label\"].append(row.hcp_roi)\n",
    "            data_dict[\"hemi\"].append(row.hemi)\n",
    "            data_dict[\"geodesic_distance\"].append(row.geodesic_distance)\n",
    "            data_dict[\"experiment_sub_id\"].append(row.experiment_sub_id)\n",
    "        _df = pd.DataFrame(data_dict)\n",
    "\n",
    "        \"\"\"STATS\n",
    "        Model a frequencies' power with spatial information of other exist stimulating and IM frequencies\n",
    "        \"\"\"\n",
    "        keep_rows = [f\"bold_power_{bold_power_type}\", \"im_count\", \"hcp_label\",\"vertex_id\"] + [f\"binary_{i}\" for i in bold_f_type_strs] + [\"hemi\",\"geodesic_distance\",\"experiment_sub_id\"]\n",
    "        filtered_df = _df[(pd.notna(_df[f\"bold_power_{bold_power_type}\"]))][keep_rows]\n",
    "        y = filtered_df[f\"bold_power_{bold_power_type}\"]\n",
    "        X = filtered_df[[f\"binary_{i}\" for i in bold_f_type_strs]+[\"hemi\",\"geodesic_distance\"]]\n",
    "        if y.shape[0] == 0:\n",
    "            for var_ix in range(6):\n",
    "                cohort_pvalues[sub_ix,var_ix] = -1\n",
    "            continue\n",
    "\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y,X).fit()\n",
    "\n",
    "        for var_ix, (var,pval) in enumerate(model.pvalues.items()):\n",
    "            if np.isnan(pval):\n",
    "                cohort_pvalues[sub_ix,var_ix] = -1\n",
    "            else:\n",
    "                cohort_pvalues[sub_ix,var_ix] = pval\n",
    "        \n",
    "        # Store data for group level analysis (mixed lm model)\n",
    "        if group_filtered_df is None:\n",
    "            group_filtered_df = filtered_df\n",
    "        else:\n",
    "            group_filtered_df = pd.concat((group_filtered_df,filtered_df))\n",
    "\n",
    "    im_code_df_dict[bold_power_type] = group_filtered_df\n",
    "\n",
    "    \"\"\"STATS\n",
    "    with mixed linear models on group level data\n",
    "    \"\"\"\n",
    "    print(group_filtered_df.shape)\n",
    "    # Remove control condition\n",
    "    group_filtered_df = group_filtered_df[~group_filtered_df.experiment_sub_id.str.contains(\"control\")]\n",
    "    print(\"After removing control:\", group_filtered_df.shape)\n",
    "    # Log transform gd\n",
    "    #group_filtered_df[\"geodesic_distance\"] = np.log(group_filtered_df[\"geodesic_distance\"])\n",
    "    # Demean all vars\n",
    "    group_filtered_df[\"geodesic_distance\"] = group_filtered_df.geodesic_distance - group_filtered_df.geodesic_distance.mean()\n",
    "    for col in [[f\"binary_{i}\" for i in bold_f_type_strs] + [\"hemi\",\"geodesic_distance\"]]:\n",
    "        group_filtered_df[col] = group_filtered_df[col] - group_filtered_df[col].mean()\n",
    "    # Detect upperbound outlier information\n",
    "    q1 = group_filtered_df[f\"bold_power_{bold_power_type}\"].quantile(.005)\n",
    "    q3 = group_filtered_df[f\"bold_power_{bold_power_type}\"].quantile(.995)\n",
    "    iqr = q3-q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = (group_filtered_df[f\"bold_power_{bold_power_type}\"] > upper_bound)\n",
    "    print(f\"Identified {outliers.sum()} outliers\")\n",
    "    outliers_df = group_filtered_df[outliers]\n",
    "    group_filtered_df = group_filtered_df[~outliers]\n",
    "    bold_ymax = group_filtered_df[f\"bold_power_{bold_power_type}\"].max()\n",
    "\n",
    "    # Get formula\n",
    "    formula = get_formula(bold_power_type, bold_f_type_strs)\n",
    "    # Remove maps with no vertex assignment\n",
    "    remove_l = []\n",
    "    for i in group_filtered_df.columns:\n",
    "        if i.startswith(\"binary\"):\n",
    "            _sum = group_filtered_df[i].sum()\n",
    "            if _sum == 0:\n",
    "                remove_l.append(i)\n",
    "    for r in remove_l:\n",
    "        print(f\"Remove {r}\")\n",
    "        formula = formula.replace(f\"+ {r} \",\" \")\n",
    "    # Mixed LM\n",
    "    model = mixedlm(\n",
    "        formula, \n",
    "        group_filtered_df, \n",
    "        groups=group_filtered_df[\"experiment_sub_id\"], \n",
    "    )\n",
    "    result = model.fit()\n",
    "    # Predict BOLD power with model\n",
    "    model_cols = [i.strip() for i in formula.split(\"~\")[-1].split(\"+\")] + [\"experiment_sub_id\"]\n",
    "    _data = group_filtered_df[model_cols]\n",
    "    y_predict = result.predict(_data)\n",
    "    print(result.summary())\n",
    "\n",
    "\n",
    "    \"\"\"Plot single subject OLS fits\n",
    "    \"\"\"\n",
    "    masked_sig = cohort_pvalues < .05\n",
    "    masked_nan = cohort_pvalues == -1\n",
    "    cohort_pvalues[~masked_sig] = 0\n",
    "    cohort_pvalues[masked_sig] = 1\n",
    "    cohort_pvalues[masked_nan] = -1\n",
    "    im = ax.imshow(cohort_pvalues, interpolation='none',cmap='Reds')\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = plt.colorbar(im,cax=cax)\n",
    "    cbar.ax.set_yticks([-1,0,1])\n",
    "    cbar.ax.set_yticklabels([\"n/a\",\"n.s.\",\"*\"], fontsize=FONTSIZE)\n",
    "    for _spine in [\"top\",\"right\",\"bottom\",\"left\"]:\n",
    "        ax.spines[_spine].set_visible(False)\n",
    "    ax.set_yticks([i for i in range(len(sub_ids))])\n",
    "    ax.set_yticklabels(sub_ids, fontsize=FONTSIZE-2, rotation=0)\n",
    "    ax.set_xticks([i for i in range(len(bold_f_type_strs)+2)])\n",
    "    ax.set_xticklabels(bold_f_type_strs+[\"hemi\",\"gd\"], fontsize=FONTSIZE-2, rotation=90)\n",
    "    ax.set_title(bold_power_type,fontsize=FONTSIZE)\n",
    "    ax.tick_params(\"both\",pad=0,width=.25,length=2)\n",
    "\n",
    "    \"\"\"Plot model predictions\n",
    "    \"\"\"\n",
    "    y = group_filtered_df[f\"bold_power_{bold_power_type}\"]\n",
    "    pval = pearsonr(y_predict,y).pvalue\n",
    "    c='k'\n",
    "    if pval < .05:\n",
    "        c='r'\n",
    "    predict_ax.scatter(y_predict,y,s=10,c=c,alpha=.1,edgecolors=\"none\")\n",
    "    predict_ax.set_title(f\"{bold_power_type}\",fontsize=FONTSIZE)\n",
    "    predict_ax.set_xlabel(\"Predicted\",fontsize=FONTSIZE)\n",
    "    predict_ax.set_ylabel(\"Observed\",fontsize=FONTSIZE)\n",
    "    predict_ax.set_xticklabels(predict_ax.get_xticks(),fontsize=FONTSIZE)\n",
    "    predict_ax.set_yticklabels(predict_ax.get_yticks(),fontsize=FONTSIZE)\n",
    "    predict_ax.set_ylim(0,bold_ymax)\n",
    "    \n",
    "    \"\"\"Plot association to gd\n",
    "    \"\"\"\n",
    "    gd = group_filtered_df[\"geodesic_distance\"]\n",
    "    y = group_filtered_df[f\"bold_power_{bold_power_type}\"]\n",
    "    pval = result.pvalues.geodesic_distance\n",
    "    c='k'\n",
    "    if pval < .05:\n",
    "        c='r'\n",
    "    geo_ax.set_title(f\"{bold_power_type}\",fontsize=FONTSIZE)\n",
    "    geo_ax.scatter(gd,y,s=10,c=c,alpha=.02,edgecolors=\"none\")\n",
    "    geo_ax.set_xticklabels([i for i in geo_ax.get_xticks() if i >= 0],fontsize=FONTSIZE)\n",
    "    geo_ax.set_yticklabels([i for i in geo_ax.get_yticks() if i >= 0],fontsize=FONTSIZE)\n",
    "    geo_ax.set_xlabel(\"geodesic distance\",fontsize=FONTSIZE)\n",
    "    geo_ax.set_ylabel(\"power\",fontsize=FONTSIZE)\n",
    "    geo_ax.set_ylim(0,bold_ymax)\n",
    "\n",
    "    \"\"\"Plot association to hemi\n",
    "    \"\"\"\n",
    "    for i,(m, fig, ax, xlabel) in enumerate(zip(\n",
    "        [\"hemi\",\"binary_f1\",\"binary_f2\",\"binary_f2_sub_f1\",\"binary_2f1\"],\n",
    "        [hemi_fig,spatial_f1_fig,spatial_f2_fig,spatial_f2_sub_f1_fig,spatial_2f1_fig],\n",
    "        [hemi_ax,spatial_f1_ax,spatial_f2_ax,spatial_f2_sub_f1_ax,spatial_2f1_ax],\n",
    "        [\"hemi\",\"f1\",\"f2\",\"f2-f1\",\"2f1\"]\n",
    "    )):\n",
    "        try:\n",
    "            x = group_filtered_df[m]\n",
    "            x_offset = np.random.uniform(-.2,.2,x.shape[0])\n",
    "            y = group_filtered_df[f\"bold_power_{bold_power_type}\"]\n",
    "            \n",
    "            pval = result.pvalues[m]\n",
    "            c='grey'\n",
    "            if pval < .05:\n",
    "                c='r'\n",
    "            ax.set_title(f\"{bold_power_type}\",fontsize=FONTSIZE)\n",
    "            ax.scatter(x+x_offset,y,s=10,c=c,alpha=.1,edgecolors=\"none\")\n",
    "            ax.set_xticklabels(hemi_ax.get_xticks(),fontsize=FONTSIZE)\n",
    "            ax.set_yticklabels(hemi_ax.get_yticks(),fontsize=FONTSIZE)\n",
    "            ax.set_xlabel(xlabel,fontsize=FONTSIZE)\n",
    "            ax.set_ylabel(\"power\",fontsize=FONTSIZE)\n",
    "            ax.set_ylim(0,bold_ymax)\n",
    "        except:\n",
    "            # Turn plot off\n",
    "            fig.delaxes(ax)\n",
    "            \n",
    "    # fill `mixed_glm_cohort_pvalues`\n",
    "    for i,m in enumerate([\"binary_f1\",\"binary_f2\",\"binary_f2_sub_f1\",\"binary_2f1\",\"hemi\",\"geodesic_distance\"]):\n",
    "        try:\n",
    "            pval = result.pvalues[m]\n",
    "            mixed_glm_cohort_pvalues[bold_power_type_ix,i] = pval\n",
    "        except:\n",
    "            mixed_glm_cohort_pvalues[bold_power_type_ix,i] = -1 # Not included in model\n",
    "\n",
    "\n",
    "    # Plot BOLD power per hcp label\n",
    "    retain_rois_per_experiment = [.75,1.,1.,1]\n",
    "    experiment_labels = [\"3TNormal\",\"7TNormal\",\"3TVary\",\"7TVary\"]\n",
    "    fig, power_per_hcp_ax_dict = plt.subplot_mosaic([experiment_labels],figsize=(7,1.2),dpi=300, layout=\"constrained\")\n",
    "    for experiment_label, retain_rois_by_thr in zip(experiment_labels,retain_rois_per_experiment):\n",
    "        ax = power_per_hcp_ax_dict[experiment_label]\n",
    "        _df = group_filtered_df[(group_filtered_df.experiment_sub_id.str.contains(experiment_label))]\n",
    "        total_experiments = len(_df.experiment_sub_id.unique())\n",
    "        hcp_label_frequency_dict = {}\n",
    "        for hcp_label in _df.hcp_label.unique():\n",
    "            __df = _df[(_df.hcp_label == hcp_label)]\n",
    "            n_experiment = len(__df[\"experiment_sub_id\"].unique())\n",
    "            if n_experiment >= retain_rois_by_thr*total_experiments:\n",
    "                hcp_label_frequency_dict[hcp_label] = n_experiment\n",
    "\n",
    "        for ms,hemi in zip(['o','^'],[\"CONTRA\",\"IPSI\"]):\n",
    "            __df = _df[(_df[\"hcp_label\"].isin([k for k,v in hcp_label_frequency_dict.items()])) & (_df[\"hcp_label\"].str.startswith(hemi))]\n",
    "            X = __df.groupby(\"hcp_label\")[[f\"bold_power_{bold_power_type}\",\"geodesic_distance\"]].mean()\n",
    "            ax.scatter(X[\"geodesic_distance\"],X[f\"bold_power_{bold_power_type}\"],marker=ms)\n",
    "            for hcp_label, (i, row) in zip(X.index,X.iterrows()):\n",
    "                ax.text(\n",
    "                    row[\"geodesic_distance\"],\n",
    "                    row[f\"bold_power_{bold_power_type}\"],\n",
    "                    hcp_label.split(\"_\")[-1],\n",
    "                    rotation=0,fontsize=FONTSIZE, va=\"center\", ha=\"center\"\n",
    "                )\n",
    "\n",
    "        ax.set_xticklabels(ax.get_xticks(),fontsize=FONTSIZE)\n",
    "        # scientific notation\n",
    "        ax.set_yticklabels([f\"{i:.1e}\" for i in ax.get_yticks()],fontsize=FONTSIZE)\n",
    "        ax.set_title(experiment_label,fontsize=FONTSIZE)\n",
    "        ax.set_ylabel(\"%BOLD Power\",fontsize=FONTSIZE)\n",
    "        ax.set_xlabel(\"log(geodesic distance)\",fontsize=FONTSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(2,2),dpi=300,layout=\"tight\")\n",
    "masked_sig = mixed_glm_cohort_pvalues < .05\n",
    "masked_nan = mixed_glm_cohort_pvalues == -1\n",
    "pvalue_matrix = np.zeros_like(mixed_glm_cohort_pvalues)\n",
    "pvalue_matrix[~masked_sig] = 0\n",
    "pvalue_matrix[masked_sig] = 1\n",
    "pvalue_matrix[masked_nan] = -1\n",
    "im = ax.imshow(pvalue_matrix, interpolation='none',cmap='Reds')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "cbar = plt.colorbar(im,cax=cax)\n",
    "cbar.ax.set_yticks([-1,0,1])\n",
    "cbar.ax.set_yticklabels([\"n/a\",\"n.s.\",\"*\"], fontsize=FONTSIZE)\n",
    "for _spine in [\"top\",\"right\",\"bottom\",\"left\"]:\n",
    "    ax.spines[_spine].set_visible(False)\n",
    "ax.set_yticks([i for i in range(len(bold_f_type_strs))])\n",
    "ax.set_yticklabels(bold_f_type_strs, fontsize=FONTSIZE-2, rotation=0)\n",
    "ax.set_xticks([i for i in range(len(bold_f_type_strs)+2)])\n",
    "ax.set_xticklabels(bold_f_type_strs+[\"hemi\",\"gd\"], fontsize=FONTSIZE-2, rotation=90)\n",
    "ax.set_ylabel(\"BOLD Power\",fontsize=FONTSIZE)\n",
    "ax.set_xlabel(\"Variables\",fontsize=FONTSIZE)\n",
    "ax.set_title(\"Linear mixed GLM on power\",fontsize=FONTSIZE)\n",
    "ax.tick_params(\"both\",pad=0,width=.25,length=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "hmm.. there is a positive gd vs. power (f2-f1) relationship. Upon inspection it appears .75Hz maps can lead to possible high power false positives in areas outside of visual hierarchy. Solution: Only look at this relationship in predefined regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. Lets look at each dataset and only include ROIs that include in >= 50% of datasets (or 5/9 datasets), `thr` is used to define this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hcp_formula(formula, df):\n",
    "    dependent_var = formula.split(\"~\")[0].replace(\" \",\"\")\n",
    "    formula_components = formula.split(\"~\")[-1].replace(\" \",\"\").split(\"+\")\n",
    "    updated_formula_components = []\n",
    "\n",
    "    for col in formula_components:\n",
    "        n_categories = len(df[col].unique())\n",
    "        if n_categories == 2:\n",
    "            updated_formula_components.append(col)\n",
    "\n",
    "    if formula.startswith(\"bold_power_f2_sub_f1\"):\n",
    "        n_categories = len(df[\"binary_f1:binary_f2\"].unique())\n",
    "        if n_categories == 2:\n",
    "            updated_formula_components.append(\"binary_f1:binary_f2\")\n",
    "\n",
    "    new_formula = f\"{dependent_var} ~ {'+'.join(updated_formula_components)}\"\n",
    "\n",
    "    return new_formula, len(updated_formula_components)\n",
    "\n",
    "def power_effects_across_hcp_labels(group_filtered_df, formula, experiment_type=\"All\"):\n",
    "\n",
    "    assert experiment_type in [\"All\", \"Normal\", \"3TNormal\", \"7TNormal\", \"3TVary\", \"7TVary\", \"3TVary_020\", \"7TVary_020\", \"3TVary_021\", \"7TVary_021\"]\n",
    "\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    # Create interaction term for f1 and f2 vertices\n",
    "    group_filtered_df[\"binary_f1:binary_f2\"] = ( (group_filtered_df.binary_f1>0).astype(int) * (group_filtered_df.binary_f2>0).astype(int) )\n",
    "    group_filtered_df[\"binary_f1:binary_f2\"] -= group_filtered_df[\"binary_f1:binary_f2\"].mean() # Demean\n",
    "        \n",
    "    # Count total ROIs to control figure size better\n",
    "    contra_ipsi_hcp_label_count = 0\n",
    "    for hemi in [\"CONTRA\", \"IPSI\"]:\n",
    "        # Contralateral HCP labels - ordered by geodesic distance\n",
    "        hcp_labels_ordered_by_gd = [i for i in group_filtered_df.sort_values(by=\"geodesic_distance\").hcp_label.unique() if i.startswith(hemi)]\n",
    "        n_hcp_labels = len(hcp_labels_ordered_by_gd)\n",
    "        contra_ipsi_hcp_label_count += n_hcp_labels\n",
    "\n",
    "    fig, ax_dict = plt.subplot_mosaic([[\"CONTRA_COUNT\",\"IPSI_COUNT\"],[\"CONTRA_COEFS\",\"IPSI_COEFS\"]],figsize=(.5+(6/20*contra_ipsi_hcp_label_count),2.5), layout=\"constrained\", dpi=300)\n",
    "\n",
    "    for hemi in [\"CONTRA\", \"IPSI\"]:\n",
    "\n",
    "        # Contralateral HCP labels - ordered by geodesic distance\n",
    "        hcp_labels_ordered_by_gd = [i for i in group_filtered_df.sort_values(by=\"geodesic_distance\").hcp_label.unique() if i.startswith(hemi)]\n",
    "        n_hcp_labels = len(hcp_labels_ordered_by_gd)\n",
    "        if n_hcp_labels == 0:\n",
    "            for ax_label in [f\"{hemi}_COUNT\",f\"{hemi}_COEFS\"]:\n",
    "                ax_dict[ax_label].set_visible(False)\n",
    "            continue\n",
    "        hcp_formula = formula.split(\" + hemi\")[0]\n",
    "        all_variables = hcp_formula.replace(\" \",\"\").split(\"~\")[1].split(\"+\")\n",
    "        n_variables = len(all_variables)\n",
    "\n",
    "        if hcp_formula.startswith(\"bold_power_f2_sub_f1\"):\n",
    "            all_variables.append(\"binary_f1:binary_f2\")\n",
    "            n_variables += 1\n",
    "        coefs = np.zeros((n_variables, n_hcp_labels))\n",
    "        pvals = np.zeros((n_variables, n_hcp_labels))\n",
    "        experiments_per_hcp_label = np.zeros((1, n_hcp_labels))\n",
    "\n",
    "        if experiment_type == \"All\":\n",
    "            total_experiments = len(group_filtered_df.experiment_sub_id.unique())\n",
    "        else:\n",
    "            total_experiments = len(group_filtered_df[(group_filtered_df.experiment_sub_id.str.contains(experiment_type))].experiment_sub_id.unique())\n",
    "\n",
    "        for hcp_ix, hcp_label in enumerate(hcp_labels_ordered_by_gd):\n",
    "            if experiment_type == \"All\":\n",
    "                hcp_group_filtered_df = group_filtered_df[(group_filtered_df.hcp_label==hcp_label)]\n",
    "            else:\n",
    "                hcp_group_filtered_df = group_filtered_df[(group_filtered_df.hcp_label==hcp_label) & (group_filtered_df.experiment_sub_id.str.contains(experiment_type))]\n",
    "            experiments_per_hcp_label[0,hcp_ix] = len(hcp_group_filtered_df.experiment_sub_id.unique()) / total_experiments\n",
    "            hcp_formula, n_vars = update_hcp_formula(formula, hcp_group_filtered_df)\n",
    "\n",
    "            if n_vars != 0:\n",
    "                if hcp_group_filtered_df.shape[0] <= 2:\n",
    "                    print(f\"{hcp_label} only has {hcp_group_filtered_df.shape[0]} experiments. Skipping.\")\n",
    "                else:\n",
    "                    #print(hcp_formula)\n",
    "                    model = mixedlm(\n",
    "                        hcp_formula,\n",
    "                        hcp_group_filtered_df, \n",
    "                        groups=hcp_group_filtered_df[\"experiment_sub_id\"], \n",
    "                    )\n",
    "                    try:\n",
    "                        result = model.fit()\n",
    "                        not_completed = False\n",
    "                    except:\n",
    "                        not_completed = True\n",
    "              \n",
    "            for m_ix, m in enumerate(all_variables):\n",
    "                vars_not_in_formula = hcp_formula.split(\"~\")[-1].replace(\" \",\"\").split(\"+\")\n",
    "                if m not in vars_not_in_formula or hcp_group_filtered_df.shape[0] <= 2 or not_completed:\n",
    "                    pval = np.nan\n",
    "                    coef = np.nan\n",
    "                else:\n",
    "                    pval = result.pvalues[m] if n_vars!=0 else np.nan\n",
    "                    coef = result.params[m] if n_vars!=0 else np.nan\n",
    "\n",
    "                coefs[m_ix,hcp_ix] = coef\n",
    "                pvals[m_ix,hcp_ix] = pval \n",
    "        \n",
    "        ax = ax_dict[f\"{hemi}_COUNT\"]\n",
    "        norm = mcolors.Normalize(vmin=0, vmax=1)\n",
    "        cax = ax.imshow(experiments_per_hcp_label, cmap=\"BuPu\",norm=norm)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cbar_ax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        cbar = fig.colorbar(cax, cax=cbar_ax)\n",
    "        #cbar.set_label(\"Experiment count\", fontsize=FONTSIZE-2)\n",
    "        cbar.set_ticks([0, 1])\n",
    "        cbar.set_ticklabels([0, int(total_experiments)], fontsize=FONTSIZE-2)\n",
    "        for i in range(experiments_per_hcp_label.shape[1]):\n",
    "            c= 'k'\n",
    "            if experiments_per_hcp_label[0,i]*total_experiments > .8*total_experiments:\n",
    "                c='w'\n",
    "            ax.text(i,0,f\"{experiments_per_hcp_label[0,i]*total_experiments:.0f}\",c=c,va=\"center\",ha=\"center\",fontsize=FONTSIZE,weight=\"bold\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f\"{hemi}\",fontsize=FONTSIZE)\n",
    "\n",
    "        ax = ax_dict[f\"{hemi}_COEFS\"]\n",
    "        _min = np.nanmin(coefs)\n",
    "        _max = np.nanmax(coefs)\n",
    "        if np.nanmin(coefs) >= 0:\n",
    "            _min = -.00001\n",
    "        if np.nanmax(coefs) <= 0:\n",
    "            _max = .00001\n",
    "        norm = mcolors.TwoSlopeNorm(vmin=_min,vmax=_max,vcenter=0)\n",
    "        cax = ax.imshow(coefs, cmap=\"bwr\", norm=norm)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cbar_ax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        cbar = fig.colorbar(cax, cax=cbar_ax)\n",
    "        #cbar.set_label(\"GLM coefs\", fontsize=FONTSIZE-2)\n",
    "        cbar.set_ticks([np.nanmin(coefs), 0, np.nanmax(coefs)])\n",
    "        cbar.set_ticklabels([f\"{i:.1e}\" for i in cbar.get_ticks().astype(float)], fontsize=FONTSIZE-2)\n",
    "        ax.set_yticks(range(n_variables))\n",
    "        ax.set_yticklabels(all_variables, fontsize=FONTSIZE-2)\n",
    "        ax.set_xticks(range(n_hcp_labels))\n",
    "        ax.set_xticklabels([i.split(\"_\")[-1] for i in hcp_labels_ordered_by_gd],fontsize=FONTSIZE-2, rotation=90)\n",
    "        for row_ix, row in enumerate(pvals):\n",
    "            for col_ix, element in enumerate(row):\n",
    "                _FONTSIZE = FONTSIZE\n",
    "                if element < .05:\n",
    "                    _text = \"*\"\n",
    "                    _FONTSIZE += 2\n",
    "                elif np.isnan(element):\n",
    "                    _text = \"N/A\"\n",
    "                    _FONTSIZE -= 2\n",
    "                else:\n",
    "                    _text = \"\"\n",
    "                ax.text(col_ix, row_ix, _text, ha=\"center\", va=\"center\", fontsize=_FONTSIZE,weight=\"bold\")\n",
    "        \n",
    "    for ax_key, ax in ax_dict.items():\n",
    "        for _spine in [\"right\",\"bottom\",\"top\",\"left\"]:\n",
    "            ax.spines[_spine].set_visible(False)\n",
    "            ax.tick_params(\"both\",pad=.5,width=0,length=0.25)\n",
    "\n",
    "    title_formula = formula.split(\"+ hemi\")[0]\n",
    "    title_formula = title_formula.split(\"+ geo\")[0]\n",
    "    fig.suptitle(f\"{title_formula}, {experiment_type}\", fontsize=FONTSIZE)\n",
    "    \n",
    "    frequency_type = formula.replace(\" \",\"\").split(\"~\")[0]\n",
    "    fig.savefig(f\"experiment-{experiment_type}_frequency-{frequency_type}_power_across_hcp_labels.png\",dpi='figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "additional_rules_dict = {\n",
    "    \"All\": (None, \"Normal\", \"Normal\"),\n",
    "    \"Normal\": (\"Normal\",\"3TNormal\", \"entrain\"),\n",
    "    \"3TNormal\": (\"3TNormal\",\"3TNormal\",\"entrain\"),\n",
    "    \"7TNormal\": (\"7TNormal\",\"7TNormal\",\"AttendAway\"),\n",
    "    \"3TVary\": (\"3TVary\",\"3TNormal\",\"entrain\"),\n",
    "    \"7TVary\": (\"7TVary\",\"7TNormal\",\"AttendAway\"),\n",
    "}\n",
    "filter_outliers = True\n",
    "apply_log_to_gd = True\n",
    "all_dfs_to_plot = {}\n",
    "for k, v in additional_rules_dict.items():\n",
    "    additional_rules = v[0]\n",
    "    experiment_label = v[1]\n",
    "    task_condition = v[2]\n",
    "    # Verbose\n",
    "    if filter_outliers:\n",
    "        print(\"Filtering outliers during mixed GLM.\")\n",
    "    if apply_log_to_gd:\n",
    "        print(\"Applying log to geodesic distance during mixed GLM.\")\n",
    "    # Get ROIs by filtering across 3TNormal dataset and based on pct of experiments with that ROI\n",
    "    dfs_to_plot = {}\n",
    "    for thr in [.5]:\n",
    "        keep_rois = {}\n",
    "        for im_type in [\"f1\",\"f2\",\"f2_sub_f1\",\"2f1\"]:\n",
    "            im_df = im_code_df_dict[im_type].copy()\n",
    "            # Select only 3TNormal & entrain condition\n",
    "            filter_im_df = im_df[(im_df.experiment_sub_id.str.contains(experiment_label)) & (im_df.experiment_sub_id.str.contains(task_condition))]\n",
    "            n_experiments = len(filter_im_df.experiment_sub_id.unique())\n",
    "            # Calculate threshold based on number of experiments\n",
    "            _thr = n_experiments * thr #\n",
    "            # Get ROI count across all experiments\n",
    "            roi_count = {}\n",
    "            for experiment_sub_id in filter_im_df.experiment_sub_id.unique():\n",
    "                for hcp_label in filter_im_df[(filter_im_df.experiment_sub_id == experiment_sub_id)].hcp_label.unique():\n",
    "                    if hcp_label not in roi_count:\n",
    "                        roi_count[hcp_label] = 0\n",
    "                    roi_count[hcp_label] += 1\n",
    "            # Filter out ROIs with less than threshold\n",
    "            thresholded_roi_list = []\n",
    "            for hcp_label, count in roi_count.items():\n",
    "                if count >= _thr:\n",
    "                    thresholded_roi_list.append(hcp_label)\n",
    "            # Filter df to exclude vertices not in `thresholded_roi_list`\n",
    "            im_df = im_df[(im_df.hcp_label.isin(thresholded_roi_list))]\n",
    "            keep_rois[im_type] = thresholded_roi_list\n",
    "\n",
    "            # Plot\n",
    "            for experiment_sub_id in im_df.experiment_sub_id.unique():\n",
    "                try:\n",
    "                    sub_im_df = im_df[(im_df.experiment_sub_id==experiment_sub_id)]\n",
    "                    power = sub_im_df[f\"bold_power_{im_type}\"]\n",
    "                    gd = sub_im_df[\"geodesic_distance\"]\n",
    "                except:\n",
    "                    print(f\"Skip {sub_id}\")\n",
    "\n",
    "        # Run statistics\n",
    "        experiment_ids = [\"3TNormal\"]*18+[\"7TNormal\"]*4+[\"3TVary\"]*6+[\"7TVary\"]*6\n",
    "        sub_ids = [\"000\",\"002\",\"003\",\"004\",\"005\",\"006\",\"007\",\"008\",\"009\"]*2+[\"Pilot001\",\"Pilot009\",\"Pilot010\",\"Pilot011\"]+[\"020\"]*3+[\"021\"]*3+[\"020\"]*3+[\"021\"]*3\n",
    "        roi_task_ids = [\"entrain\"]*18+[\"AttendAway\"]*4+[f\"entrain{i}\" for i in [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]]*2\n",
    "        task_ids = [\"control\"]*9+[\"entrain\"]*9+[\"AttendAway\"]*4+[f\"entrain{i}\" for i in [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]]*2\n",
    "        bold_f_type_strs = [\"f1\",\"f2\",\"f2_sub_f1\",\"2f1\"]\n",
    "\n",
    "        n_sub_ids = len(sub_ids)\n",
    "        n_bold_f_types = len(bold_f_type_strs)\n",
    "\n",
    "        fig, ax_dict = plt.subplot_mosaic([bold_f_type_strs], layout=\"constrained\", figsize=(8,2),dpi=400)\n",
    "        mixed_glm_cohort_pvalues = np.zeros((len(bold_f_type_strs),n_bold_f_types+2))\n",
    "        mixed_glm_cohort_coefs = np.zeros((len(bold_f_type_strs),n_bold_f_types+2))\n",
    "        for bold_power_type_ix, bold_power_type in enumerate(bold_f_type_strs):\n",
    "\n",
    "            ax = ax_dict[bold_power_type]\n",
    "            #predict_ax = predict_ax_dict[bold_power_type]\n",
    "            #geo_ax = geo_ax_dict[bold_power_type]\n",
    "            \"\"\"\n",
    "            hemi_ax = hemi_ax_dict[bold_power_type]\n",
    "            spatial_f1_ax = spatial_f1_ax_dict[bold_power_type]\n",
    "            spatial_f2_ax = spatial_f2_ax_dict[bold_power_type]\n",
    "            spatial_f2_sub_f1_ax = spatial_f2_sub_f1_ax_dict[bold_power_type]\n",
    "            spatial_2f1_ax = spatial_2f1_ax_dict[bold_power_type]\n",
    "            \"\"\"\n",
    "\n",
    "            group_filtered_df = None\n",
    "            cohort_pvalues = np.zeros((n_sub_ids,n_bold_f_types+2)) # +1 add hemi column, +1 add gd column\n",
    "            for sub_ix, (experiment_id,sub_id,roi_task_id,task_id) in enumerate(zip(experiment_ids,sub_ids,roi_task_ids,task_ids)):\n",
    "\n",
    "                # Reorganize\n",
    "                sub_expanded_df = expanded_df[(expanded_df.sub_id==sub_id) & (expanded_df.experiment_id==experiment_id) & (expanded_df.roi_task_id==expanded_df.roi_task_id) & (expanded_df.task_id==task_id)]\n",
    "                data_dict = defaultdict(list)\n",
    "                for vertex_id in sub_expanded_df.vertex_id.unique():\n",
    "                    _df = sub_expanded_df[sub_expanded_df.vertex_id==vertex_id]\n",
    "                    bold_power_f1 = np.nan\n",
    "                    bold_power_f2 = np.nan\n",
    "                    bold_power_f2_sub_f1 = np.nan\n",
    "                    bold_power_2f1 = np.nan\n",
    "                    binary_f1 = 0\n",
    "                    binary_f2 = 0\n",
    "                    binary_f2_sub_f1 = 0\n",
    "                    binary_2f1 = 0\n",
    "                    for _, row in _df.iterrows():\n",
    "                        if row.im_code == \"f1\":\n",
    "                            binary_f1 = 1\n",
    "                            bold_power_f1 = row.power\n",
    "                        elif row.im_code == \"f2\":\n",
    "                            binary_f2 = 1\n",
    "                            bold_power_f2 = row.power\n",
    "                        elif row.im_code == \"f2-f1\":\n",
    "                            binary_f2_sub_f1 = 1\n",
    "                            bold_power_f2_sub_f1 = row.power\n",
    "                        elif row.im_code == \"2f1\":\n",
    "                            binary_2f1 = 1\n",
    "                            bold_power_2f1 = row.power\n",
    "                        else:\n",
    "                            continue\n",
    "                        #else:\n",
    "                            #raise ValueError(f\"{row.im_code} not supported.\")\n",
    "                    data_dict[\"bold_power_f1\"].append(bold_power_f1)\n",
    "                    data_dict[\"bold_power_f2\"].append(bold_power_f2)\n",
    "                    data_dict[\"bold_power_f2_sub_f1\"].append(bold_power_f2_sub_f1)\n",
    "                    data_dict[\"bold_power_2f1\"].append(bold_power_2f1)\n",
    "                    data_dict[\"binary_f1\"].append(binary_f1)\n",
    "                    data_dict[\"binary_f2\"].append(binary_f2)\n",
    "                    data_dict[\"binary_f2_sub_f1\"].append(binary_f2_sub_f1)\n",
    "                    data_dict[\"binary_2f1\"].append(binary_2f1)\n",
    "                    data_dict[\"im_count\"].append(_df.shape[0])\n",
    "                    data_dict[\"vertex_id\"].append(vertex_id)\n",
    "                    data_dict[\"sub_id\"].append(sub_id)\n",
    "                    data_dict[\"hcp_label\"].append(row.hcp_roi)\n",
    "                    data_dict[\"hemi\"].append(row.hemi)\n",
    "                    data_dict[\"geodesic_distance\"].append(row.geodesic_distance)\n",
    "                    data_dict[\"experiment_sub_id\"].append(row.experiment_sub_id)\n",
    "                _df = pd.DataFrame(data_dict)\n",
    "\n",
    "                \"\"\"STATS\n",
    "                Model a frequencies' power with spatial information of other exist stimulating and IM frequencies\n",
    "                \"\"\"\n",
    "                keep_rows = [f\"bold_power_{bold_power_type}\", \"im_count\", \"hcp_label\",\"vertex_id\"] + [f\"binary_{i}\" for i in bold_f_type_strs] + [\"hemi\",\"geodesic_distance\",\"experiment_sub_id\"]\n",
    "                filtered_df = _df[(pd.notna(_df[f\"bold_power_{bold_power_type}\"]))][keep_rows]\n",
    "                y = filtered_df[f\"bold_power_{bold_power_type}\"]\n",
    "                X = filtered_df[[f\"binary_{i}\" for i in bold_f_type_strs]+[\"hemi\",\"geodesic_distance\"]]\n",
    "                if y.shape[0] == 0:\n",
    "                    for var_ix in range(6):\n",
    "                        cohort_pvalues[sub_ix,var_ix] = -1\n",
    "                    continue\n",
    "\n",
    "                X = sm.add_constant(X)\n",
    "                model = sm.OLS(y,X).fit()\n",
    "\n",
    "                for var_ix, (var,pval) in enumerate(model.pvalues.items()):\n",
    "                    if np.isnan(pval):\n",
    "                        cohort_pvalues[sub_ix,var_ix] = -1\n",
    "                    else:\n",
    "                        cohort_pvalues[sub_ix,var_ix] = pval\n",
    "                # Store data for group level analysis (mixed lm model)\n",
    "                if group_filtered_df is None:\n",
    "                    group_filtered_df = filtered_df\n",
    "                else:\n",
    "                    group_filtered_df = pd.concat((group_filtered_df,filtered_df))\n",
    "\n",
    "            if len(keep_rois[bold_power_type]) == 1:\n",
    "                print(f\"Only one ROI found for {bold_power_type}, can't perform mixed GLM due to singular matrix\")\n",
    "                continue\n",
    "            # Filter vertices that contain ROIs based on keep_rois\n",
    "            group_filtered_df = group_filtered_df[(group_filtered_df.hcp_label.isin(keep_rois[bold_power_type]))]\n",
    "            dfs_to_plot[bold_power_type] = group_filtered_df\n",
    "            \"\"\"STATS\n",
    "            with mixed linear models on group level data\n",
    "            \"\"\"\n",
    "            #print(group_filtered_df.shape)\n",
    "            # Remove control condition\n",
    "            if additional_rules is None:\n",
    "                group_filtered_df = group_filtered_df[(~group_filtered_df.experiment_sub_id.str.contains(\"control\"))]\n",
    "            else:\n",
    "                group_filtered_df = group_filtered_df[(~group_filtered_df.experiment_sub_id.str.contains(\"control\")) & (group_filtered_df.experiment_sub_id.str.contains(additional_rules))]\n",
    "            print(\"After removing control:\", group_filtered_df.shape)\n",
    "            # Log transform gd\n",
    "            if apply_log_to_gd:\n",
    "                group_filtered_df[\"geodesic_distance\"] = np.log(group_filtered_df[\"geodesic_distance\"])\n",
    "            # Demean all vars\n",
    "            group_filtered_df[\"geodesic_distance\"] = group_filtered_df.geodesic_distance - group_filtered_df.geodesic_distance.mean()\n",
    "            for col in [[f\"binary_{i}\" for i in bold_f_type_strs] + [\"hemi\",\"geodesic_distance\"]]:\n",
    "                group_filtered_df[col] = group_filtered_df[col] - group_filtered_df[col].mean()\n",
    "            # Detect upperbound outlier information\n",
    "            q1 = group_filtered_df[f\"bold_power_{bold_power_type}\"].quantile(.01)\n",
    "            q3 = group_filtered_df[f\"bold_power_{bold_power_type}\"].quantile(.99)\n",
    "            iqr = q3-q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            outliers = (group_filtered_df[f\"bold_power_{bold_power_type}\"] > upper_bound)\n",
    "            print(f\"Identified {outliers.sum()} outliers\")\n",
    "            outliers_df = group_filtered_df[outliers]\n",
    "            bold_ymax = group_filtered_df[~outliers][f\"bold_power_{bold_power_type}\"].max()\n",
    "            outlier_hcp_labels = group_filtered_df[outliers][\"hcp_label\"].unique()\n",
    "            print(\"OUTLIER ROIS INCLUDE:\", outlier_hcp_labels, outliers_df.shape)\n",
    "            if filter_outliers:\n",
    "                group_filtered_df = group_filtered_df[~outliers]\n",
    "            # Get formula\n",
    "            formula = get_formula(bold_power_type, bold_f_type_strs)\n",
    "            # Remove hemi regressor if only 1 laterality is found\n",
    "            if not any(i.startswith(\"CONTRA\") for i in keep_rois[bold_power_type]) or not any(i.startswith(\"IPSI\") for i in keep_rois[bold_power_type]):\n",
    "                formula = formula.replace(\"+ hemi \", \" \")\n",
    "\n",
    "            # Remove maps with no vertex assignment\n",
    "            remove_l = []\n",
    "            for i in group_filtered_df.columns:\n",
    "                if i.startswith(\"binary\"):\n",
    "                    _sum = group_filtered_df[i].sum()\n",
    "                    if _sum == 0:\n",
    "                        remove_l.append(i)\n",
    "            for r in remove_l:\n",
    "                #print(f\"Remove {r}\")\n",
    "                formula = formula.replace(f\"+ {r} \",\" \")\n",
    "            # Mixed LM\n",
    "            #print(f\"formula: {formula}\")\n",
    "            try:\n",
    "                model = mixedlm(\n",
    "                    formula, \n",
    "                    group_filtered_df, \n",
    "                    groups=group_filtered_df[\"experiment_sub_id\"], \n",
    "                )\n",
    "                result = model.fit()\n",
    "            except np.linalg.LinAlgError as e:\n",
    "                print(f\"An error occured {type(e).__name__} - {e}\")\n",
    "                continue\n",
    "            # Predict BOLD power with model\n",
    "            model_cols = [i.strip() for i in formula.split(\"~\")[-1].split(\"+\")] + [\"experiment_sub_id\"]\n",
    "            _data = group_filtered_df[model_cols]\n",
    "            y_predict = result.predict(_data)\n",
    "            print(result.summary())\n",
    "            \"\"\"Plot single subject OLS fits\n",
    "            \"\"\"\n",
    "            masked_sig = cohort_pvalues < .05\n",
    "            masked_nan = cohort_pvalues == -1\n",
    "            cohort_pvalues[~masked_sig] = 0\n",
    "            cohort_pvalues[masked_sig] = 1\n",
    "            cohort_pvalues[masked_nan] = -1\n",
    "            im = ax.imshow(cohort_pvalues, interpolation='none',cmap='Reds')\n",
    "            from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            cbar = plt.colorbar(im,cax=cax)\n",
    "            cbar.ax.set_yticks([-1,0,1])\n",
    "            cbar.ax.set_yticklabels([\"n/a\",\"n.s.\",\"*\"], fontsize=FONTSIZE)\n",
    "            for _spine in [\"top\",\"right\",\"bottom\",\"left\"]:\n",
    "                ax.spines[_spine].set_visible(False)\n",
    "            ax.set_yticks([i for i in range(len(sub_ids))])\n",
    "            ax.set_yticklabels(sub_ids, fontsize=FONTSIZE-2, rotation=0)\n",
    "            ax.set_xticks([i for i in range(len(bold_f_type_strs)+2)])\n",
    "            ax.set_xticklabels(bold_f_type_strs+[\"hemi\",\"gd\"], fontsize=FONTSIZE-2, rotation=90)\n",
    "            ax.set_title(bold_power_type,fontsize=FONTSIZE)\n",
    "            ax.tick_params(\"both\",pad=0,width=.25,length=2)\n",
    "\n",
    "            # fill `mixed_glm_cohort_pvalues`\n",
    "            for i,m in enumerate([\"binary_f1\",\"binary_f2\",\"binary_f2_sub_f1\",\"binary_2f1\",\"hemi\",\"geodesic_distance\"]):\n",
    "                try:\n",
    "                    pval = result.pvalues[m]\n",
    "                    mixed_glm_cohort_pvalues[bold_power_type_ix,i] = pval\n",
    "                    coef = result.params[m]\n",
    "                    mixed_glm_cohort_coefs[bold_power_type_ix,i] = coef\n",
    "                except:\n",
    "                    mixed_glm_cohort_pvalues[bold_power_type_ix,i] = -1 # Not included in model\n",
    "                    mixed_glm_cohort_coefs[bold_power_type_ix,i] = 0\n",
    "\n",
    "            # Plot association to power effects across HCP labels\n",
    "            power_effects_across_hcp_labels(group_filtered_df, formula, experiment_type=k)\n",
    "\n",
    "        # Plot mixed GLM p-values\n",
    "        fig, ax = plt.subplots(figsize=(2,2),dpi=300,layout=\"tight\")\n",
    "        masked_sig = mixed_glm_cohort_pvalues < .05\n",
    "        masked_nan = mixed_glm_cohort_pvalues == -1\n",
    "        pvalue_matrix = np.zeros_like(mixed_glm_cohort_pvalues)\n",
    "        pvalue_matrix[~masked_sig] = 0\n",
    "        pvalue_matrix[masked_sig] = 1\n",
    "        pvalue_matrix[masked_nan] = -1\n",
    "        im = ax.imshow(pvalue_matrix, interpolation='none',cmap='Reds',)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        cbar = plt.colorbar(im,cax=cax)\n",
    "        cbar.ax.set_yticks([-1,0,1])\n",
    "        cbar.ax.set_yticklabels([\"n/a\",\"n.s.\",\"*\"], fontsize=FONTSIZE)\n",
    "        for _spine in [\"top\",\"right\",\"bottom\",\"left\"]:\n",
    "            ax.spines[_spine].set_visible(False)\n",
    "        ax.set_yticks([i for i in range(len(bold_f_type_strs))])\n",
    "        ax.set_yticklabels(bold_f_type_strs, fontsize=FONTSIZE-2, rotation=0)\n",
    "        ax.set_xticks([i for i in range(len(bold_f_type_strs)+2)])\n",
    "        ax.set_xticklabels(bold_f_type_strs+[\"hemi\",\"gd\"], fontsize=FONTSIZE-2, rotation=90)\n",
    "        ax.set_ylabel(\"BOLD Power\",fontsize=FONTSIZE)\n",
    "        ax.set_xlabel(\"Variables\",fontsize=FONTSIZE)\n",
    "        ax.set_title(f\"{thr} Linear mixed GLM on power\",fontsize=FONTSIZE)\n",
    "        ax.tick_params(\"both\",pad=0,width=.25,length=2)\n",
    "        # Plot mixed GLM coefficients\n",
    "        fig, ax = plt.subplots(figsize=(2,2),dpi=300,layout=\"tight\")\n",
    "        _max = np.abs(mixed_glm_cohort_coefs).max()\n",
    "        cax = ax.imshow(mixed_glm_cohort_coefs, cmap='bwr',vmin=-_max/4, vmax=_max/4)\n",
    "        cbar = fig.colorbar(cax, ax=ax, shrink=.3)\n",
    "        for _spine in [\"top\",\"right\",\"bottom\",\"left\"]:\n",
    "            ax.spines[_spine].set_visible(False)\n",
    "        ax.set_yticks([i for i in range(len(bold_f_type_strs))])\n",
    "        ax.set_yticklabels(bold_f_type_strs, fontsize=FONTSIZE-2, rotation=0)\n",
    "        ax.set_xticks([i for i in range(len(bold_f_type_strs)+2)])\n",
    "        ax.set_xticklabels(bold_f_type_strs+[\"hemi\",\"gd\"], fontsize=FONTSIZE-2, rotation=90)\n",
    "        ax.set_ylabel(\"BOLD Power\",fontsize=FONTSIZE)\n",
    "        ax.set_xlabel(\"Variables\",fontsize=FONTSIZE)\n",
    "        ax.set_title(f\"{additional_rules}/{thr} Linear mixed GLM on power\",fontsize=FONTSIZE)\n",
    "        ax.tick_params(\"both\",pad=0,width=.25,length=2)\n",
    "\n",
    "    all_dfs_to_plot[k] = dfs_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get HCP color map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get HCP labels\n",
    "\"\"\"\n",
    "dlabel_dir = Path(\"/opt/app/notebooks/data/dlabels\")\n",
    "hcp_label = dlabel_dir / \"Q1-Q6_RelatedValidation210.CorticalAreas_dil_Final_Final_Areas_Group_Colors.32k_fs_LR.dlabel.nii\"\n",
    "\n",
    "_HCP_INFO = !wb_command -file-information {hcp_label}\n",
    "HCP_LABELS = []\n",
    "HCP_COUNTER = 0\n",
    "hcp_c_dict = {}\n",
    "for i in _HCP_INFO:\n",
    "    if len(i) == 60 and any([\"L_\" in i, \"R_\" in i]):\n",
    "        hcp_colors = tuple([float(f\"0.{k}\") for k in [j.split(' ') [0] for j in i.split('0.')][-3:]] + [1])\n",
    "        if \"R_\" in i:\n",
    "            hcp_c_dict[i.split(\"R_\")[-1].split(\"_ROI\")[0]]=hcp_colors\n",
    "\n",
    "hcp_c_dict[\"V1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot only contralateral HCP rois vs geodesic distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_gd = False\n",
    "\n",
    "mosaic = [\"f1\",\"f2\",\"f2_sub_f1\",\"2f1\"]\n",
    "for k, v in additional_rules_dict.items():\n",
    "\n",
    "    fig, ax_dict = plt.subplot_mosaic([mosaic], layout=\"constrained\", figsize=(5.5, 1.5), dpi=300)\n",
    "\n",
    "    dfs_to_plot = all_dfs_to_plot[k].copy()\n",
    "    for bold_power_type in [\"f1\",\"f2\",\"f2_sub_f1\",\"2f1\"]:\n",
    "        ax = ax_dict[bold_power_type]\n",
    "        # Get df for power of interest\n",
    "        try:\n",
    "            group_filtered_df = dfs_to_plot[bold_power_type].copy()\n",
    "        except:\n",
    "            continue\n",
    "        # Refilter dataframes\n",
    "        plot_dict = defaultdict(list)\n",
    "        for experiment_sub_id in group_filtered_df.experiment_sub_id.unique():\n",
    "            experiment_df = group_filtered_df[group_filtered_df.experiment_sub_id == experiment_sub_id]\n",
    "            for hcp_label in experiment_df.hcp_label.unique():\n",
    "                if hcp_label.startswith(\"IPSI\"):\n",
    "                    continue\n",
    "\n",
    "                experiment_hcp_df = experiment_df[experiment_df.hcp_label == hcp_label]\n",
    "                vertex_count = experiment_hcp_df.shape[0]\n",
    "                gd = experiment_hcp_df[\"geodesic_distance\"].mean()\n",
    "                power = experiment_hcp_df[f\"bold_power_{bold_power_type}\"].mean()\n",
    "                plot_dict[\"hcp_label\"].append(hcp_label.split(\"_\")[-1])\n",
    "                plot_dict[\"experiment_sub_id\"].append(experiment_sub_id)\n",
    "                plot_dict[\"vertex_count\"].append(vertex_count)\n",
    "                plot_dict[\"geodesic_distance\"].append(gd)\n",
    "                plot_dict[f\"bold_power_{bold_power_type}\"].append(power)\n",
    "        plot_df = pd.DataFrame(plot_dict)\n",
    "\n",
    "        if log_gd:\n",
    "            plot_df[\"geodesic_distance\"] = np.log(plot_df.geodesic_distance)\n",
    "\n",
    "        power_max = 0\n",
    "        gd_max = 0\n",
    "        gd_min = None\n",
    "        for experiment_sub_id in plot_df.experiment_sub_id.unique():\n",
    "            if \"control\" in experiment_sub_id:\n",
    "                continue\n",
    "            if v[0] is None:\n",
    "                pass\n",
    "            elif v[0] not in experiment_sub_id:\n",
    "                continue\n",
    "            experiment_sub_df = plot_df[(plot_df.experiment_sub_id==experiment_sub_id) & (plot_df.vertex_count>1)]\n",
    "            experiment_sub_df = experiment_sub_df.sort_values(\"geodesic_distance\") # Sort by gd\n",
    "            # Update min/max\n",
    "            power_max = max(power_max, experiment_sub_df[f\"bold_power_{bold_power_type}\"].max())\n",
    "            gd_max = max(gd_max, experiment_sub_df.geodesic_distance.max())\n",
    "            if gd_min is None:\n",
    "                gd_min = experiment_sub_df.geodesic_distance.min()\n",
    "            gd_min = min(gd_min, experiment_sub_df.geodesic_distance.min())\n",
    "\n",
    "            ax.plot(experiment_sub_df.geodesic_distance, experiment_sub_df[f\"bold_power_{bold_power_type}\"],c='k',zorder=5, linewidth=.2, alpha=.2)\n",
    "            for _, row in experiment_sub_df.iterrows():\n",
    "                ax.scatter(\n",
    "                    row.geodesic_distance, \n",
    "                    row[f\"bold_power_{bold_power_type}\"],\n",
    "                    s=10+row[\"vertex_count\"]/10, c=hcp_c_dict[row.hcp_label],\n",
    "                    marker=\"o\", edgecolor='lightgrey',linewidth=.2, alpha=.6,\n",
    "                    zorder=10\n",
    "                )\n",
    "        \n",
    "        power_min = 20\n",
    "        ax.set_yticks([0,power_max])\n",
    "        ax.set_yticklabels([f\"{i:.2e}\" if i!=0 else \"0\" for i in [0,power_max]],fontsize=FONTSIZE)\n",
    "        ax.set_xticks([gd_min-gd_max*.1, gd_max+gd_max*.1])\n",
    "        ax.set_title(f\"{k}, {bold_power_type}\")\n",
    "        ax.set_xticklabels([\" \"]*2, fontsize=FONTSIZE)\n",
    "\n",
    "        for _spine in [\"top\",\"right\",\"bottom\",]:\n",
    "            ax.spines[_spine].set_visible(False)\n",
    "        ax.spines.left.set_linewidth(.25)\n",
    "        ax.spines.left.set_bounds(0,power_max)\n",
    "\n",
    "        ax.tick_params(\"y\",pad=0,width=.25,length=4)\n",
    "        ax.tick_params(\"x\",pad=0,width=.25,length=0)\n",
    "\n",
    "        if bold_power_type == \"f1\":\n",
    "            ax.set_ylabel(\"%BOLD Power\",fontsize=FONTSIZE)\n",
    "        ax.set_xlabel(\"Geodesic distance from V1\",fontsize=FONTSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot vertex count across HCP rois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parcellated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global variables\n",
    "VERTEX_TO = 59412\n",
    "hemi_dict = {\n",
    "    \"CONTRA\": \"L\",\n",
    "    \"IPSI\": \"R\",\n",
    "}\n",
    "palette_params = {\n",
    "    \"disp-zero\": False,\n",
    "    \"disp-neg\": False,\n",
    "    \"disp-pos\": True,\n",
    "    \"pos-user\": (0, 1),\n",
    "    \"neg-user\": (-1,0),\n",
    "    \"interpolate\": True,\n",
    "}\n",
    "PALETTE = \"magma\"\n",
    "\n",
    "# Re-sort DF\n",
    "vertex_count_per_roi = expanded_df.groupby([\"experiment_sub_id\",\"im_code\",\"hcp_roi\"]).count().reset_index()\n",
    "\n",
    "# Inputs\n",
    "n_experiments_all = [9,4,3,3,3,3]\n",
    "hcp_roi_count_labels = [\"3TNormalEntrain\",\"7TNormalEntrain\",\"3TVary020Entrain\",\"7TVary020Entrain\",\"3TVary021Entrain\",\"7TVary021Entrain\"]\n",
    "patterns = [\"3TNormal.*entrain\",\"7TNormal.*AttendAway\",\"3TVary_020.*entrain.*\",\"7TVary_020.*entrain.*\",\"3TVary_021.*entrain.*\",\"7TVary_021.*entrain.*\"]\n",
    "im_codes = [\"f1\",\"f2\",\"f2-f1\",\"f1+f2\",\"2f1\",\"2f2\",\"2f1-f2\",\"2f2-f1\"]\n",
    "\n",
    "\n",
    "for hcp_roi_count_label, pattern, n_experiments in zip(hcp_roi_count_labels, patterns, n_experiments_all):\n",
    "    for im_code in im_codes:\n",
    "        _vertex_count_per_roi = vertex_count_per_roi[(vertex_count_per_roi.experiment_sub_id.str.contains(pattern)) & (vertex_count_per_roi.im_code==im_code)] # Filter df based on im type\n",
    "        print(hcp_roi_count_label, im_code, n_experiments)\n",
    "        roi_counts = _vertex_count_per_roi.groupby(\"hcp_roi\").size().reset_index(name=\"row_count\")\n",
    "        roi_counts[\"row_count\"] = roi_counts[\"row_count\"] / n_experiments\n",
    "        cohort_count_map = np.zeros((59412,))\n",
    "        for hemi_type in [\"CONTRA\",\"IPSI\"]:\n",
    "            hemi_roi_counts = roi_counts[(roi_counts.hcp_roi.str.startswith(hemi_type))]\n",
    "            for roi in hemi_roi_counts.hcp_roi:\n",
    "                hcp_suffix = roi.split(\"_\")[-1]\n",
    "                hemi_to_map_to = hemi_dict[hemi_type]\n",
    "                hcp_dscalar = f\"/tmp/{hemi_to_map_to}_{hcp_suffix}_ROI.dscalar.nii\"\n",
    "                hcp_coords = nib.load(hcp_dscalar).get_fdata()[0,:]==1\n",
    "                cohort_count_map[hcp_coords] = roi_counts[roi_counts.hcp_roi==roi].row_count.values[0]\n",
    "        \n",
    "        for hemisphere, flatmap, mapstyle_str in zip([\"left\",\"right\",\"left\"], [False,False,True], [\"lh-surf\",\"rh-surf\",\"flat\"]):\n",
    "            png_out = f\"{hcp_roi_count_label}_map-{mapstyle_str}_im-{im_code}_hcp-cohort-roi-count.png\" # Save png path\n",
    "            dscalar(\n",
    "                png_out, cohort_count_map, \n",
    "                orientation=\"portrait\", \n",
    "                hemisphere=hemisphere,\n",
    "                palette=PALETTE,\n",
    "                palette_params=palette_params,\n",
    "                transparent=False,\n",
    "                flatmap=flatmap,\n",
    "                flatmap_style='hcp_border',\n",
    "            )\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global variables\n",
    "VERTEX_TO = 59412\n",
    "hemi_dict = {\n",
    "    \"CONTRA\": \"L\",\n",
    "    \"IPSI\": \"R\",\n",
    "}\n",
    "palette_params = {\n",
    "    \"disp-zero\": False,\n",
    "    \"disp-neg\": False,\n",
    "    \"disp-pos\": True,\n",
    "    \"pos-user\": (0, 1),\n",
    "    \"neg-user\": (-1,0),\n",
    "    \"interpolate\": True,\n",
    "}\n",
    "PALETTE = \"magma\"\n",
    "\n",
    "# Re-sort DF\n",
    "vertex_count_per_roi = control_expanded_df.groupby([\"experiment_sub_id\",\"im_code\",\"hcp_roi\"]).count().reset_index()\n",
    "\n",
    "# Inputs\n",
    "n_experiments_all = [9]\n",
    "hcp_roi_count_labels = [\"3TControl\"]\n",
    "patterns = [\"3TControl.*control\"]\n",
    "im_codes = [\"f1\",\"f2\",\"f2-f1\",\"f1+f2\",\"2f1\",\"2f2\",\"2f1-f2\",\"2f2-f1\"]\n",
    "\n",
    "\n",
    "for hcp_roi_count_label, pattern, n_experiments in zip(hcp_roi_count_labels, patterns, n_experiments_all):\n",
    "    for im_code in im_codes:\n",
    "        _vertex_count_per_roi = vertex_count_per_roi[(vertex_count_per_roi.experiment_sub_id.str.contains(pattern)) & (vertex_count_per_roi.im_code==im_code)] # Filter df based on im type\n",
    "        print(hcp_roi_count_label, im_code, n_experiments)\n",
    "        roi_counts = _vertex_count_per_roi.groupby(\"hcp_roi\").size().reset_index(name=\"row_count\")\n",
    "        roi_counts[\"row_count\"] = roi_counts[\"row_count\"] / n_experiments\n",
    "        cohort_count_map = np.zeros((59412,))\n",
    "        for hemi_type in [\"CONTRA\",\"IPSI\"]:\n",
    "            hemi_roi_counts = roi_counts[(roi_counts.hcp_roi.str.startswith(hemi_type))]\n",
    "            for roi in hemi_roi_counts.hcp_roi:\n",
    "                hcp_suffix = roi.split(\"_\")[-1]\n",
    "                hemi_to_map_to = hemi_dict[hemi_type]\n",
    "                hcp_dscalar = f\"/tmp/{hemi_to_map_to}_{hcp_suffix}_ROI.dscalar.nii\"\n",
    "                hcp_coords = nib.load(hcp_dscalar).get_fdata()[0,:]==1\n",
    "                cohort_count_map[hcp_coords] = roi_counts[roi_counts.hcp_roi==roi].row_count.values[0]\n",
    "        \n",
    "        for hemisphere, flatmap, mapstyle_str in zip([\"left\",\"right\",\"left\"], [False,False,True], [\"lh-surf\",\"rh-surf\",\"flat\"]):\n",
    "            png_out = f\"{hcp_roi_count_label}_map-{mapstyle_str}_im-{im_code}_hcp-cohort-roi-count.png\" # Save png path\n",
    "            dscalar(\n",
    "                png_out, cohort_count_map, \n",
    "                orientation=\"portrait\", \n",
    "                hemisphere=hemisphere,\n",
    "                palette=PALETTE,\n",
    "                palette_params=palette_params,\n",
    "                transparent=False,\n",
    "                flatmap=flatmap,\n",
    "                flatmap_style='hcp_border',\n",
    "            )\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "VERTEX_TO = 59412\n",
    "hemi_dict = {\n",
    "    \"CONTRA\": \"L\",\n",
    "    \"IPSI\": \"R\",\n",
    "}\n",
    "palette_params = {\n",
    "    \"disp-zero\": False,\n",
    "    \"disp-neg\": False,\n",
    "    \"disp-pos\": True,\n",
    "    \"pos-user\": (0, 1),\n",
    "    \"neg-user\": (-1,0),\n",
    "    \"interpolate\": True,\n",
    "}\n",
    "PALETTE = \"magma\"\n",
    "\n",
    "# Re-sort DF\n",
    "vertex_count_per_roi = control_expanded_df.groupby([\"experiment_sub_id\",\"im_code\",\"hcp_roi\"]).count().reset_index()\n",
    "\n",
    "# Inputs\n",
    "n_experiments_all = [9]\n",
    "hcp_roi_count_labels = [\"3TControl\"]\n",
    "patterns = [\"3TControl.*control\"]\n",
    "im_codes = [\"f1\",\"f2\",\"f2-f1\",\"f1+f2\",\"2f1\",\"2f2\",\"2f1-f2\",\"2f2-f1\"]\n",
    "\n",
    "\n",
    "for hcp_roi_count_label, pattern, n_experiments in zip(hcp_roi_count_labels, patterns, n_experiments_all):\n",
    "    for im_code in im_codes:\n",
    "        _vertex_count_per_roi = vertex_count_per_roi[(vertex_count_per_roi.experiment_sub_id.str.contains(pattern)) & (vertex_count_per_roi.im_code==im_code)] # Filter df based on im type\n",
    "        print(hcp_roi_count_label, im_code, n_experiments)\n",
    "        roi_counts = _vertex_count_per_roi.groupby(\"hcp_roi\").size().reset_index(name=\"row_count\")\n",
    "        roi_counts[\"row_count\"] = roi_counts[\"row_count\"] / n_experiments\n",
    "        cohort_count_map = np.zeros((59412,))\n",
    "        for hemi_type in [\"CONTRA\",\"IPSI\"]:\n",
    "            hemi_roi_counts = roi_counts[(roi_counts.hcp_roi.str.startswith(hemi_type))]\n",
    "            for roi in hemi_roi_counts.hcp_roi:\n",
    "                hcp_suffix = roi.split(\"_\")[-1]\n",
    "                hemi_to_map_to = hemi_dict[hemi_type]\n",
    "                hcp_dscalar = f\"/tmp/{hemi_to_map_to}_{hcp_suffix}_ROI.dscalar.nii\"\n",
    "                hcp_coords = nib.load(hcp_dscalar).get_fdata()[0,:]==1\n",
    "                cohort_count_map[hcp_coords] = roi_counts[roi_counts.hcp_roi==roi].row_count.values[0]\n",
    "        \n",
    "        for hemisphere, flatmap, mapstyle_str in zip([\"left\",\"right\",\"left\"], [False,False,True], [\"lh-surf\",\"rh-surf\",\"flat\"]):\n",
    "            png_out = f\"{hcp_roi_count_label}_map-{mapstyle_str}_im-{im_code}_hcp-cohort-roi-count.png\" # Save png path\n",
    "            dscalar(\n",
    "                png_out, cohort_count_map, \n",
    "                orientation=\"portrait\", \n",
    "                hemisphere=hemisphere,\n",
    "                palette=PALETTE,\n",
    "                palette_params=palette_params,\n",
    "                transparent=False,\n",
    "                flatmap=flatmap,\n",
    "                flatmap_style='hcp_border',\n",
    "            )\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "VERTEX_TO = 59412\n",
    "hemi_dict = {\n",
    "    \"CONTRA\": \"L\",\n",
    "    \"IPSI\": \"R\",\n",
    "}\n",
    "palette_params = {\n",
    "    \"disp-zero\": False,\n",
    "    \"disp-neg\": False,\n",
    "    \"disp-pos\": True,\n",
    "    \"pos-user\": (0, 1),\n",
    "    \"neg-user\": (-1,0),\n",
    "    \"interpolate\": True,\n",
    "}\n",
    "PALETTE = \"magma\"\n",
    "pattern_dict = {\n",
    "    \"3TNormalEntrainL\": \"|\".join([f\"_{i}_entrain\" for i in [\"000\",\"002\",\"003\",\"004\",\"009\"]]),\n",
    "    \"3TNormalEntrainR\": \"|\".join([f\"_{i}_entrain\" for i in [\"005\",\"006\",\"007\",\"008\"]])\n",
    "}\n",
    "\n",
    "# Re-sort DF\n",
    "dense_vertex_count = expanded_df.groupby([\"experiment_sub_id\",\"im_code\"]).agg({\"vertex_id\": lambda x: list(x)}).reset_index()\n",
    "\n",
    "# Inputs\n",
    "n_experiments_all = [5,4,9,4,3,3,3,3]\n",
    "hcp_roi_count_labels = [\"3TNormalEntrainL\",\"3TNormalEntrainR\",\"3TNormalEntrain\",\"7TNormalEntrain\",\"3TVary020Entrain\",\"7TVary020Entrain\",\"3TVary021Entrain\",\"7TVary021Entrain\"]\n",
    "patterns = [None,None,\"3TNormal.*entrain\",\"7TNormal.*AttendAway\",\"3TVary_020.*entrain.*\",\"7TVary_020.*entrain.*\",\"3TVary_021.*entrain.*\",\"7TVary_021.*entrain.*\"]\n",
    "im_codes = [\"f1\",\"f2\",\"f2-f1\",\"f1+f2\",\"2f1\",\"2f2\",\"2f1-f2\",\"2f2-f1\"]\n",
    "\n",
    "\n",
    "for hcp_roi_count_label, pattern, n_experiments in zip(hcp_roi_count_labels, patterns, n_experiments_all):\n",
    "    for im_code in im_codes:\n",
    "        if pattern is None:\n",
    "            pattern = pattern_dict[hcp_roi_count_label]\n",
    "            _dense_vertex_count = dense_vertex_count[(dense_vertex_count.im_code==im_code) & (dense_vertex_count.apply(lambda row: row.astype(str).str.contains(pattern).any(),axis=1))]\n",
    "        else:\n",
    "            _dense_vertex_count = dense_vertex_count[(dense_vertex_count.experiment_sub_id.str.contains(pattern)) & (dense_vertex_count.im_code==im_code)]\n",
    "        print(hcp_roi_count_label, im_code, n_experiments)\n",
    "        cohort_count_map = np.zeros((59412,))\n",
    "        for row_ix, row in _dense_vertex_count.iterrows():\n",
    "            for vertex_id in row.vertex_id:\n",
    "                cohort_count_map[vertex_id] += 1\n",
    "        cohort_count_map = cohort_count_map / n_experiments\n",
    "        \n",
    "        for hemisphere, flatmap, mapstyle_str in zip([\"left\",\"right\",\"left\"], [False,False,True], [\"lh-surf\",\"rh-surf\",\"flat\"]):\n",
    "            png_out = f\"{hcp_roi_count_label}_map-{mapstyle_str}_im-{im_code}_dense-cohort-roi-count.png\" # Save png path\n",
    "            dscalar(\n",
    "                png_out, cohort_count_map, \n",
    "                orientation=\"portrait\", \n",
    "                hemisphere=hemisphere,\n",
    "                palette=PALETTE,\n",
    "                palette_params=palette_params,\n",
    "                transparent=False,\n",
    "                flatmap=flatmap,\n",
    "                flatmap_style='hcp_border',\n",
    "            )\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global variables\n",
    "VERTEX_TO = 59412\n",
    "hemi_dict = {\n",
    "    \"CONTRA\": \"L\",\n",
    "    \"IPSI\": \"R\",\n",
    "}\n",
    "palette_params = {\n",
    "    \"disp-zero\": False,\n",
    "    \"disp-neg\": False,\n",
    "    \"disp-pos\": True,\n",
    "    \"pos-user\": (0, 1),\n",
    "    \"neg-user\": (-1,0),\n",
    "    \"interpolate\": True,\n",
    "}\n",
    "PALETTE = \"magma\"\n",
    "pattern_dict = {\n",
    "    \"3TNormalControlL\": \"|\".join([f\"_{i}_control\" for i in [\"000\",\"002\",\"003\",\"004\",\"009\"]]),\n",
    "    \"3TNormalControlR\": \"|\".join([f\"_{i}_control\" for i in [\"005\",\"006\",\"007\",\"008\"]])\n",
    "}\n",
    "\n",
    "# Re-sort DF\n",
    "dense_vertex_count = control_expanded_df.groupby([\"experiment_sub_id\",\"im_code\"]).agg({\"vertex_id\": lambda x: list(x)}).reset_index()\n",
    "\n",
    "# Inputs\n",
    "n_experiments_all = [5,4]\n",
    "hcp_roi_count_labels = [\"3TNormalControlL\",\"3TNormalControlR\"]\n",
    "patterns = [None,None]\n",
    "im_codes = [\"f1\",\"f2\",\"f2-f1\",\"f1+f2\",\"2f1\",\"2f2\",\"2f1-f2\",\"2f2-f1\"]\n",
    "\n",
    "\n",
    "for hcp_roi_count_label, pattern, n_experiments in zip(hcp_roi_count_labels, patterns, n_experiments_all):\n",
    "    for im_code in im_codes:\n",
    "        if pattern is None:\n",
    "            pattern = pattern_dict[hcp_roi_count_label]\n",
    "            _dense_vertex_count = dense_vertex_count[(dense_vertex_count.im_code==im_code) & (dense_vertex_count.apply(lambda row: row.astype(str).str.contains(pattern).any(),axis=1))]\n",
    "        else:\n",
    "            _dense_vertex_count = dense_vertex_count[(dense_vertex_count.experiment_sub_id.str.contains(pattern)) & (dense_vertex_count.im_code==im_code)]\n",
    "        print(hcp_roi_count_label, im_code, n_experiments)\n",
    "        cohort_count_map = np.zeros((59412,))\n",
    "        for row_ix, row in _dense_vertex_count.iterrows():\n",
    "            for vertex_id in row.vertex_id:\n",
    "                cohort_count_map[vertex_id] += 1\n",
    "        cohort_count_map = cohort_count_map / n_experiments\n",
    "        \n",
    "        for hemisphere, flatmap, mapstyle_str in zip([\"left\",\"right\",\"left\"], [False,False,True], [\"lh-surf\",\"rh-surf\",\"flat\"]):\n",
    "            png_out = f\"{hcp_roi_count_label}_map-{mapstyle_str}_im-{im_code}_dense-cohort-roi-count.png\" # Save png path\n",
    "            dscalar(\n",
    "                png_out, cohort_count_map, \n",
    "                orientation=\"portrait\", \n",
    "                hemisphere=hemisphere,\n",
    "                palette=PALETTE,\n",
    "                palette_params=palette_params,\n",
    "                transparent=False,\n",
    "                flatmap=flatmap,\n",
    "                flatmap_style='hcp_border',\n",
    "            )\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
